import streamlit as st
import pandas as pd
import datetime as dt
from zoneinfo import ZoneInfo
import requests

# ====== Settings ======

API\_KEY = st.secrets.get("YOUTUBE\_API\_KEY", "")
REGION\_CODE = "KR"         # í•œêµ­ ê²°ê³¼ ìš°ì„ 
RELEVANCE\_LANG = "ko"      # í•œêµ­ì–´ ìš°ì„ 
KST = ZoneInfo("Asia/Seoul")
PT  = ZoneInfo("America/Los\_Angeles")
DAILY\_QUOTA = 10\_000       # YouTube Data API ê¸°ë³¸ ì¼ì¼ ì¿¼í„°

# ì¿¼í„° ì„¸ì…˜ ëˆ„ì ì‹œí‚´

import json, os
from pathlib import Path

DATA\_DIR = Path(".")
QUOTA\_FILE = DATA\_DIR / "quota\_usage.json"   # ì•± í´ë”ì— ì €ì¥ (ì•±ì´ ì‚´ì•„ìˆëŠ” í•œ ìœ ì§€)

def \_today\_pt\_str():
from zoneinfo import ZoneInfo
PT = ZoneInfo("America/Los\_Angeles")
now\_pt = dt.datetime.now(PT)
return now\_pt.strftime("%Y-%m-%d")

def load\_quota\_used():
"""íŒŒì¼ì—ì„œ ì˜¤ëŠ˜(PT) ì‚¬ìš©ëŸ‰ì„ ì½ì–´ì˜¨ë‹¤. ë‚ ì§œ ë‹¤ë¥´ë©´ 0ìœ¼ë¡œ ë¦¬ì…‹."""
today = \_today\_pt\_str()
if QUOTA\_FILE.exists():
try:
data = json.loads(QUOTA\_FILE.read\_text(encoding="utf-8"))
if data.get("pt\_date") == today:
return int(data.get("used", 0))
except Exception:
pass
return 0

def save\_quota\_used(value):
"""ì˜¤ëŠ˜(PT) ì‚¬ìš©ëŸ‰ì„ íŒŒì¼ì— ì €ì¥."""
data = {"pt\_date": \_today\_pt\_str(), "used": int(value)}
QUOTA\_FILE.write\_text(json.dumps(data), encoding="utf-8")

def add\_quota(cost):
"""ì¿¼í„°ë¥¼ ëˆ„ì (íŒŒì¼+ì„¸ì…˜ ëª¨ë‘)"""
\# ì„¸ì…˜(í™”ë©´ í‘œì‹œìš©)
st.session\_state\["quota\_used"] = st.session\_state.get("quota\_used", 0) + int(cost)
\# íŒŒì¼(ì˜êµ¬ ëˆ„ì )
current\_file\_val = load\_quota\_used()
save\_quota\_used(current\_file\_val + int(cost))

# ====== Time window (ë§ˆì§€ë§‰ 48ì‹œê°„, KST ê¸°ì¤€) ======

def kst\_window\_last\_48h():
now\_kst = dt.datetime.now(KST)
start\_kst = now\_kst - dt.timedelta(hours=48)
start\_utc = start\_kst.astimezone(dt.timezone.utc).isoformat().replace("+00:00", "Z")
end\_utc   = now\_kst.astimezone(dt.timezone.utc).isoformat().replace("+00:00", "Z")
return start\_utc, end\_utc, now\_kst

# ====== ISO8601 PT-duration -> seconds ======

def parse\_iso8601\_duration(s):
if not s or not s.startswith("PT"):
return None
s2 = s\[2:]; h=m=sec=0; num=""
for ch in s2:
if ch.isdigit(): num += ch
else:
if ch == "H":
h = int(num or 0)
elif ch == "M":
m = int(num or 0)
elif ch == "S":
sec = int(num or 0)
num = ""
return h*3600 + m*60 + sec

def fmt\_hms(seconds):
if seconds is None: return ""
h = seconds//3600; m=(seconds%3600)//60; s=seconds%60
return f"{h:02d}:{m:02d}:{s:02d}" if h>0 else f"{m:02d}:{s:02d}"

# ====== API helper (ì¿¼í„° ì¹´ìš´íŠ¸ í¬í•¨) ======

def api\_get(url, params, cost):
r = requests.get(url, params=params, timeout=20)
\# ì„±ê³µ/ì‹¤íŒ¨ì™€ ë¬´ê´€, ìœ íš¨/ë¬´íš¨ ìš”ì²­ ëª¨ë‘ ë¹„ìš© ë°œìƒ -> ë¬¸ì„œ ê·œì •
add\_quota(cost)
r.raise\_for\_status()
return r.json()

SEARCH\_URL = "[https://www.googleapis.com/youtube/v3/search](https://www.googleapis.com/youtube/v3/search)"
VIDEOS\_URL = "[https://www.googleapis.com/youtube/v3/videos](https://www.googleapis.com/youtube/v3/videos)"

def search\_ids(keyword, max\_pages=1):
start\_iso, end\_iso, \_ = kst\_window\_last\_48h()
vids, token, pages = \[], None, 0
while True:
params = {
"key": API\_KEY,
"part": "snippet",
"q": keyword,
"type": "video",
"order": "date",
"publishedAfter": start\_iso,
"publishedBefore": end\_iso,
"maxResults": 50,
"videoDuration": "short",
"regionCode": REGION\_CODE,
"relevanceLanguage": RELEVANCE\_LANG,
}
if token: params\["pageToken"] = token
data = api\_get(SEARCH\_URL, params, cost=100)  # search.list = 100
ids = \[it.get("id", {}).get("videoId") for it in data.get("items", \[]) if it.get("id", {}).get("videoId")]
vids.extend(ids)
token = data.get("nextPageToken"); pages += 1
if not token or pages >= max\_pages or len(vids) >= 200:
break
\# de-dup
seen, ordered = set(), \[]
for v in vids:
if v not in seen:
ordered.append(v); seen.add(v)
return ordered

def fetch\_details(video\_ids):
out=\[]
for i in range(0, len(video\_ids), 50):
chunk = video\_ids\[i\:i+50]
params = {"key": API\_KEY, "part": "snippet,contentDetails,statistics", "id": ",".join(chunk)}
data = api\_get(VIDEOS\_URL, params, cost=1)  # videos.list = 1
out.extend(data.get("items", \[]))
return out

def to\_kst(iso\_str):
t = dt.datetime.fromisoformat(iso\_str.replace("Z", "+00:00")).astimezone(KST)
return t.strftime("%Y-%m-%d %H:%M:%S (%Z)")

def make\_dataframe(keyword, max\_pages=1):
ids = search\_ids(keyword, max\_pages=max\_pages)
details = fetch\_details(ids)
rows=\[]
for item in details:
vid=item.get("id",""); sn=item.get("snippet",{}); cd=item.get("contentDetails",{}); stt=item.get("statistics",{})
secs = parse\_iso8601\_duration(cd.get("duration",""))
if secs is None or secs>60:  # Shortsë§Œ
continue
rows.append({
"title": sn.get("title",""),
"view\_count": stt.get("viewCount",""),
"length": fmt\_hms(secs),
"channel": sn.get("channelTitle",""),
"url": f"[https://www.youtube.com/watch?v={vid}](https://www.youtube.com/watch?v={vid})",
"published\_at\_kst": to\_kst(sn.get("publishedAt","")) if sn.get("publishedAt") else "",
})
df = pd.DataFrame(rows, columns=\["title","view\_count","length","channel","url","published\_at\_kst"])
df\["view\_count"] = pd.to\_numeric(df\["view\_count"], errors="coerce").fillna(0).astype(int)
return df

def next\_reset\_info():
now\_pt = dt.datetime.now(PT)
reset\_pt = (now\_pt + dt.timedelta(days=1)).replace(hour=0, minute=0, second=0, microsecond=0)
remaining = reset\_pt - now\_pt
reset\_kst = reset\_pt.astimezone(KST)
return reset\_pt, reset\_kst, remaining

# ====== ì¿¼í„° ì¹´ìš´íŠ¸ ======

# ì„¸ì…˜ ìƒíƒœì— ì¿¼í„° ì¹´ìš´í„° ì¤€ë¹„

if "quota\_used" not in st.session\_state:
    st.session\_state\["quota\_used"] = load\_quota\_used()
else:
    #ë‚ ì§œê°€ ë°”ë€Œì—ˆì„ ìˆ˜ë„ ìˆìœ¼ë‹ˆ ì¬ë™ê¸°í™”
    st.session\_state\["quota\_used"] = load\_quota\_used()

# ====== UI ======

st.set\_page\_config(page\_title="YouTube Shorts 48h Finder", page\_icon="ğŸ“º", layout="wide")
st.title("ğŸ“º 48ì‹œê°„ ì´ë‚´ ì—…ë¡œë“œëœ YouTube Shorts ì°¾ê¸° (KR)")

if not API\_KEY:
st.error("âš ï¸ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì¢Œì¸¡ ë©”ë‰´(â–¶) > Settings > Secrets ì— YOUTUBE\_API\_KEYë¥¼ ì¶”ê°€í•˜ì„¸ìš”.")
st.stop()

with st.sidebar:
st.header("ì„¤ì •")
keyword = st.text\_input("ê²€ìƒ‰ì–´", "")
max\_pages = st.radio("ê²€ìƒ‰ í˜ì´ì§€ ìˆ˜(ì¿¼í„° ì ˆì•½)", options=\[1,2], index=0)
st.caption("ë²”ìœ„: í˜„ì¬ ì‹œê°(KST) ê¸°ì¤€ **ì§€ë‚œ 48ì‹œê°„**")
run\_btn = st.button("ê²€ìƒ‰ ì‹¤í–‰")

# ì¿¼í„° íŒ¨ë„

used = st.session\_state\["quota\_used"]
remaining = max(0, DAILY\_QUOTA - used)
pct = min(1.0, used / DAILY\_QUOTA) if DAILY\_QUOTA else 0.0

reset\_pt, reset\_kst, remaining\_td = next\_reset\_info()

quota\_col1, quota\_col2 = st.columns(\[2,1])
with quota\_col1:
st.subheader("ğŸ”‹ ì¿¼í„° ì‚¬ìš©ëŸ‰(ì¶”ì •)")
st.progress(pct, text=f"ì‚¬ìš© {used} / {DAILY\_QUOTA}  (ë‚¨ì€ {remaining})")
with quota\_col2:
st.metric("ë‚¨ì€ ì¿¼í„°(ì¶”ì •)", value=f"{remaining:,}", delta=f"ë¦¬ì…‹ê¹Œì§€ {remaining\_td}".replace("days","ì¼").replace("day","ì¼"))
st.caption(f"â€» ì¼ì¼ ì¿¼í„°ëŠ” PT ìì •(í•œêµ­ì‹œê°„ ë‹¤ìŒë‚  16\~17ì‹œ)ì— ë¦¬ì…‹")

# ì‹¤í–‰

if run\_btn:
with st.spinner("ê²€ìƒ‰ ì¤‘â€¦ â³"):
df = make\_dataframe(keyword, max\_pages=max\_pages)
df\_top = df.sort\_values("view\_count", ascending=False, ignore\_index=True).head(20)
st.success(f"ê²€ìƒ‰ ì™„ë£Œ: í›„ë³´ {len(df)}ê°œ ì¤‘ ìƒìœ„ 20ê°œ í‘œì‹œ")

```
sort_col = st.selectbox("ì •ë ¬ ì»¬ëŸ¼", ["view_count","title","length","channel","published_at_kst"])
sort_order = st.radio("ì •ë ¬ ìˆœì„œ", ["ë‚´ë¦¼ì°¨ìˆœ","ì˜¤ë¦„ì°¨ìˆœ"], horizontal=True, index=0)
asc = (sort_order == "ì˜¤ë¦„ì°¨ìˆœ")
df_show = df_top.sort_values(sort_col, ascending=asc, ignore_index=True)

df_show = df_show[["title","view_count","length","channel","url","published_at_kst"]]

st.dataframe(df_show, use_container_width=True)

csv_bytes = df_show.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig")
st.download_button("CSV ë‹¤ìš´ë¡œë“œ", data=csv_bytes,
                   file_name=f"shorts_48h_{keyword}.csv", mime="text/csv")

st.info(f"ì´ë²ˆ ì‹¤í–‰ìœ¼ë¡œ ì¶”ì • ì‚¬ìš©ëŸ‰: search.list {100 * (max_pages)} + videos.list {1}")
```


