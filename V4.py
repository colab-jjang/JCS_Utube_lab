import os
import io
import re
import json
import datetime as dt
from typing import List, Dict, Tuple, Optional
from zoneinfo import ZoneInfo

import requests
import pandas as pd
import streamlit as st

from urllib.parse import unquote

# ==== Cloud backend (read-only, Gist) ====
def _gist_headers():
    tok = st.secrets.get("GH_TOKEN", "")
    return {"Authorization": f"token {tok}"} if tok else {}

def _gist_endpoint(gist_id: str) -> str:
    return f"https://api.github.com/gists/{gist_id}"

def cloud_load_whitelist() -> Optional[set] :
    gist_id = st.secrets.get("GIST_ID")
    fname = st.secrets.get("GIST_FILENAME", "whitelist_channels.json")
    if not gist_id:
        return None
    try:
        r = requests.get(_gist_endpoint(gist_id), headers=_gist_headers(), timeout=20)
        if r.status_code != 200:
            return None
        files = r.json().get("files", {})
        if fname not in files:
            return None
        content = files[fname].get("content", "") or "[]"
        data = json.loads(content)
        if isinstance(data, dict) and "channel_id" in data:
            data = data["channel_id"]
        if isinstance(data, list):
            return set(str(x) for x in data)
    except Exception:
        return None

def cloud_save_whitelist(ch_ids: set) -> bool:
    gist_id = st.secrets.get("GIST_ID")
    fname = st.secrets.get("GIST_FILENAME", "whitelist_channels.json")
    if not gist_id:
        return False
    payload = {
        "files": {
            fname: {
                "content": json.dumps(sorted(list(ch_ids)), ensure_ascii=False, indent=2)
            }
        }
    }
    try:
        r = requests.patch(_gist_endpoint(gist_id),
            headers={**_gist_headers(), "Accept": "application/vnd.github+json"},
            json=payload,
            timeout=20,)
        if r.status_code != 200:
            st.error(f"Gist ì €ì¥ ì‹¤íŒ¨: {r.status_code} {r.text[:200]}")
        return r.status_code == 200
    except Exception as e:
        st.error(f"Gist ì €ì¥ ì˜ˆì™¸: {e}")
        return False

# =========================================================
# ê¸°ë³¸ ìƒìˆ˜/í™˜ê²½
# =========================================================
APP_TITLE = "ìœ íŠœë¸Œ ìˆì¸  í‚¤ì›Œë“œ/ì˜ìƒ ë­í‚¹ v4"
KST = ZoneInfo("Asia/Seoul")
PT = ZoneInfo("America/Los_Angeles")
TTL_SECS_DEFAULT = 3600
COL_ORDER = [
    "title","view_count","length_mmss","channel",
    "like_count","comment_count","published_at_kst",
]

YOUTUBE_API_KEY = (
    (st.secrets.get("YOUTUBE_API_KEY", "") if hasattr(st, "secrets") else "")
    or os.getenv("YOUTUBE_API_KEY", "")
).strip()

API_BASE = "https://www.googleapis.com/youtube/v3"

# =========================================================
# UI í…Œë§ˆ(ê°„ì´ ë‹¤í¬/í°íŠ¸ ìŠ¤ì¼€ì¼)
# =========================================================
def apply_theme(dark_mode: bool, font_scale: float):
    st.markdown(
        f"""
        <style>
          html, body, [data-testid=\"stAppViewContainer\"] * {{
            font-size: {font_scale:.2f}rem !important;
          }}
        </style>
        """,
        unsafe_allow_html=True,
    )
    if dark_mode:
        st.markdown(
            """
            <style>
              [data-testid=\"stHeader\"] { background: #111 !important; }
            </style>
            """,
            unsafe_allow_html=True,
        )

# =========================================================
# ê°•í™”ëœ í•œêµ­ì–´ ê¸ˆì§€ íŒ¨í„´/ë‹¨ì–´ (v3 í™•ì¥)
# =========================================================
COMMON_BANNED_PAT = [
    r"ì„ë°© ?í•˜ë¼", r"ì… ?ë‹¥ì¹˜ê³ ", r"ë¬´ìŠ¨ ?ì¼", r"ìˆ˜ ?ìˆ(?:ì„ê¹Œ|ë‚˜)", r"ìˆ˜ ?ì—†ë‚˜",
    r"(?<!\w)#[\wê°€-í£_]+", r"(?<!\w)@[\wê°€-í£_]+", r"\b\d{1,2}:\d{2}(?::\d{2})?\b",
    r"\[(?:ìë§‰|ENG|SUB|KOR|KO|JP|CN|FULL|4K|LIVE|ìƒë°©|ì‹¤ì‹œê°„)\]",
    r"[ã…‹ã… ã…]{2,}", r"[!?]{2,}", r"í—‰|í—|ì™€ìš°|í’‰|ã„·ã„·|ã„¹ã…‡|ë ˆì „ë“œ|ì‹¤í™”ëƒ",
    r"ì¶©ê²©|ê²½ì•…|ì†Œë¦„|ëŒ€ì°¸ì‚¬|ì´ˆìœ ì˜|í•µí­íƒ„|ëŒ€í­ë¡œ|ì¶©ê²©ê³ ë°±|ë¯¸ì¹œ|ë¯¸ì³¤|ì´ˆëŒ€í˜•|ì´ˆíŠ¹ê¸‰",
    r"ë‹¨ë…|ì†ë³´|ê¸´ê¸‰|ê¸´ê¸‰ì†ë³´|ë°©ê¸ˆì „|ì§€ê¸ˆë‚œë¦¬|ìµœí›„í†µì²©|ì „ë§|ì „ê²©|ì‹¤ì‹œê°„í­ë¡œ",
    r"ì™œ ?ê·¸ë¬|ì–´ë–»ê²Œ ?ì´ëŸ´|ë§ì´ ?ë˜ë‚˜|ì‚¬ì‹¤ ?ì¸ê°€|ë¯¿ê¸° ?ì–´ë µ|ì•„ë‹ˆ ?ê·¼ë°",
    r"êµ¬ë…|ì¢‹ì•„ìš”|ì•Œë¦¼ì„¤ì •|ëŒ“ê¸€ ?ë‹¬|í´ë¦­|ë§í¬|ê³µìœ |ì‹ ì²­|ì°¸ì—¬|í™•ì¸ ?í•´ë³´|ì‹œì²­ ?í•´ë³´",
    r"ì œíœ´|ê´‘ê³ |ìŠ¤í°ì„œ|ì¿ í°|í• ì¸ì½”ë“œ|í”„ë¡œëª¨ì…˜|ì´ë²¤íŠ¸|ë¬¸ì˜ ?ì£¼ì„¸ìš”|ë§í¬ ?ê³ ì •",
    r"ì˜¤ëŠ˜|ì–´ì œ|ë‚´ì¼|ë°©ê¸ˆ|í˜„ì¬|ì§€ê¸ˆ|ê³§|ê¸ˆì¼|ê¸ˆì£¼|ì´ë²ˆ ?ì£¼|ì´ë²ˆ ?ë‹¬|ì˜¬í•´|ì‘ë…„|ë‚´ë…„",
]
COMMON_STOPWORDS = {
    "ë‹¤íë””ê¹…","ë‚˜ëŠ” ì ˆë¡œ","ì „ë¬¸","ì‚¬ê³ ","sbs","kbs","mbc","jtbc","tvì¡°ì„ ","mbn",
    "ê²ƒ","ê±°","ê±°ì˜","ìˆ˜","ë“±","ë°","ë°ë“±","ì œ","ê·¸","ì´","ì €","ìš”","ë„¤","ì","ìš°ë¦¬","ì €í¬","ë„ˆí¬","ë‹¹ì‹ ","ì—¬ëŸ¬ë¶„","ë³¸ì¸","ìì‹ ",
    "í˜„ì¥","ì˜ìƒ","ì‚¬ì§„","í™”ë©´","ì¥ë©´","ë¶€ë¶„","ë‚´ìš©","ê´€ë ¨","ìë£Œ","ë¬¸ì„œ","ê¸°ì‚¬","ì œëª©","ì„¤ëª…","ë³¸ë¬¸","ìš”ì•½","ë§í¬","ì›ë¬¸","ì¶œì²˜","ìº¡ì²˜","ì¸ë„¤ì¼","ëŒ“ê¸€ì°½","ì±„íŒ…",
    "êµ¬ë…","ì¢‹ì•„ìš”","ì•Œë¦¼","ì•Œë¦¼ì„¤ì •","ëŒ“ê¸€","í´ë¦­","ê³µìœ ","ì‹ ì²­","ì°¸ì—¬","í™•ì¸","ì‹œì²­","ì¬ìƒ","ì¬ìƒëª©ë¡","í”Œë ˆì´ë¦¬ìŠ¤íŠ¸","ì—…ë¡œë“œ","ë§í¬ê³ ì •","ê³ ì •ëŒ“ê¸€",
    "ì˜¤ëŠ˜","ì–´ì œ","ë‚´ì¼","ë°©ê¸ˆ","ì§€ê¸ˆ","í˜„ì¬","ê³§","ê¸ˆì¼","ì´ë²ˆì£¼","ì´ë²ˆë‹¬","ì˜¬í•´","ì‘ë…„","ë‚´ë…„","ìƒˆë²½","ì˜¤ì „","ì˜¤í›„","ë°©ì†¡ì¤‘","ìƒë°©","ì‹¤ì‹œê°„",
    "í—‰","í—","ì™€ìš°","í’‰","ë ˆì „ë“œ","ì‹¤í™”ëƒ","ã„·ã„·","ã„¹ã…‡","ã…‹ã…‹","ã…ã…","ã… ã… ",
    "ì¶©ê²©","ê²½ì•…","ì†Œë¦„","ëŒ€ì°¸ì‚¬","ì´ˆìœ ì˜","í•µí­íƒ„","ëŒ€í­ë¡œ","ì¶©ê²©ê³ ë°±","ë¯¸ì¹œ","ì´ˆëŒ€í˜•","ì´ˆíŠ¹ê¸‰","ë‹¨ë…","ì†ë³´","ê¸´ê¸‰","ê¸´ê¸‰ì†ë³´","ë°©ê¸ˆì „","ë‚œë¦¬","ìµœí›„í†µì²©","ì „ë§","ì „ê²©",
    "ì œíœ´","ê´‘ê³ ","ìŠ¤í°ì„œ","ì¿ í°","í• ì¸","í• ì¸ì½”ë“œ","í”„ë¡œëª¨ì…˜","ì´ë²¤íŠ¸","ë¬¸ì˜",
    "ì‚¬ì‹¤","ì´ìŠˆ","ë¬¸ì œ","ìƒí™©","ì‚¬ê±´","ì˜í˜¹","ë…¼ë€","ë°œí‘œ","ì†Œì‹","ì „ë§","ì˜ˆìƒ","ê°€ëŠ¥ì„±","í™•ë¥ ","ê³„íš","ë³´ê³ ","ë¶„ì„","ì ê²€","ê²€í† ","ê²°ê³¼","ì§„í–‰","í˜„í™©","ê³µì§€","ê³µì§€ì‚¬í•­",
    "ì±„ë„","êµ¬ë…ì","ì¡°íšŒìˆ˜","ì¢‹ì•„ìš”ìˆ˜","ëŒ“ê¸€ìˆ˜","ì¡°íšŒ","ì¢‹ì•„ìš”","ëŒ“ê¸€","ì—…ë¡œë”","ì œì‘ì§„",
    "â€¦","..",".","â€”","-","_","/",":",";","!","?","#","@",
    "ytn","ì—°í•©ë‰´ìŠ¤","ì—°í•©","í•œê²¨ë ˆ","ê²½í–¥","êµ­ë¯¼ì¼ë³´","ë™ì•„ì¼ë³´","ì¡°ì„ ì¼ë³´","ì¤‘ì•™ì¼ë³´","ë‰´ì‹œìŠ¤","ë‰´ìŠ¤1","ì˜¤ë§ˆì´ë‰´ìŠ¤","í”„ë ˆì‹œì•ˆ","sbsë‰´ìŠ¤","kbsë‰´ìŠ¤","mbcë‰´ìŠ¤","jtbcë‰´ìŠ¤",
    "ê´€ë ¨ì˜ìƒ","ì „ì²´ì˜ìƒ","í’€ì˜ìƒ","í’€ë²„ì „","ìš”ì•½ë³¸","ë‹¤ì‹œë³´ê¸°","ë³´ë„","íŠ¹ì§‘","ë‹¨ì‹ ","ë‹¨ë…ë³´ë„","ì†ë³´ë³´ë„","ìƒì¤‘ê³„","ì¤‘ê³„","í˜„ì¥ì¤‘ê³„","ì¸í„°ë·°","ì§ìº ","í´ë¦½","ì‡¼ì¸ ",
    "shorts","short","live","full","eng","kor","sub","subs","4k","ì²˜ë¦¬","ì–¸ë¡ ","ì´ë¯¸ì§€","ì¶”ì„ì „","ì¶”ì„ ì „","ë¯¸êµ­","ì‹œì‘í•œë‹¤","ë³¸íšŒ","ë³´ë„","freepik","ì „í•˜ê² ë‹¤",
    "ì˜ìƒ","ë‰´ìŠ¤","ê´€ë ¨","ì¡°íšŒ","ì±„ë„","news",
}
COMMON_STOPWORDS.update({
    "í•œë‹¤","í•˜ê¸°","í•©ë‹ˆë‹¤","í–ˆë‹¤","í•˜ì˜€ë‹¤","ì‹œì‘í•œë‹¤","ì‹œì‘í•©ë‹ˆë‹¤",
    "ì „í•œë‹¤","ì „í•©ë‹ˆë‹¤","ì²˜ë¦¬","ì–¸ë¡ ","ì´ë¯¸ì§€","freepik",
    "ì¶”ì„ì „","ì¶”ì„ ì „"
})
KO_JOSA = [
    "ì€","ëŠ”","ì´","ê°€","ì„","ë¥¼","ì—","ì—ì„œ","ì—ê²Œ","ê»˜","ì™€","ê³¼","ë„","ìœ¼ë¡œ","ë¡œ","ì—ê²Œì„œ",
    "ë§ˆë‹¤","ë¶€í„°","ê¹Œì§€","ì¡°ì°¨","ë§Œ","ë¿","ì²˜ëŸ¼","ê°™ì´","ë³´ë‹¤","ì˜","ì´ë¼","ë¼","ì´ë‚˜","ë‚˜",
    "ë“ ì§€","ë¼ë„","ë¼ë„ìš”","ë‘","ì•¼","ìš”","ê»˜ì„œ","ì´ë‚˜ë§ˆ","ë¶€í„°ê°€","ìœ¼ë¡œì„œ","ìœ¼ë¡œì¨","ë¡œì„œ",
    "ë¡œì¨","ë§ˆì €","ë°–ì—","ì´ë©°","í•˜ë©°","í•˜ê³ ","í•´ì„œ","ì¸ë°","ì¸ë°ìš”","ì¸ë°ë‹¤","ì¸ë°ë‹¤ê°€",
    "ê»˜ìš”","ë°ìš”","ë“¤","ë“¤ì—","ë“¤ë¡œ","ë“¤ë„","ë“¤ì€","ì±„","í•˜ê¸°"
]
EN_STOP = {
    "the","a","an","and","or","but","to","of","for","on","in","at","by","with","from","as","is","are","was","were",
    "be","been","being","it","this","that","these","those","you","your","i","we","they","he","she","him","her","them",
    "my","our","their","me","us","do","does","did","done","can","will","would","should","could","if","so","not"
}

# =========================================================
# ì¿¼í„° ì¶”ì • (íŒŒì¼ì— ì €ì¥í•´ì„œ ìœ ì§€)
# =========================================================
DATA_DIR = "."
QUOTA_FILE = os.path.join(DATA_DIR, "quota_usage.json")

def _today_pt_str():
    now_pt = dt.datetime.now(PT)
    return now_pt.strftime("%Y-%m-%d")

def load_quota_used() -> int:
    today = _today_pt_str()
    if os.path.exists(QUOTA_FILE):
        try:
            data = json.loads(open(QUOTA_FILE, "r", encoding="utf-8").read())
            if data.get("pt_date") == today:
                return int(data.get("used", 0))
        except Exception:
            pass
    return 0

def save_quota_used(value: int):
    data = {"pt_date": _today_pt_str(), "used": int(value)}
    with open(QUOTA_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

class QuotaMeter:
    COST = {"channels.list": 1, "playlistItems.list": 1,
            "videos.list": 1, "search.list": 100}
    def __init__(self, daily_budget: int = 10000):
        self.daily_budget = daily_budget
        self.used_units = load_quota_used()
    def add(self, api_name: str, calls: int = 1):
        self.used_units += self.COST.get(api_name, 1) * max(1, calls)
        save_quota_used(self.used_units)
    @property
    def remaining(self):
        return max(0, self.daily_budget - self.used_units)
    @staticmethod
    def next_reset_pt():
        now_pt = dt.datetime.now(PT)
        tomorrow = (now_pt + dt.timedelta(days=1)).date()
        return dt.datetime.combine(tomorrow, dt.time(0,0,0), tzinfo=PT)

def get_quota():
    if "quota" not in st.session_state:
        st.session_state["quota"] = QuotaMeter()
    return st.session_state["quota"]

# =========================================================
# í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸(ê¸°ë³¸/ì €ì¥/Secrets) ë¶€íŠ¸ìŠ¤íŠ¸ë© + ì €ì¥
# =========================================================
DEFAULT_WHITELIST = [
    # ì¤€ë¹„ë˜ë©´ ì‹¤ì œ ì±„ë„ID(UC...)ë¥¼ ì´ ë¦¬ìŠ¤íŠ¸ì— ì±„ì›Œ ë„£ìœ¼ì„¸ìš”.
]
WL_STORE_PATH = ".whitelist_channels.json"

def load_whitelist_bootstrap() -> set:
    # (0) í´ë¼ìš°ë“œ ìš°ì„ 
    try:
        wl = cloud_load_whitelist()
        if wl:
            return wl
    except Exception:
        pass
    # (1) ì €ì¥íŒŒì¼
    try:
        if os.path.exists(WL_STORE_PATH):
            with open(WL_STORE_PATH, "r", encoding="utf-8") as f:
                data = json.load(f)
                if isinstance(data, list):
                    return set(data)
    except Exception:
        pass
    # (2) secrets
    try:
        if "WHITELIST_CHANNELS" in st.secrets:
            sec = st.secrets["WHITELIST_CHANNELS"]
            if isinstance(sec, list):
                return set(sec)
            if isinstance(sec, str):
                toks = [t.strip() for t in re.split(r"[\n,]+", sec) if t.strip()]
                return set(toks)
    except Exception:
        pass
    # (3) ì½”ë“œ ê¸°ë³¸ê°’
    return set(DEFAULT_WHITELIST)


def persist_whitelist(ch_ids: set):
    # 1) í´ë¼ìš°ë“œ ë¨¼ì €
    if cloud_save_whitelist(ch_ids):
        st.success("í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ë¥¼ í´ë¼ìš°ë“œ(Gist)ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
        # ìºì‹œìš©ìœ¼ë¡œ ë¡œì»¬ì—ë„ ì¨ë‘ê¸°(ì„ íƒ)
        try:
            with open(WL_STORE_PATH, "w", encoding="utf-8") as f:
                json.dump(sorted(list(ch_ids)), f, ensure_ascii=False, indent=2)
        except Exception:
            pass
        return
    # 2) í´ë°±: ë¡œì»¬ ì €ì¥
    try:
        with open(WL_STORE_PATH, "w", encoding="utf-8") as f:
            json.dump(sorted(list(ch_ids)), f, ensure_ascii=False, indent=2)
        st.success("í´ë¼ìš°ë“œ ì €ì¥ ì‹¤íŒ¨ â†’ ë¡œì»¬ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        st.warning(f"í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ì €ì¥ ì¤‘ ê²½ê³ : {e}")


# =========================================================
# ìœ í‹¸/íŒŒì„œ
# =========================================================
@st.cache_data(show_spinner=False, ttl=TTL_SECS_DEFAULT)
def trending_news_politics(region_code: str, max_pages: int = 1) -> Dict[str, dict]:
    """ë‰´ìŠ¤Â·ì •ì¹˜(25) mostPopular â†’ í›„ë‹¨ì—ì„œ 24h + Shorts(â‰¤60s) í•„í„°"""
    if not YOUTUBE_API_KEY:
        return {}
    quota = get_quota()
    url = f"{API_BASE}/videos"
    params = {
    "key": YOUTUBE_API_KEY,
    "q": "query",
    "type": "video",
    "part": "snippet",
    "maxResults": 50,
    "order": "date",
    "publishedAfter": published_after_utc,
    "videoDuration": "short",
    "relevanceLanguage": "ko",
    "regionCode": "KR",
    "videoCategoryId": "25",
    "safeSearch": "none",
    }

    out: Dict[str, dict] = {}
    page = 0
    next_token = None
    try:
        while True:
            if next_token:
                params["pageToken"] = next_token
            r = requests.get(url, params=params, timeout=20)
            quota.add("videos.list")
            if r.status_code != 200:
                break
            data = r.json()
            for it in data.get("items", []):
                out[it["id"]] = it
            next_token = data.get("nextPageToken")
            page += 1
            if not next_token or page >= max_pages:
                break
    except Exception as e:
        st.warning(f"íŠ¸ë Œë“œ ì¡°íšŒ ê²½ê³ : {e}")
    return out

def iso8601_to_seconds(iso_duration: str) -> int:
    if not iso_duration or not iso_duration.startswith("PT"):
        return 0
    h = m = s = 0
    num = ""
    for ch in iso_duration[2:]:
        if ch.isdigit():
            num += ch
        else:
            if ch == "H":
                h = int(num or 0)
            elif ch == "M":
                m = int(num or 0)
            elif ch == "S":
                s = int(num or 0)
            num = ""
    return h * 3600 + m * 60 + s


def sec_to_mmss(seconds: int) -> str:
    m = seconds // 60
    s = seconds % 60
    return f"{m:02d}:{s:02d}"


def parse_channel_input(text: str) -> List[str]:
    return [t.strip() for t in re.split(r"[\n,]+", text or "") if t.strip()]


@st.cache_data(show_spinner=False, ttl=TTL_SECS_DEFAULT)
def resolve_handle_to_channel_id(handle_or_name: str) -> Optional[str]:
    """@handle ë˜ëŠ” ì»¤ìŠ¤í…€ URL â†’ ì±„ë„ ID ë³€í™˜ (ì¿¼í„° 1Uë§Œ ì†Œëª¨)"""
    if not YOUTUBE_API_KEY:
        return None
    quota = get_quota()
    try:
        r = requests.get(
            f"{API_BASE}/channels",
            params={
                "key": YOUTUBE_API_KEY,
                "forHandle": handle_or_name.lstrip("@"),
                "part": "id",
            },
            timeout=15,
        )
        quota.add("channels.list")  # 1U ì†Œëª¨
        if r.status_code == 200:
            items = r.json().get("items", [])
            if items:
                return items[0]["id"]
    except Exception as e:
        st.warning(f"ì±„ë„ í•´ì„ ê²½ê³ : {e}")
    return None

def extract_channel_id(token: str) -> Optional[str]:
    raw = (token or "").strip()
    print(f"[DEBUG] ì…ë ¥ token: {raw}")   # ë””ë²„ê·¸ ë¡œê·¸

    token = unquote(raw).strip()
    token = re.sub(r"[/?#]+$", "", token)
    print(f"[DEBUG] ì •ë¦¬ëœ token: {token}")

    # --- @handle URL ì²˜ë¦¬ ---
    if "youtube.com/@" in token:
        handle = token.split("youtube.com/@", 1)[1]
        print(f"[DEBUG] handle URL ê°ì§€ â†’ {handle}")
        cid = resolve_handle_to_channel_id(handle)
        print(f"[DEBUG] API ë³€í™˜ ê²°ê³¼: {cid}")
        return cid

    if token.startswith("UC") and len(token) >= 10:
        print(f"[DEBUG] ì§ì ‘ UC ID ê°ì§€ â†’ {token}")
        return token

    m = re.search(r"youtube\.com/(channel/|c/|user/|@)([^/?#]+)", token)
    if m:
        kind, key = m.group(1), m.group(2)
        print(f"[DEBUG] ì¼ë°˜ URL ë§¤ì¹˜ â†’ kind={kind}, key={key}")
        if kind == "channel/":
            return key
        cid = resolve_handle_to_channel_id(key)
        print(f"[DEBUG] API ë³€í™˜ ê²°ê³¼: {cid}")
        return cid

    if token.startswith("@"):
        print(f"[DEBUG] ë‹¨ìˆœ handle â†’ {token}")
        cid = resolve_handle_to_channel_id(token[1:])
        print(f"[DEBUG] API ë³€í™˜ ê²°ê³¼: {cid}")
        return cid

    # fallback
    print(f"[DEBUG] ê¸°íƒ€ ì¼€ì´ìŠ¤ â†’ {token}")
    cid = resolve_handle_to_channel_id(token)
    print(f"[DEBUG] API ë³€í™˜ ê²°ê³¼: {cid}")
    return cid


@st.cache_data(show_spinner=False, ttl=TTL_SECS_DEFAULT)
def playlist_recent_video_ids(playlist_id: str, published_after_utc: str) -> List[Tuple[str, str]]:
    if not YOUTUBE_API_KEY:
        return []
    quota = get_quota()
    out = []
    url = f"{API_BASE}/playlistItems"
    params = {
        "key": YOUTUBE_API_KEY,
        "playlistId": playlist_id,
        "part": "snippet,contentDetails",
        "maxResults": 50,
    }
    try:
        while True:
            r = requests.get(url, params=params, timeout=20)
            quota.add("playlistItems.list")
            if r.status_code != 200:
                break
            data = r.json()
            for it in data.get("items", []):
                vid = it["contentDetails"]["videoId"]
                pub = it["contentDetails"].get("videoPublishedAt") or it["snippet"].get("publishedAt")
                if pub and pub >= published_after_utc:
                    out.append((vid, pub))
            token = data.get("nextPageToken")
            if not token or len(out) > 500:
                break
            params["pageToken"] = token
    except Exception as e:
        st.warning(f"í”Œë ˆì´ë¦¬ìŠ¤íŠ¸ ì¡°íšŒ ê²½ê³ : {e}")
    return out


@st.cache_data(show_spinner=False, ttl=TTL_SECS_DEFAULT)
def fetch_videos_details(video_ids: List[str]) -> Dict[str, dict]:
    if not YOUTUBE_API_KEY or not video_ids:
        return {}
    quota = get_quota()
    url = f"{API_BASE}/videos"
    results: Dict[str, dict] = {}
    try:
        for i in range(0, len(video_ids), 50):
            batch = video_ids[i : i + 50]
            r = requests.get(
                url,
                params={
                    "key": YOUTUBE_API_KEY,
                    "id": ",".join(batch),
                    "part": "snippet,contentDetails,statistics",
                },
                timeout=20,
            )
            quota.add("videos.list")
            if r.status_code != 200:
                continue
            for it in r.json().get("items", []):
                results[it["id"]] = it
    except Exception as e:
        st.warning(f"ë¹„ë””ì˜¤ ìƒì„¸ ì¡°íšŒ ê²½ê³ : {e}")
    return results


@st.cache_data(show_spinner=False, ttl=TTL_SECS_DEFAULT)
def global_search_recent(query: str, published_after_utc: str, max_pages: int = 1) -> List[str]:
    if not YOUTUBE_API_KEY or not query:
        return []
    quota = get_quota()
    out_ids: List[str] = []
    url = f"{API_BASE}/search"
    params = {
        "key": YOUTUBE_API_KEY,
        "q": query,
        "type": "video",
        "part": "snippet",
        "maxResults": 50,
        "order": "date",
        "publishedAfter": published_after_utc,
        "videoDuration": "short",  # <=4ë¶„, í›„ë‹¨ì—ì„œ 60ì´ˆë¡œ ì¬í•„í„°
    }
    try:
        page = 0
        while True:
            r = requests.get(url, params=params, timeout=20)
            quota.add("search.list")
            if r.status_code != 200:
                break
            data = r.json()
            for it in data.get("items", []):
                out_ids.append(it["id"]["videoId"])
            token = data.get("nextPageToken")
            page += 1
            if not token or page >= max_pages:
                break
            params["pageToken"] = token
    except Exception as e:
        st.warning(f"ì „ì—­ ê²€ìƒ‰ ê²½ê³ : {e}")
    return out_ids

@st.cache_data(show_spinner=False, ttl=24*3600)
def fetch_channel_titles(channel_ids: list[str]) -> pd.DataFrame:
    """ì±„ë„ ID ë¦¬ìŠ¤íŠ¸ â†’ ì±„ë„ëª… ë§¤í•‘ DataFrame.
       YouTube APIì˜ channels.list í˜¸ì¶œ (50ê°œì”© ë°°ì¹˜).
       24ì‹œê°„ ìºì‹œ ì ìš©."""
    if not channel_ids or not YOUTUBE_API_KEY:
        return pd.DataFrame(columns=["channel_id", "channel_title"])
    
    out = []
    url = f"{API_BASE}/channels"
    quota = get_quota()
    
    for i in range(0, len(channel_ids), 50):
        batch = channel_ids[i:i+50]
        try:
            r = requests.get(
                url,
                params={
                    "key": YOUTUBE_API_KEY,
                    "id": ",".join(batch),
                    "part": "snippet",
                },
                timeout=15,
            )
            quota.add("channels.list")
            if r.status_code == 200:
                for it in r.json().get("items", []):
                    out.append({
                        "channel_id": it.get("id", ""),
                        "channel_title": (it.get("snippet", {}) or {}).get("title", ""),
                    })
        except Exception as e:
            st.warning(f"ì±„ë„ëª… ì¡°íšŒ ê²½ê³ : {e}")
    
    return pd.DataFrame(out)

# =========================================================
# í‚¤ì›Œë“œ(ëª…ì‚¬êµ¬) ì¶”ì¶œ
# =========================================================

def normalize_text(s: str) -> str:
    s = re.sub(r"https?://\S+", " ", s or "")
    s = re.sub(r"[^\w\s@#\-ê°€-í£A-Za-z0-9]", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s


def trim_josa_ko(token: str) -> str:
    for j in sorted(KO_JOSA, key=len, reverse=True):
        if token.endswith(j) and len(token) > len(j) + 1:
            return token[: -len(j)]
    return token


def extract_keywords(text: str, banned_patterns: List[str], banned_words: set, top_k: int = 20):
    if not text:
        return []
    text = normalize_text(text)
    for pat in banned_patterns:
        text = re.sub(pat, " ", text, flags=re.IGNORECASE)
    tokens = text.split()
    ko_tokens, en_tokens = [], []
    for t in tokens:
        if re.search(r"[ê°€-í£]", t):
            ko_tokens.append(trim_josa_ko(t))
        else:
            tt = t.lower()
            if tt in EN_STOP:
                continue
            en_tokens.append(tt)

     # === ì¶”ê°€ í•„í„°ë§ ===
    def is_valid_word(w: str) -> bool:
        if len(w) < 2:  # 1ê¸€ì ì œê±°
            return False
        if w.isdigit():  # ìˆ«ì ì œê±°
            return False
        if re.match(r"^[0-9]+[a-zA-Z]*$", w):  # ex: 00, 24h, 5g
            return False
        if w in banned_words:  # stopwords
            return False
        return True

    # stopwords ì ìš© + ìµœì†Œ ê¸¸ì´ í•„í„°
    ko_tokens = [t for t in ko_tokens if len(t) >= 2 and t not in banned_words]
    en_tokens = [t for t in en_tokens if len(t) >= 2 and t not in banned_words]

    phrases = ko_tokens + en_tokens   # ğŸš¨ ì—¬ê¸°ì„œ ngrams ì œê±°!

    def canon(p):
        c = re.sub(r"\s+", "", p.lower())
        c = re.sub(r"[^\wê°€-í£]", "", c)
        c = trim_josa_ko(c)
        return c

    freq: Dict[str, int] = {}
    display: Dict[str, str] = {}
    for p in set(phrases):
        key = canon(p)
        if not key or key in banned_words or len(key) < 2:
            continue
        if key.endswith(("ë‹¤","í•œë‹¤","í•˜ê¸°","í•©ë‹ˆë‹¤","í•˜ì˜€ë‹¤")):  # ë™ì‚¬í˜• ì œê±°
            continue
        freq[key] = freq.get(key, 0) + 1
        display.setdefault(key, p)

    ranked = sorted(freq.items(), key=lambda x: x[1], reverse=True)[: top_k]
    return [(display[k], c) for k, c in ranked]

def aggregate_keywords(rows: List[dict], banned_patterns: List[str], banned_words: set, top_k: int = 20):
    blob = []
    for r in rows:
        blob.append(r.get("title", ""))
        blob.append(r.get("description", ""))
    pairs = extract_keywords("\n".join(blob), banned_patterns, banned_words, top_k=top_k)
    return pd.DataFrame([{"keyword": k, "count": c} for k, c in pairs])

# --- Helper: í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ í‚¤ì›Œë“œ ë­í‚¹ ìƒì„± ---
def build_keyword_ranking(rows_all: List[Dict], banned_patterns: List[str], banned_words: set, top_k: int = 300) -> pd.DataFrame:
    """í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ì±„ë„ì—ì„œ 24h ìˆ˜ì§‘ëœ ëª¨ë“  Shortsë¥¼ ê¸°ë°˜ìœ¼ë¡œ í‚¤ì›Œë“œ ë­í‚¹ êµ¬ì„±.
    ê° í‚¤ì›Œë“œ:
      - channel_overlap: í‚¤ì›Œë“œê°€ ë“±ì¥í•œ 'ì„œë¡œ ë‹¤ë¥¸ ì±„ë„' ìˆ˜
      - top_view_count: ê·¸ í‚¤ì›Œë“œ í¬í•¨ ì˜ìƒ ì¤‘ ìµœëŒ€ ì¡°íšŒìˆ˜
      - top_channel/top_url: ìµœëŒ€ ì¡°íšŒìˆ˜ ì˜ìƒì˜ ì±„ë„/URL
    ì •ë ¬: top_view_count ì˜¤ë¦„ì°¨ìˆœ.
    """
    keyword_to_channels: Dict[str, set] = {}
    keyword_to_best: Dict[str, Tuple[int, str, str]] = {}
    for r in rows_all:
        title = r.get("title", "")
        desc = r.get("description", "")
        ch = r.get("channel", "")
        url = r.get("url", "")
        views = int(r.get("view_count", 0) or 0)
        kv_pairs = extract_noun_phrases(f"{title}\n{desc}", banned_patterns, banned_words, top_k=top_k)
        for kw in {k for k,_ in kv_pairs}:  # ë¹„ë””ì˜¤ ë‚´ ì¤‘ë³µ ì œê±°
            keyword_to_channels.setdefault(kw, set()).add(ch)
            best = keyword_to_best.get(kw)
            if best is None or views > best[0]:
                keyword_to_best[kw] = (views, ch, url)
    recs = []
    for kw, ch_set in keyword_to_channels.items():
        best_views, best_ch, best_url = keyword_to_best.get(kw, (0, "", ""))
        recs.append({
            "keyword": kw,
            "channel_overlap": len(ch_set),
            "top_view_count": int(best_views),
            "top_channel": best_ch,
            "top_url": best_url,
        })
    dfk = pd.DataFrame(recs)
    if dfk.empty:
        return dfk
    return dfk.sort_values(["top_view_count", "channel_overlap", "keyword"], ascending=[True, False, True]).reset_index(drop=True)

# =========================================================
# Streamlit ì‹œì‘
# =========================================================
st.set_page_config(page_title=APP_TITLE, layout="wide")
st.title(APP_TITLE)

# ì„¸ì…˜ ë¶€íŠ¸ìŠ¤íŠ¸ë©
if "quota" not in st.session_state:
    st.session_state["quota"] = QuotaMeter()
if "whitelist_ids" not in st.session_state:
    st.session_state["whitelist_ids"] = load_whitelist_bootstrap()

# ---------------------------------------------------------
# ì‚¬ì´ë“œë°”
# ---------------------------------------------------------
with st.sidebar:
    st.subheader("ë°ì´í„° ì†ŒìŠ¤")
    data_source = st.radio(
        "ìˆ˜ì§‘ ëª¨ë“œ",
        ["ì „ì²´ íŠ¸ë Œë“œ(ë‰´ìŠ¤Â·ì •ì¹˜)", "ë“±ë¡ ì±„ë„ ë­í‚¹", "ì „ì—­ í‚¤ì›Œë“œ ê²€ìƒ‰"],
        index=0,
    )

    st.subheader("ì •ë ¬")
    metric = st.selectbox("ì •ë ¬ ê¸°ì¤€", ["view_count", "views_per_hour", "comment_count", "like_count"], index=0)
    ascending = st.toggle("ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬", value=False)

    st.caption("ìºì‹œ TTL: 1ì‹œê°„(ê³ ì •) â€¢ ìˆ˜ì§‘ ì°½: ìµœê·¼ 24ì‹œê°„(ê³ ì •) â€¢ Shorts â‰¤ 60ì´ˆ(ê³ ì •)")

    if data_source == "ë“±ë¡ ì±„ë„ ë­í‚¹":
        st.subheader("í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ê´€ë¦¬")
        wl_ids = set(st.session_state.get("whitelist_ids", set()))
            
       # (1) í´ë¼ìš°ë“œì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸° ë²„íŠ¼
        if st.button("ì €ì¥ëœ í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ë³´ê¸°", use_container_width=True):
            wl_cloud = cloud_load_whitelist()
            if wl_cloud is None:
                st.error("í´ë¼ìš°ë“œ(Gist)ì—ì„œ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (í† í°/GIST_ID/íŒŒì¼ëª…/ë„¤íŠ¸ì›Œí¬ í™•ì¸)")
            else:
                st.caption(f"í´ë¼ìš°ë“œ í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ì±„ë„ ìˆ˜: {len(wl_cloud)}ê°œ")
        
                if len(wl_cloud) == 0:
                    st.info("í´ë¼ìš°ë“œì— í˜„ì¬ ì±„ë„ì´ 0ê°œì…ë‹ˆë‹¤. (ì €ì¥ ë²„íŠ¼ìœ¼ë¡œ ì±„ë„ì„ ì˜¬ë ¤ì£¼ì„¸ìš”)")
                else:
                    df_view = fetch_channel_titles(sorted(list(wl_cloud)))
                    if not df_view.empty:
                        st.dataframe(df_view[["channel_title"]], use_container_width=True, height=250)
                    else:
                        # API í‚¤ ì—†ê±°ë‚˜ ë§¤í•‘ ì‹¤íŒ¨í•˜ë©´ IDë¼ë„ í‘œì‹œ
                        st.dataframe(pd.DataFrame({"channel_title": sorted(list(wl_cloud))}),
                                     use_container_width=True, height=250)

        # (2) ì—…ë¡œë“œ (CSV/XLSX)
        wl_file = st.file_uploader(
            "CSV ë˜ëŠ” XLSX ì—…ë¡œë“œ (channel_id / handle / url)", 
            type=["csv", "xlsx"], key="whitelist_file"
        )
        if wl_file:
            try:
                if wl_file.name.lower().endswith(".csv"):
                    df_w = pd.read_csv(wl_file)
                else:
                    df_w = pd.read_excel(wl_file)
        
                cols = {c.strip().lower(): c for c in df_w.columns}
        
                if "channel_id" in cols:
                    raw_list = [str(x) for x in df_w[cols["channel_id"]].dropna().tolist()]
                elif "handle" in cols:
                    raw_list = [f"@{str(x).lstrip('@')}" for x in df_w[cols["handle"]].dropna().tolist()]
                elif "url" in cols:
                    raw_list = [str(x) for x in df_w[cols["url"]].dropna().tolist()]
                else:
                    st.warning("CSV/XLSXì— channel_id / handle / url ì»¬ëŸ¼ ì¤‘ í•˜ë‚˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
                # === [ì—¬ê¸° ì‚½ì…] íŒŒì¼ ì»¬ëŸ¼/ì›ë³¸ ë¦¬ìŠ¤íŠ¸ í™•ì¸ ===
                #st.write("íŒŒì¼ ì»¬ëŸ¼ëª…:", df_w.columns.tolist())
                #st.write("raw_list (ì›ë³¸):", raw_list)
        
                # === ì—¬ê¸°ì„œ ID ë³€í™˜ ===
                added = []
                for tok in raw_list: 
                    cid = extract_channel_id(tok) 
                    if cid: 
                        added.append(cid)
        
                # === [ì—¬ê¸° ì‚½ì…] ë³€í™˜ ê²°ê³¼ í™•ì¸ ===
                #st.write("ì¶”ì¶œëœ ì±„ë„ID (added):", added)
                
                st.caption(f"ì¶”ê°€ëœ ì±„ë„ ìˆ˜: {len(added)} (ì´ {len(wl_ids)})")
                
                wl_ids.update(added)
                st.session_state["whitelist_ids"] = wl_ids
        
                # === ID â†’ ì±„ë„ëª… ë§¤í•‘ ===
                df_titles = fetch_channel_titles(list(added))
                if not df_titles.empty:
                    st.session_state["_id2title"] = {r["channel_id"]: r["channel_title"] for _, r in df_titles.iterrows()}
        
                st.caption(f"ì¶”ê°€ëœ ì±„ë„ ìˆ˜: {len(added)} (ì´ {len(wl_ids)})")
        
            except Exception as e:
                st.warning(f"í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ íŒŒì¼ íŒŒì‹± ì˜¤ë¥˜: {e}")
        
        # (3) ìˆ˜ë™ ì¶”ê°€/ì œê±° UI (ë²„íŠ¼ ë¶„ë¦¬)
        new_tokens = st.text_area("ìˆ˜ë™ ì¶”ê°€ (@handle / URL / channel_id)", height=80, placeholder="@KBSNEWS, https://www.youtube.com/@jtbcnews")
        if st.button("ì„ íƒ ì¶”ê°€", use_container_width=True):
            added = []
            for tok in parse_channel_input(new_tokens):
                cid = extract_channel_id(tok)
                if cid:
                    added.append(cid)
            wl_ids.update(added)
            st.session_state["whitelist_ids"] = wl_ids
            st.success(f"ì¶”ê°€ ì™„ë£Œ: {len(added)}ê°œ (ì´ {len(wl_ids)})")
        
        # ì„ íƒ ì‚­ì œ (ë¦¬ìŠ¤íŠ¸ì—ì„œ ê³ ë¥´ê¸°)
        # ì´ë¦„ìœ¼ë¡œ ë³´ì´ëŠ” ë©€í‹°ì…€ë ‰íŠ¸(ë‚´ë¶€ ê°’ì€ ID)
        id2title = st.session_state.get("_id2title", {})
        selected_remove = st.multiselect(
            "ì‚­ì œí•  ì±„ë„ ì„ íƒ",
            options=sorted(list(wl_ids)),
            format_func=lambda cid: id2title.get(cid, cid)
        )
        col_rm1, col_rm2 = st.columns(2)
        with col_rm1:
            if st.button("ì„ íƒ ì‚­ì œ", use_container_width=True, disabled=not bool(selected_remove)):
                before = len(wl_ids)
                wl_ids = {x for x in wl_ids if x not in set(selected_remove)}
                st.session_state["whitelist_ids"] = wl_ids
                st.success(f"ì œê±° ì™„ë£Œ: {before - len(wl_ids)}ê°œ (ì´ {len(wl_ids)})")
        with col_rm2:
            if st.button("ì „ì²´ ë¹„ìš°ê¸°", use_container_width=True, disabled=not bool(wl_ids)):
                st.session_state["whitelist_ids"] = set()
                wl_ids = set()
                st.success("í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ë¥¼ ëª¨ë‘ ë¹„ì› ìŠµë‹ˆë‹¤.")
        
        c1, c2 = st.columns(2)
        with c1:
            if st.button("í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ì €ì¥", use_container_width=True):
                ids_to_save = st.session_state.get("whitelist_ids", set())
                st.caption(f"ì €ì¥ ì‹œë„: {len(ids_to_save)}ê°œë¥¼ í´ë¼ìš°ë“œì— ì €ì¥í•©ë‹ˆë‹¤.")
                persist_whitelist(ids_to_save)
        with c2:
            if wl_ids:
                wl_csv = io.StringIO()
                pd.DataFrame({"channel_id": sorted(list(wl_ids))}).to_csv(wl_csv, index=False, encoding="utf-8-sig")
                st.download_button("CSV ë‚´ë ¤ë°›ê¸°", wl_csv.getvalue().encode("utf-8-sig"), file_name="whitelist_channels.csv", mime="text/csv", use_container_width=True)
        
        st.caption(f"í˜„ì¬ ì ìš© ì±„ë„ ìˆ˜: **{len(wl_ids)}**")
        #------------- ì—¬ê¸°ê¹Œì§€------------------------------

# API í‚¤ ìƒíƒœ ë°°ì§€(ì§„ë‹¨ìš©)    
    st.caption(f"YouTube API Key: {'âœ… ì„¤ì •ë¨' if bool(YOUTUBE_API_KEY) else 'âŒ ì—†ìŒ'}")

#í† í° ì˜ ë“¤ì–´ê°€ëŠ”ì§€ í™•ì¸í•˜ëŠ” ë¶€ë¶„
    st.caption(
        f"Gist Secrets â–¶ ID: {'âœ…' if (st.secrets.get('GIST_ID') or '').strip() else 'âŒ'} Â· "
        f"Token: {'âœ…' if (st.secrets.get('GH_TOKEN') or '').strip() else 'âŒ'} Â· "
        f"File: {st.secrets.get('GIST_FILENAME','(default)')}"
    )


    # ëª¨ë“œë³„ ì…ë ¥
if data_source == "ë“±ë¡ ì±„ë„ ë­í‚¹":
    mode = st.radio("ì±„ë„ ì…ë ¥ ë°©ì‹", ["ìˆ˜ë™ ì…ë ¥", "íŒŒì¼ ì—…ë¡œë“œ(CSV/XLSX)"], horizontal=True)
elif data_source == "ì „ì—­ í‚¤ì›Œë“œ ê²€ìƒ‰":
    st.subheader("ì „ì—­ í‚¤ì›Œë“œ ê²€ìƒ‰")
    global_query = st.text_input("ê²€ìƒ‰ì–´(24h ë‚´, Shorts)", placeholder="ì˜ˆ) êµ­íšŒ, ëŒ€ì„ , ê²½ì œ, ì™¸êµ, ì•ˆë³´ ...")
    max_pages = st.slider("ê²€ìƒ‰ í˜ì´ì§€ ìˆ˜(ì¿¼í„° ì£¼ì˜)", 1, 5, 1)
else:
    st.subheader("ì „ì²´ íŠ¸ë Œë“œ(ë‰´ìŠ¤Â·ì •ì¹˜)")
    region_code = st.selectbox("ì§€ì—­(Region)", ["KR", "US", "JP", "TW", "VN", "TH", "DE", "FR", "GB", "BR"], index=0)
    trend_pages = st.slider("íŠ¸ë Œë“œ í˜ì´ì§€ ìˆ˜(Ã—50ê°œ)", 1, 5, 1)

    # (UI ìˆ¨ê¹€) í‚¤ì›Œë“œ ê¸ˆì§€ì–´ ì„¹ì…˜ ì œê±° â†’ ê¸°ë³¸ê°’ ì‚¬ìš©
user_patterns = COMMON_BANNED_PAT
user_stops = COMMON_STOPWORDS

# ---------------------------------------------------------
# ë³¸ë¬¸: ì‹¤í–‰/ìˆ˜ì§‘
# ---------------------------------------------------------
now_utc = dt.datetime.now(dt.timezone.utc)
published_after_utc = (now_utc - dt.timedelta(hours=24)).isoformat()
go = st.button("ìˆ˜ì§‘/ê°±ì‹  ì‹¤í–‰", type="primary")

# ë“±ë¡ ì±„ë„ ì…ë ¥(í•„ìš” ì‹œ)
channel_inputs: List[str] = []
if data_source == "ë“±ë¡ ì±„ë„ ë­í‚¹":
    if mode == "ìˆ˜ë™ ì…ë ¥":
        st.markdown("**ì±„ë„ ì…ë ¥**: ì±„ë„ID(UCâ€¦), URL, @handle, ì±„ë„ëª… í—ˆìš©. ì‰¼í‘œ/ì¤„ë°”ê¿ˆ êµ¬ë¶„")
        manual_text = st.text_area(
            "ì±„ë„ ëª©ë¡",
            placeholder="@KBSNEWS, https://www.youtube.com/@jtbcnews, UCxxxxxxxxxxxxxxxxxxxx",
            height=240,
        )
        channel_inputs = parse_channel_input(manual_text) if manual_text else []
    else:
        up = st.file_uploader(
            "CSV/XLSX ì—…ë¡œë“œ(ì»¬ëŸ¼: channel_id ë˜ëŠ” url ë˜ëŠ” handle)", type=["csv", "xlsx"], key="channels_csv"
        )
        if up:
            try:
                if up.name.lower().endswith(".csv"):
                    df_up = pd.read_csv(up)
                else:
                    df_up = pd.read_excel(up)
                cols = [c.lower() for c in df_up.columns]
                if "channel_id" in cols:
                    channel_inputs = [str(x) for x in df_up["channel_id"].dropna().tolist()]
                elif "url" in cols:
                    channel_inputs = [str(x) for x in df_up["url"].dropna().tolist()]
                elif "handle" in cols:
                    channel_inputs = [f"@{str(x).lstrip('@')}" for x in df_up["handle"].dropna().tolist()]
                else:
                    st.warning("íŒŒì¼ì— channel_id / url / handle ì»¬ëŸ¼ ì¤‘ í•˜ë‚˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            except Exception as e:
                st.warning(f"ì±„ë„ ëª©ë¡ íŒŒì¼ íŒŒì‹± ì˜¤ë¥˜: {e}")

if go:
    if not YOUTUBE_API_KEY:
        st.error("YouTube API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. .streamlit/secrets.toml ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ì— ì„¤ì •í•˜ì„¸ìš”.")
        st.stop()

    rows: List[Dict] = []
    try:
        if data_source == "ì „ì²´ íŠ¸ë Œë“œ(ë‰´ìŠ¤Â·ì •ì¹˜)":
            url = f"{API_BASE}/videos"
            params = {
                "key": YOUTUBE_API_KEY,
                "part": "snippet,contentDetails,statistics",
                "chart": "mostPopular",
                "videoCategoryId": "25",   # ë‰´ìŠ¤Â·ì •ì¹˜
                "regionCode": region_code,
                "maxResults": 50,
            }
            video_ids = []
            for _ in range(trend_pages):
                r = requests.get(url, params=params, timeout=20)
                get_quota().add("videos.list")
                if r.status_code != 200:
                    break
                data = r.json()
                for it in data.get("items", []):
                    vid = it["id"]
                    cd = it.get("contentDetails", {})
                    sp = it.get("snippet", {}) or {}
                    stt = it.get("statistics", {}) or {}
        
                    dur = iso8601_to_seconds(cd.get("duration", "PT0S"))
                    if dur > 120:   # Shorts 120ì´ˆ ì´í•˜
                        continue
                    pub = sp.get("publishedAt")
                    if not pub:
                        continue
                    pub_dt = dt.datetime.fromisoformat(pub.replace("Z", "+00:00"))
                    #if pub_dt < dt.datetime.fromisoformat(published_after_utc):
                        #continue
        
                    views = int(stt.get("viewCount", 0) or 0)
                    likes = int(stt.get("likeCount", 0) or 0)
                    comments = int(stt.get("commentCount", 0) or 0)
                    hours = max((now_utc - pub_dt).total_seconds() / 3600, 1 / 60)
                    vph = views / hours
                    pub_kst = pub_dt.astimezone(KST)
        
                    rows.append(
                        {
                            "video_id": vid,
                            "title": sp.get("title", ""),
                            "description": sp.get("description", ""),
                            "channel": sp.get("channelTitle", ""),
                            "length_sec": dur,
                            "length_mmss": sec_to_mmss(dur),
                            "view_count": views,
                            "like_count": likes,
                            "comment_count": comments,
                            "views_per_hour": vph,
                            "published_at_kst": pub_kst.strftime("%Y-%m-%d %H:%M:%S"),
                            "url": f"https://www.youtube.com/watch?v={vid}",
                        }
                    )
                token = data.get("nextPageToken")
                if not token:
                    break
                params["pageToken"] = token

        elif data_source == "ì „ì—­ í‚¤ì›Œë“œ ê²€ìƒ‰":
            if not global_query:
                st.warning("ì „ì—­ ê²€ìƒ‰ ëª¨ë“œì—ì„œëŠ” ê²€ìƒ‰ì–´ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            else:
                ids = global_search_recent(global_query, published_after_utc, max_pages=max_pages)
                details = fetch_videos_details(ids)
                for vid, it in details.items():
                    cd = it.get("contentDetails", {})
                    sp = it.get("snippet", {}) or {}
                    stt = it.get("statistics", {}) or {}
                    dur = iso8601_to_seconds(cd.get("duration", "PT0S"))
                    if dur > 180:
                        continue
                    pub = sp.get("publishedAt")
                    if not pub:
                        continue
                    pub_dt = dt.datetime.fromisoformat(pub.replace("Z", "+00:00"))
                    ch_id = sp.get("channelId", "")

                    views = int(stt.get("viewCount", 0) or 0)
                    likes = int(stt.get("likeCount", 0) or 0)
                    comments = int(stt.get("commentCount", 0) or 0)
                    hours = max((now_utc - pub_dt).total_seconds() / 3600, 1 / 60)
                    vph = views / hours
                    pub_kst = pub_dt.astimezone(KST)

                    rows.append(
                        {
                            "video_id": vid,
                            "title": sp.get("title", ""),
                            "description": sp.get("description", ""),
                            "channel": sp.get("channelTitle", ""),
                            "length_sec": dur,
                            "length_mmss": sec_to_mmss(dur),
                            "view_count": views,
                            "like_count": likes,
                            "comment_count": comments,
                            "views_per_hour": vph,
                            "published_at_kst": pub_kst.strftime("%Y-%m-%d %H:%M:%S"),
                            "url": f"https://www.youtube.com/watch?v={vid}",
                        }
                    )

        else:  # ë“±ë¡ ì±„ë„ ë­í‚¹
            if not channel_inputs:
                st.warning("ì±„ë„ì„ ì…ë ¥(ë˜ëŠ” ì—…ë¡œë“œ)í•˜ì„¸ìš”.")
            else:
                def channel_title_and_uploads(cid: str):
                    r = requests.get(
                        f"{API_BASE}/channels",
                        params={"key": YOUTUBE_API_KEY, "id": cid, "part": "snippet,contentDetails"},
                        timeout=15,
                    )
                    get_quota().add("channels.list")
                    if r.status_code == 200:
                        items = r.json().get("items", [])
                        if items:
                            return (
                                items[0]["snippet"]["title"],
                                items[0]["contentDetails"]["relatedPlaylists"]["uploads"],
                            )
                    return None, None

                ch_ids: List[str] = []
                for token in channel_inputs:
                    cid = extract_channel_id(token)
                    if cid:
                        ch_ids.append(cid)
                    else:
                        st.warning(f"ì±„ë„ í•´ì„ ì‹¤íŒ¨: {token}")
                ch_ids = list(dict.fromkeys(ch_ids))

                all_video_pairs: List[Tuple[str, str]] = []
                ch_name_map: Dict[str, str] = {}
                for cid in ch_ids:
                    try:
                        ch_title, pid = channel_title_and_uploads(cid)
                        if not pid:
                            st.warning(f"ì—…ë¡œë“œ ëª©ë¡ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {cid}")
                            continue
                        ch_name_map[cid] = ch_title or cid
                        pairs = playlist_recent_video_ids(pid, published_after_utc)
                        all_video_pairs.extend(pairs)
                    except Exception as e:
                        st.warning(f"ì±„ë„ ì²˜ë¦¬ ê²½ê³ ({cid}): {e}")

                details = fetch_videos_details([v for v, _ in all_video_pairs])
                for vid, pub in all_video_pairs:
                    it = details.get(vid)
                    if not it:
                        continue
                    cd = it.get("contentDetails", {})
                    sp = it.get("snippet", {}) or {}
                    stt = it.get("statistics", {}) or {}
                    dur = iso8601_to_seconds(cd.get("duration", "PT0S"))
                    if dur > 180:
                        continue
                    ch_id = sp.get("channelId", "")
                    wl = st.session_state.get("whitelist_ids", set())
                    if wl and ch_id not in wl:
                        continue
                    title = sp.get("title", "")
                    desc = sp.get("description", "")
                    ch = sp.get("channelTitle", "") or ch_name_map.get(ch_id, "")
                    pub_kst = dt.datetime.fromisoformat(pub.replace("Z", "+00:00")).astimezone(KST)
                    views = int(stt.get("viewCount", 0) or 0)
                    likes = int(stt.get("likeCount", 0) or 0)
                    comments = int(stt.get("commentCount", 0) or 0)
                    hours = max((now_utc - dt.datetime.fromisoformat(pub.replace("Z", "+00:00"))).total_seconds() / 3600, 1 / 60)
                    vph = views / hours
                    rows.append(
                        {
                            "video_id": vid,
                            "title": title,
                            "description": desc,
                            "channel": ch,
                            "length_sec": dur,
                            "length_mmss": sec_to_mmss(dur),
                            "view_count": views,
                            "like_count": likes,
                            "comment_count": comments,
                            "views_per_hour": vph,
                            "published_at_kst": pub_kst.strftime("%Y-%m-%d %H:%M:%S"),
                            "url": f"https://www.youtube.com/watch?v={vid}",
                        }
                    )

        # ---- ì¶œë ¥/CSV/í‚¤ì›Œë“œ ----

        df = pd.DataFrame(rows)
        if df.empty:
            st.info("ì¡°ê±´ì— ë§ëŠ” 24ì‹œê°„ ë‚´ Shorts ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            sort_by = metric if metric in df.columns else "view_count"
            df_sorted = df.sort_values(by=sort_by, ascending=ascending, kind="mergesort").reset_index(drop=True)
            df_top = df_sorted.head(20)

            # ---- ë“±ë¡ ì±„ë„ ì „ì²´ ì¤‘ ì¡°íšŒìˆ˜ Top N ----
            if data_source == "ë“±ë¡ ì±„ë„ ë­í‚¹" and not df.empty:
                st.subheader("ë“±ë¡ ì±„ë„ ì „ì²´ ì¡°íšŒìˆ˜ Top ì˜ìƒ")
                df_views = df.sort_values(by="view_count", ascending=False).reset_index(drop=True)
                df_top_views = df_views.head(20)   # Top 20
                
                show_cols_views = [c for c in COL_ORDER if c in df_top_views.columns] + ["url"]
                st.dataframe(df_top_views[show_cols_views], use_container_width=True)
    
                # CSV ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
                csv_buf2 = io.StringIO()
                df_top_views[show_cols_views].to_csv(csv_buf2, index=False, encoding="utf-8-sig")
                st.download_button(
                    "ë“±ë¡ ì±„ë„ Top20 ì¡°íšŒìˆ˜ CSV ë‹¤ìš´ë¡œë“œ",
                    data=csv_buf2.getvalue().encode("utf-8-sig"),
                    file_name="registered_channels_top20_views.csv",
                    mime="text/csv",
                )

            
            # ===== í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ì „ì²´ ì˜ìƒ ì¤‘ ì¡°íšŒìˆ˜ Top N =====
#            st.subheader("í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ì „ì²´ ì¡°íšŒìˆ˜ Top ì˜ìƒ")
 #           df_views = df.sort_values(by="view_count", ascending=False).reset_index(drop=True)
  #          df_top_views = df_views.head(20)
   # 
    #        show_cols_views = [c for c in COL_ORDER if c in df_top_views.columns] + ["url"]
     #       st.dataframe(df_top_views[show_cols_views], use_container_width=True)
    
            # CSV ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
            #csv_buf2 = io.StringIO()
            #df_top_views[show_cols_views].to_csv(csv_buf2, index=False, encoding="utf-8-sig")
           # st.download_button(
             #   "í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ Top20 ì¡°íšŒìˆ˜ CSV ë‹¤ìš´ë¡œë“œ",
            #    data=csv_buf2.getvalue().encode("utf-8-sig"),
           #     file_name="whitelist_top20_views.csv",
            #    mime="text/csv",
           # )
    
            # ===== ê¸°ì¡´ Top 20 ë­í‚¹ =====
            show_cols = [c for c in COL_ORDER if c in df_top.columns]
            st.subheader("Top 20 ë­í‚¹")
            st.dataframe(df_top[show_cols], use_container_width=True)
    
            # ì˜ìƒ Top20 CSV (ë§í¬ í¬í•¨)
            csv_buf = io.StringIO()
            out_cols = [
                "title",
                "view_count",
                "length_mmss",
                "channel",
                "like_count",
                "comment_count",
                "published_at_kst",
                "url",
            ]
            df_top[out_cols].to_csv(csv_buf, index=False, encoding="utf-8-sig")
            st.download_button(
                "ì˜ìƒ Top20 CSV ë‹¤ìš´ë¡œë“œ",
                data=csv_buf.getvalue().encode("utf-8-sig"),
                file_name="shorts_top20.csv",
                mime="text/csv",
            )
    
            # ===== í‚¤ì›Œë“œ Top20 =====
            kw_df = aggregate_keywords(
                rows=df_top.to_dict(orient="records"),
                banned_patterns=user_patterns,
                banned_words=user_stops,
            )
            st.subheader("í‚¤ì›Œë“œ Top20")
            st.dataframe(kw_df, use_container_width=True)
            kw_buf = io.StringIO()
            kw_df.to_csv(kw_buf, index=False, encoding="utf-8-sig")
            st.download_button(
                "í‚¤ì›Œë“œ Top20 CSV ë‹¤ìš´ë¡œë“œ",
                data=kw_buf.getvalue().encode("utf-8-sig"),
                file_name="keywords_top20.csv",
                mime="text/csv",
            )
    
            # ===== í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ í‚¤ì›Œë“œ ë­í‚¹ =====
            if data_source == "ë“±ë¡ ì±„ë„ ë­í‚¹":
                st.subheader("í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ í‚¤ì›Œë“œ ë­í‚¹ (24h, ì¡°íšŒìˆ˜ ì˜¤ë¦„ì°¨ìˆœ)")
                kw_rank_df = build_keyword_ranking(rows, user_patterns, user_stops, top_k=300)
                if kw_rank_df.empty:
                    st.info("í‚¤ì›Œë“œê°€ ì¶”ì¶œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                else:
                    st.dataframe(kw_rank_df, use_container_width=True)
                    kwr_buf = io.StringIO()
                    kw_rank_df.to_csv(kwr_buf, index=False, encoding="utf-8-sig")
                    st.download_button(
                        "í‚¤ì›Œë“œ ë­í‚¹ CSV ë‹¤ìš´ë¡œë“œ",
                        data=kwr_buf.getvalue().encode("utf-8-sig"),
                        file_name="keyword_ranking_24h.csv",
                        mime="text/csv",
                    )
    
    except Exception as e:
        st.warning(f"ì‹¤í–‰ ë„ì¤‘ ê²½ê³ : {e}")


# ---------------------------------------------------------
# ì¿¼í„° í˜„í™©(ì„¸ì…˜ ì¶”ì •)
# ---------------------------------------------------------
st.divider()
st.subheader("API ì¿¼í„° í˜„í™©(ì„¸ì…˜ ì¶”ì •)")
quota = get_quota()
reset = QuotaMeter.next_reset_pt()
now_pt = dt.datetime.now(PT)
remain_sec = max(0, int((reset - now_pt).total_seconds()))
h, m, s = remain_sec // 3600, (remain_sec % 3600) // 60, remain_sec % 60

used = int(quota.used_units)
budget = int(quota.daily_budget)
remaining = max(0, budget - used)
pct = 0 if budget == 0 else min(used / budget, 1.0)

c1, c2, c3 = st.columns([3,2,2])
with c1:
    st.write("ì‚¬ìš©ëŸ‰")
    st.progress(pct, text=f"{used:,} / {budget:,}U  ({pct*100:.1f}%)")
with c2:
    st.metric("ë‚¨ì€ëŸ‰", f"{remaining:,}U")
with c3:
    st.metric("ë¦¬ì…‹ê¹Œì§€", f"{h:02d}:{m:02d}:{s:02d}")

st.caption("â€» ì‹¤ì œ ì¿¼í„°ëŠ” Google Cloud Console ê¸°ì¤€ì´ë©°, ì´ ê°’ì€ ì„¸ì…˜ ë‚´ ì¶”ì •ì¹˜ì…ë‹ˆë‹¤.")
st.caption("Â© v4 Â· Shorts ì „ìš©, 24ì‹œê°„ ë‚´ ì—…ë¡œë“œ, ìºì‹œ TTL=1h, ì˜¤ë¥˜ ì‹œ ê²½ê³ ë§Œ ì¶œë ¥")
