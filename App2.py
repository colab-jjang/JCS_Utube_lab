# -*- coding: utf-8 -*-
# ğŸ“º 48ì‹œê°„ ìœ íŠœë¸Œ ìˆì¸  íŠ¸ë Œë“œ ëŒ€ì‹œë³´ë“œ (ì •ì¹˜Â·ë‰´ìŠ¤)
import streamlit as st
import pandas as pd
import requests, re, json, time
from collections import Counter
from pathlib import Path
import datetime as dt
from zoneinfo import ZoneInfo
import xml.etree.ElementTree as ET
from bs4 import BeautifulSoup

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê¸°ë³¸ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="K-Politics/News Shorts Trend Board", page_icon="ğŸ“º", layout="wide")

API_KEY = st.secrets.get("YOUTUBE_API_KEY", "")
if not API_KEY:
    st.error("âš ï¸ API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. App â†’ Settings â†’ Secrets ì— `YOUTUBE_API_KEY = \"ë°œê¸‰í‚¤\"` ë¥¼ ë„£ì–´ì£¼ì„¸ìš”.")
    st.stop()

KST = ZoneInfo("Asia/Seoul")
PT  = ZoneInfo("America/Los_Angeles")
SEARCH_URL = "https://www.googleapis.com/youtube/v3/search"
VIDEOS_URL = "https://www.googleapis.com/youtube/v3/videos"
DAILY_QUOTA = 10_000

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì¿¼í„°(ì¼ì¼) ì˜êµ¬ ëˆ„ì  ì €ì¥: íŒŒì¼ ë°©ì‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€
DATA_DIR   = Path(".")
QUOTA_FILE = DATA_DIR / "quota_usage.json"

def _today_pt_str():
    now_pt = dt.datetime.now(PT)
    return now_pt.strftime("%Y-%m-%d")

def load_quota_used():
    today = _today_pt_str()
    if QUOTA_FILE.exists():
        try:
            data = json.loads(QUOTA_FILE.read_text(encoding="utf-8"))
            if data.get("pt_date") == today:
                return int(data.get("used", 0))
        except Exception:
            pass
    return 0

def save_quota_used(value: int):
    data = {"pt_date": _today_pt_str(), "used": int(value)}
    QUOTA_FILE.write_text(json.dumps(data), encoding="utf-8")

def add_quota(cost: int):
    # ìœ íš¨/ë¬´íš¨ í˜¸ì¶œ ëª¨ë‘ ë¹„ìš© ë°œìƒ(ë³´ìˆ˜ì  ì¶”ì •)
    used = st.session_state.get("quota_used", 0) + int(cost)
    st.session_state["quota_used"] = used
    # íŒŒì¼ ë™ê¸°í™”
    cur = load_quota_used()
    save_quota_used(cur + int(cost))

# ì„¸ì…˜ ì´ˆê¸°í™”(íŒŒì¼ê³¼ ë™ê¸°í™”)
if "quota_used" not in st.session_state:
    st.session_state["quota_used"] = load_quota_used()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì‹œê°„ì°½: ìµœê·¼ 48ì‹œê°„(KST) â”€â”€â”€â”€â”€â”€â”€â”€â”€
def kst_window_last_48h():
    now_kst = dt.datetime.now(KST)
    start_kst = now_kst - dt.timedelta(hours=48)
    start_utc = start_kst.astimezone(dt.timezone.utc).isoformat().replace("+00:00", "Z")
    end_utc   = now_kst.astimezone(dt.timezone.utc).isoformat().replace("+00:00", "Z")
    return start_utc, end_utc, now_kst

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ YouTube API helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€
def api_get(url, params, cost):
    r = requests.get(url, params=params, timeout=20)
    add_quota(cost)
    r.raise_for_status()
    return r.json()

def parse_iso8601_duration(s):
    if not s or not s.startswith("PT"): return None
    s2 = s[2:]; h=m=sec=0; num=""
    for ch in s2:
        if ch.isdigit(): num += ch
        else:
            if ch=="H": h=int(num or 0)
            elif ch=="M": m=int(num or 0)
            elif ch=="S": sec=int(num or 0)
            num=""
    return h*3600 + m*60 + sec

def fmt_hms(seconds):
    if seconds is None: return ""
    h = seconds//3600; m=(seconds%3600)//60; s=seconds%60
    return f"{h:02d}:{m:02d}:{s:02d}" if h>0 else f"{m:02d}:{s:02d}"

def to_kst(iso_str):
    if not iso_str: return ""
    t = dt.datetime.fromisoformat(iso_str.replace("Z","+00:00")).astimezone(KST)
    return t.strftime("%Y-%m-%d %H:%M:%S")

def to_kst_dt(iso_str):
    return dt.datetime.fromisoformat(iso_str.replace("Z","+00:00")).astimezone(KST) if iso_str else None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë°ì´í„° ìˆ˜ì§‘ (ìºì‹œ) â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_data(show_spinner=False)
def fetch_shorts_df(pages:int=1, bucket:int=0):
    """pages: 1â‰ˆ50, 2â‰ˆ100, 4â‰ˆ200 / bucket: TTL ë¶„ë¦¬ìš© í‚¤"""
    _ = bucket  # ìºì‹œ í‚¤ë¡œë§Œ ì‚¬ìš©
    start_iso, end_iso, _now = kst_window_last_48h()

    # 1) ID ìˆ˜ì§‘ (search.list = 100/í˜¸ì¶œ)
    vids, token = [], None
    for _ in range(pages):
        params = {
            "key": API_KEY, "part": "snippet", "type":"video", "order":"date",
            "publishedAfter": start_iso, "publishedBefore": end_iso,
            "maxResults": 50, "videoDuration":"short",
            "regionCode":"KR", "relevanceLanguage":"ko", "safeSearch":"moderate",
            "q": "ë‰´ìŠ¤ OR ì •ì¹˜ OR ì†ë³´ OR ë¸Œë¦¬í•‘"
        }
        if token: params["pageToken"] = token
        data = api_get(SEARCH_URL, params, cost=100)
        ids = [it.get("id",{}).get("videoId") for it in data.get("items",[]) if it.get("id",{}).get("videoId")]
        vids.extend(ids)
        token = data.get("nextPageToken")
        if not token:
            break

    # de-dup
    seen=set(); ordered=[]
    for v in vids:
        if v not in seen: ordered.append(v); seen.add(v)

    # 2) ìƒì„¸ (videos.list = 1/í˜¸ì¶œ, 50ê°œì”©)
    details=[]
    for i in range(0, len(ordered), 50):
        chunk = ordered[i:i+50]
        if not chunk: continue
        params = {"key": API_KEY, "part": "snippet,contentDetails,statistics", "id": ",".join(chunk)}
        data = api_get(VIDEOS_URL, params, cost=1)
        details.extend(data.get("items", []))

    # 3) DF ë§Œë“¤ê¸° (Shortsë§Œ)
    rows=[]
    for it in details:
        vid = it.get("id")
        sn  = it.get("snippet", {})
        cd  = it.get("contentDetails", {})
        stt = it.get("statistics", {})
        secs = parse_iso8601_duration(cd.get("duration",""))
        if secs is None or secs>60:
            continue
        pub_iso = sn.get("publishedAt","")
        rows.append({
            "video_id": vid,
            "title": sn.get("title",""),
            "description": sn.get("description",""),
            "view_count": int(pd.to_numeric(stt.get("viewCount","0"), errors="coerce") or 0),
            "length": fmt_hms(secs),
            "length_seconds": secs,
            "channel": sn.get("channelTitle",""),
            "url": f"https://www.youtube.com/watch?v={vid}",
            "published_at_kst": to_kst(pub_iso),
            "published_dt_kst": to_kst_dt(pub_iso),
        })
    df = pd.DataFrame(rows, columns=[
        "video_id","title","description","view_count","length","length_seconds",
        "channel","url","published_at_kst","published_dt_kst"
    ])

    # â”€ ê¸‰ìƒìŠ¹ ì§€í‘œ ê³„ì‚°
    now_kst = dt.datetime.now(KST)
    if not df.empty:
        df["hours_since_upload"] = (now_kst - pd.to_datetime(df["published_dt_kst"])).dt.total_seconds() / 3600.0
        df["hours_since_upload"] = df["hours_since_upload"].clip(lower=(1.0/60.0))  # ìµœì†Œ 1ë¶„
        df["views_per_hour"] = (df["view_count"] / df["hours_since_upload"]).round(1)
    else:
        df["hours_since_upload"] = []
        df["views_per_hour"] = []
    return df

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ í‚¤ì›Œë“œ ì¶”ì¶œ â”€â”€â”€â”€â”€â”€â”€â”€â”€
STOPWORDS = set("""
ê·¸ë¦¬ê³  ê·¸ëŸ¬ë‚˜ ê·¸ë˜ì„œ ë˜í•œ ë˜ëŠ” ë° ë¨¼ì € ì§€ê¸ˆ ë°”ë¡œ ë§¤ìš° ì •ë§ ê·¸ëƒ¥ ë„ˆë¬´ ë³´ë‹¤ ë³´ë‹¤ë„ ë•ŒëŠ” ë¼ëŠ” ì´ëŸ° ì €ëŸ° ê·¸ëŸ°
í•©ë‹ˆë‹¤ í–ˆë‹¤ í–ˆë‹¤ê°€ í•˜ëŠ” í•˜ê³  í•˜ë©° í•˜ë©´ ëŒ€í•œ ìœ„í•´ ì—ì„œ ì—ê²Œ ì—ë„ ì—ëŠ” ìœ¼ë¡œ ë¡œ ë¥¼ ì€ ëŠ” ì´ ê°€ ë„ ì˜ ì— ì™€ ê³¼
""".split())
STOPWORDS |= {"ì†ë³´","ë¸Œë¦¬í•‘","ë‹¨ë…","í˜„ì¥","ì˜ìƒ","ë‰´ìŠ¤","ê¸°ì","ë¦¬í¬íŠ¸","ë¼ì´ë¸Œ","ì—°í•©ë‰´ìŠ¤","ì±„ë„","êµ¬ë…","ëŒ€í†µë ¹","ìœ íŠœë¸Œ","ì •ì¹˜","í™ˆí˜ì´ì§€","ëŒ€í•œë¯¼êµ­","ê¸ˆì§€","ì‹œì‚¬","ëª¨ì•„","ë‹µí•´ì£¼ì„¸ìš”"}
STOPWORDS |= {"http","https","www","com","co","kr","net","org",
              "youtu","youtube","be","shorts","watch","tv",
              "news","live","breaking","official","channel",
              "video","clip","yonhap","yonhapnews"}

KO_JOSA   = ("ì€","ëŠ”","ì´","ê°€","ì„","ë¥¼","ì˜","ì—","ì—ì„œ","ì—ê²Œ","ê»˜","ì™€","ê³¼","ìœ¼ë¡œ","ë¡œ","ë„","ë§Œ","ê¹Œì§€","ë¶€í„°","ë§ˆë‹¤","ì¡°ì°¨","ë¼ë„","ë§ˆì €","ë°–ì—","ì²˜ëŸ¼","ë¿","ê»˜ì„œ")
KO_SUFFIX = ("í•˜ê¸°","í•˜ì„¸ìš”","í•˜ì‹­ì‹œì˜¤","í•´ì£¼ì„¸ìš”","í•©ë‹ˆë‹¤","í–ˆë‹¤","ì¤‘","ê´€ë ¨","ì˜ìƒ","ì±„ë„","ë‰´ìŠ¤","ë³´ê¸°","ë“±ë¡","êµ¬ë…","í™ˆí˜ì´ì§€","ë©ë‹ˆë‹¤")

def strip_korean_suffixes(t: str) -> str:
    for suf in KO_SUFFIX:
        if t.endswith(suf) and len(t) > len(suf)+1:
            t = t[:-len(suf)]
    for j in KO_JOSA:
        if t.endswith(j) and len(t) > len(j)+1:
            t = t[:-len(j)]
    return t

def tokenize_ko_en(text: str):
    text = str(text or "")
    text = re.sub(r"https?://\S+", " ", text)
    text = re.sub(r"www\.\S+", " ", text)
    text = re.sub(r"\S+@\S+", " ", text)
    text = re.sub(r"[#@_/\\]", " ", text)

    raw = re.findall(r"[0-9A-Za-zê°€-í£]+", text.lower())
    out = []
    for t in raw:
        if not t or t.isdigit(): 
            continue

        if re.fullmatch(r"[ê°€-í£]+", t):
            t = strip_korean_suffixes(t)

        if t in STOPWORDS or len(t) < 2:
            continue

        if re.fullmatch(r"[a-z]+", t) and len(t) <= 2:
            continue

        if t in STOPWORDS or len(t) < 2:
            continue

        if t.endswith("tv") and len(t) > 2:
            t = t[:-2]
            if t in STOPWORDS or len(t) < 2:
                continue

        out.append(t)
    return out

def top_keywords_from_df(df: pd.DataFrame, topk:int=10):
    corpus = (df["title"].fillna("") + " " + df["description"].fillna("")).tolist()
    cnt = Counter()
    for line in corpus:
        cnt.update(tokenize_ko_en(line))
    items = [(w,c) for w,c in cnt.most_common() if not re.fullmatch(r"\d+", w)]
    return items[:topk]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ íŠ¸ë Œë“œ ì†ŒìŠ¤ (êµ¬ê¸€/ë„¤ì´ë²„/ìœ íŠœë¸Œ) â”€â”€â”€â”€â”€â”€â”€â”€â”€
TREND_STOPWORDS = {
    "https","http","www","com","co","kr","net","org","youtube","shorts","watch",
    "ì±„ë„","êµ¬ë…","ë‰´ìŠ¤","ë¼ì´ë¸Œ","ë¸Œë¦¬í•‘","ì†ë³´","ì˜ìƒ","ê¸°ì","ë¦¬í¬íŠ¸","ê³µì‹",
    "yonhap","yonhapnews","ì—°í•©ë‰´ìŠ¤","tv","ì „ë¬¸","ì‹œì‘","ì „ë¬¸ê°€","ë„¤ì´ë²„","í™ˆí˜ì´ì§€"
}

def _clean_words(words):
    out = []
    for w in words:
        w = (w or "").strip()
        if not w: 
            continue
        w_low = w.lower()
        if w_low in TREND_STOPWORDS:
            continue
        if re.fullmatch(r"[a-z]+", w_low) and len(w_low) <= 2:
            continue
        out.append(w)
    seen=set(); uniq=[]
    for x in out:
        if x not in seen:
            uniq.append(x); seen.add(x)
    return uniq[:10]

@st.cache_data(show_spinner=False, ttl=900)
def google_trends_top(source_mode: str = "auto"):
    headers = {
        "User-Agent": "Mozilla/5.0",
        "Accept": "application/json,text/plain,*/*",
        "Referer": "https://trends.google.com/",
        "Accept-Language": "ko-KR,ko;q=0.9,en;q=0.8",
    }
    bases = ["https://trends.google.com", "https://trends.google.co.kr"]

    def _google_daily():
        for base in bases:
            try:
                url = f"{base}/trends/api/dailytrends"
                r = requests.get(url, headers=headers,
                                 params={"hl":"ko","tz":"540","geo":"KR"},
                                 timeout=15, allow_redirects=True)
                r.raise_for_status()
                data = json.loads(r.text.lstrip(")]}',\n "))
                days = data.get("default", {}).get("trendingSearchesDays", [])
                if not days: 
                    continue
                items = days[0].get("trendingSearches", [])
                kws = [it.get("title", {}).get("query", "") for it in items if it.get("title")]
                return _clean_words(kws), "google-daily"
            except Exception:
                continue
        return [], "none"

    def _google_realtime():
        for base in bases:
            try:
                url = f"{base}/trends/api/realtimetrends"
                r = requests.get(
                    url, headers=headers,
                    params={"hl":"ko","tz":"540","cat":"all","fi":0,"fs":0,"geo":"KR","ri":300,"rs":20},
                    timeout=15, allow_redirects=True
                )
                r.raise_for_status()
                data = json.loads(r.text.lstrip(")]}',\n "))
                stories = data.get("storySummaries", {}).get("trendingStories", [])
                kws = []
                for s in stories:
                    for e in s.get("entityNames", []):
                        e = (e or "").strip()
                        if e and e not in kws:
                            kws.append(e)
                return _clean_words(kws), "google-realtime"
            except Exception:
                continue
        return [], "none"

    def _google_rss():
        for base in bases:
            try:
                url = f"{base}/trends/trendingsearches/daily/rss?geo=KR&hl=ko"
                r = requests.get(url, headers={"User-Agent": headers["User-Agent"], "Accept":"application/rss+xml"},
                                 timeout=15, allow_redirects=True)
                r.raise_for_status()
                root = ET.fromstring(r.content)
                titles = []
                for item in root.findall(".//item"):
                    t = (item.findtext("title") or "").strip()
                    if t: titles.append(t)
                    if len(titles) >= 10: break
                return _clean_words(titles), "google-rss"
            except Exception:
                continue
        return [], "none"

def _naver_fallback():
    """
    ë„¤ì´ë²„ ì¸ê¸°/ì •ì¹˜ ë‰´ìŠ¤ ì—¬ëŸ¬ í˜ì´ì§€ë¥¼ ìˆœì°¨ ì‹œë„í•´ì„œ ì œëª©ì„ ìˆ˜ì§‘í•œë‹¤.
    í˜ì´ì§€ êµ¬ì¡° ë³€ê²½ì— ëŒ€ë¹„í•´ ë‹¤ì¤‘ ì…€ë ‰í„° ì‚¬ìš©.
    """
    headers = {"User-Agent": "Mozilla/5.0"}
    urls = [
        # ì¸ê¸° ë‰´ìŠ¤ (ì¼ê°„)
        "https://news.naver.com/main/ranking/popularDay.naver",
        # ì •ì¹˜ ì„¹ì…˜
        "https://news.naver.com/section/100",
        # ë©”ì¸ ë‰´ìŠ¤(í´ëŸ¬ìŠ¤í„°)
        "https://news.naver.com/",
    ]
    selectors = [
        # ì¸ê¸°ë‰´ìŠ¤ ë­í‚¹
        "ol.ranking_list a",
        "div.rankingnews_box a",
        # ì •ì¹˜ ì„¹ì…˜ ëª©ë¡/íƒ€ì´í‹€
        "ul.sa_list a.sa_text_title",
        "a.sa_text_title_link",
        # ë©”ì¸ í´ëŸ¬ìŠ¤í„°
        "a.cluster_text_headline",
        # ê·¸ ì™¸ ì¼ë°˜ íƒ€ì´í‹€
        "a[href*='/read?']",
    ]

    texts = []
    for url in urls:
        try:
            r = requests.get(url, headers=headers, timeout=12)
            r.raise_for_status()
            soup = BeautifulSoup(r.text, "html.parser")
            got = False
            for css in selectors:
                for a in soup.select(css):
                    t = a.get_text(" ", strip=True)
                    if t:
                        texts.append(t)
                        got = True
            if got:  # ì´ URLì—ì„œ ë­”ê°€ ì–»ì—ˆìœ¼ë©´ ë‹¤ìŒ URLì€ ìŠ¤í‚µ
                break
        except Exception:
            continue

    if not texts:
        return [], "none"

    # í† í°í™” & ì¹´ìš´íŠ¸
    cnt = Counter()
    for line in texts:
        toks = re.findall(r"[0-9A-Za-zê°€-í£]+", line.lower())
        for t in toks:
            if t.isdigit(): 
                continue
            if t in TREND_STOPWORDS or len(t) < 2:
                continue
            cnt[t] += 1

    cand = [w for w, _ in cnt.most_common(30)]
    return _clean_words(cand), "naver"

    # ëª¨ë“œë³„
    if source_mode == "google":
        for fn in (_google_realtime, _google_daily, _google_rss):
            kws, src = fn()
            if kws: return kws, src
        return [], "none"
    if source_mode == "naver":
        kws, src = _naver_fallback()
        return (kws, src) if kws else ([], "none")
    if source_mode == "youtube":
        return [], "youtube"

    # auto: google â†’ naver
    for fn in (_google_realtime, _google_daily, _google_rss):
        kws, src = fn()
        if kws: return kws, src
    kws, src = _naver_fallback()
    if kws: return kws, src
    return [], "none"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ UI â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.title("ğŸ“º 48ì‹œê°„ ìœ íŠœë¸Œ ìˆì¸  íŠ¸ë Œë“œ ëŒ€ì‹œë³´ë“œ (ì •ì¹˜Â·ë‰´ìŠ¤)")

with st.sidebar:
    st.header("ìˆ˜ì§‘ ì˜µì…˜")
    size = st.selectbox("ìˆ˜ì§‘ ê·œëª¨(Shorts í›„ë³´ ìˆ˜)", [50, 100, 200], index=1)
    pages = {50:1, 100:2, 200:4}[size]

    ttl_choice = st.selectbox("ìºì‹œ TTL(ìë™ ì ˆì•½)", ["15ë¶„","30ë¶„(ì¶”ì²œ)","60ë¶„"], index=1)
    ttl_map = {"15ë¶„":900, "30ë¶„(ì¶”ì²œ)":1800, "60ë¶„":3600}
    ttl_sec = ttl_map[ttl_choice]

    rank_mode = st.radio("ì •ë ¬ ê¸°ì¤€", ["ìƒìŠ¹ì†ë„(ë·°/ì‹œê°„)", "ì¡°íšŒìˆ˜(ì´í•©)"], horizontal=True, index=0)
    sort_order = st.radio("ì •ë ¬ ìˆœì„œ", ["ë‚´ë¦¼ì°¨ìˆœ", "ì˜¤ë¦„ì°¨ìˆœ"], horizontal=True, index=0)
    show_speed_cols = st.checkbox("ìƒìŠ¹ì†ë„/ê²½ê³¼ì‹œê°„ ì»¬ëŸ¼ í‘œì‹œ", value=True)

    trend_source = st.radio(
        "íŠ¸ë Œë“œ ì†ŒìŠ¤ ì„ íƒ",
        ["ìë™(êµ¬ê¸€â†’ë„¤ì´ë²„)", "êµ¬ê¸€ë§Œ", "ë„¤ì´ë²„ë§Œ", "ìœ íŠœë¸Œë§Œ"],
        index=0
    )

    run = st.button("ìƒˆë¡œê³ ì¹¨(ë°ì´í„° ìˆ˜ì§‘)")

# ìºì‹œ êµ¬ë¶„ìš© ë²„í‚·
bucket = int(time.time() // ttl_sec)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë°ì´í„° ìˆ˜ì§‘ & ê°€ê³µ â”€â”€â”€â”€â”€â”€â”€â”€â”€
if run:
    st.cache_data.clear()
    st.success("ë°ì´í„° ìƒˆë¡œê³ ì¹¨ ì‹œì‘!")

df = fetch_shorts_df(pages=pages, bucket=bucket)

base_col = "views_per_hour" if rank_mode.startswith("ìƒìŠ¹ì†ë„") else "view_count"
ascending_flag = (sort_order == "ì˜¤ë¦„ì°¨ìˆœ")

# ë¶„ì„ìš© ìƒìœ„ í’€ (ìµœì†Œ 50, ìµœëŒ€ df í¬ê¸°)
base_pool_n = max(50, len(df))
df_pool = df.sort_values(base_col, ascending=ascending_flag, ignore_index=True).head(base_pool_n)

# ìœ íŠœë¸Œ í‚¤ì›Œë“œ Top10
yt_kw = top_keywords_from_df(df_pool, topk=10)
yt_kw_words = [w for w, _ in yt_kw]

# íŠ¸ë Œë“œ ì†ŒìŠ¤ ëª¨ë“œ ë§¤í•‘
mode_map = {
    "ìë™(êµ¬ê¸€â†’ë„¤ì´ë²„)": "auto",
    "êµ¬ê¸€ë§Œ": "google",
    "ë„¤ì´ë²„ë§Œ": "naver",
    "ìœ íŠœë¸Œë§Œ": "youtube",
}
source_mode = mode_map[trend_source]

# íŠ¸ë Œë“œ í‚¤ì›Œë“œ
g_kw, g_src = google_trends_top(source_mode=source_mode)
st.caption(f"íŠ¸ë Œë“œ ì†ŒìŠ¤: {g_src if g_kw else 'Unavailable'} Â· í‚¤ì›Œë“œ {len(g_kw)}ê°œ Â· ëª¨ë“œ={trend_source}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì¿¼í„°/ë¦¬ì…‹ ì •ë³´ â”€â”€â”€â”€â”€â”€â”€â”€â”€
now_pt = dt.datetime.now(PT)
reset_pt = (now_pt + dt.timedelta(days=1)).replace(hour=0,minute=0,second=0,microsecond=0)
remain_td = reset_pt - now_pt
used = st.session_state.get("quota_used", 0)
remaining = max(0, DAILY_QUOTA - used)
pct = min(1.0, used / DAILY_QUOTA)

quota1, quota2 = st.columns([2,1])
with quota1:
    st.subheader("ğŸ”‹ ì˜¤ëŠ˜ ì¿¼í„°(ì¶”ì •)")
    st.progress(pct, text=f"ì‚¬ìš© {used} / {DAILY_QUOTA}  (ë‚¨ì€ {remaining})")
with quota2:
    st.metric("ë‚¨ì€ ì¿¼í„°(ì¶”ì •)", value=f"{remaining:,}", delta=f"ë¦¬ì…‹ê¹Œì§€ {str(remain_td).split('.')[0]}")
st.caption("â€» YouTube Data API ì¼ì¼ ì¿¼í„°ëŠ” ë§¤ì¼ PT(ë¯¸êµ­ ì„œë¶€) ìì •ì— ë¦¬ì…‹ë©ë‹ˆë‹¤. (KST ê¸°ì¤€ ë‹¤ìŒë‚  16~17ì‹œ, ì„œë¨¸íƒ€ì„ ë”°ë¼ ë³€ë™)")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ìƒë‹¨ ë³´ë“œ: í‚¤ì›Œë“œ ë·° â”€â”€â”€â”€â”€â”€â”€â”€â”€
left, right = st.columns(2)
with left:
    st.subheader("ğŸ“ˆ ìœ íŠœë¸Œ(48hÂ·ìƒìœ„ í’€) í‚¤ì›Œë“œ Top10")
    if yt_kw:
        df_kw = pd.DataFrame(yt_kw, columns=["keyword","count"])
        df_kw_sorted = df_kw.sort_values("count", ascending=ascending_flag)
        st.bar_chart(df_kw_sorted.set_index("keyword")["count"])
        st.dataframe(df_kw_sorted, use_container_width=True, hide_index=True)
        st.download_button("ìœ íŠœë¸Œ í‚¤ì›Œë“œ CSV",
                           df_kw_sorted.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig"),
                           file_name="yt_keywords_top10.csv", mime="text/csv")
    else:
        st.info("í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•  ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ìˆ˜ì§‘ ê·œëª¨/í˜ì´ì§€ë¥¼ ëŠ˜ë ¤ë³´ì„¸ìš”.")

with right:
    st.subheader("ğŸŒ Trends Top10")
    if g_kw:
        df_g = pd.DataFrame({"keyword": g_kw})
        st.dataframe(df_g, use_container_width=True, hide_index=True)
        st.download_button("íŠ¸ë Œë“œ í‚¤ì›Œë“œ CSV",
                           df_g.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig"),
                           file_name="trends_top10.csv", mime="text/csv")
    else:
        st.info("ì„ íƒí•œ ì†ŒìŠ¤ì—ì„œ íŠ¸ë Œë“œ í‚¤ì›Œë“œë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (ëª¨ë“œë¥¼ ë°”ê¿”ë³´ì„¸ìš”)")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ êµì§‘í•©(ì–‘ìª½ ë‹¤ ëœ¨ëŠ” í‚¤ì›Œë“œ) â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _norm(s: str) -> str:
    s = s.lower().strip()
    s = re.sub(r"\s+", "", s)
    s = re.sub(r"[^\wê°€-í£]", "", s)
    return s

yt_norm = [_norm(w) for w in yt_kw_words]
g_norm  = [_norm(g) for g in g_kw]
hot = []
for raw_y, y in zip(yt_kw_words, yt_norm):
    for g in g_norm:
        if y and g and (y in g or g in y):
            hot.append(raw_y); break
_seen=set()
hot_intersection = [x for x in hot if not (x in _seen or _seen.add(x))]

st.subheader("ğŸ”¥ êµì§‘í•©(ë‘˜ ë‹¤ ëœ¨ëŠ” í‚¤ì›Œë“œ)")
st.write(", ".join(f"`{w}`" for w in hot_intersection) if hot_intersection else "í˜„ì¬ êµì§‘í•© í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ í•˜ë‹¨: ê²°ê³¼ í…Œì´ë¸” â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.subheader("ğŸ¬ ê´€ë ¨ ìˆì¸  ë¦¬ìŠ¤íŠ¸")
default_kw = (hot_intersection[0] if hot_intersection
              else (yt_kw_words[0] if yt_kw_words else ""))
pick_kw = st.text_input("í‚¤ì›Œë“œë¡œ í•„í„°(ë¶€ë¶„ ì¼ì¹˜)", value=default_kw)

df_show = df_pool.copy()
if pick_kw.strip():
    pat = re.compile(re.escape(pick_kw.strip()), re.IGNORECASE)
    mask = df_show["title"].str.contains(pat) | df_show["description"].str.contains(pat)
    df_show = df_show[mask]

cols = ["title","view_count","length","channel","url","published_at_kst"]
if show_speed_cols:
    cols = ["title","view_count","views_per_hour","hours_since_upload","length","channel","url","published_at_kst"]

df_show = df_show.sort_values(base_col, ascending=ascending_flag, ignore_index=True)[cols]

# í‘œ ìœ ì§€ìš©(ë‹¤ìš´ë¡œë“œ í›„ì—ë„ ê·¸ëŒ€ë¡œ ë³´ì´ê²Œ)
st.session_state["df_show_frozen"] = df_show.copy()
st.dataframe(st.session_state["df_show_frozen"], use_container_width=True)

csv_bytes = st.session_state["df_show_frozen"].to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig")
st.download_button("í˜„ì¬ í‘œ CSV ë‹¤ìš´ë¡œë“œ", data=csv_bytes, file_name="shorts_ranked.csv", mime="text/csv", key="dl_df_show")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì•ˆë‚´ â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.markdown("""
---
**ì°¸ê³ **
- ìœ íŠœë¸Œ APIëŠ” ì—…ë¡œë” êµ­ê°€ë¥¼ í™•ì • ì œê³µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë³¸ ì•±ì€ `regionCode=KR`, `relevanceLanguage=ko`ë¡œ í•œêµ­ ìš°ì„  ê²°ê³¼ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
- ì¿¼í„° ë¹„ìš©(ì¶”ì •): `search.list = 100/í˜¸ì¶œ`, `videos.list = 1/í˜¸ì¶œ(50ê°œ ë‹¨ìœ„)`. ìˆ˜ì§‘ ê·œëª¨ê°€ ì»¤ì§ˆìˆ˜ë¡ ë¹„ìš©ì´ ëŠ˜ì–´ë‚©ë‹ˆë‹¤.
- ìºì‹œ TTLì„ ê¸¸ê²Œ ì„¤ì •í•˜ë©´ ì¿¼í„° ì‚¬ìš©ëŸ‰ì„ í¬ê²Œ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- íŠ¸ë Œë“œ ì†ŒìŠ¤ëŠ” *êµ¬ê¸€(realtimeâ†’dailyâ†’rss)* ì‹¤íŒ¨ ì‹œ *ë„¤ì´ë²„ ì¸ê¸°ë‰´ìŠ¤*ë¡œ ìë™ ëŒ€ì²´(â€œìë™â€ ëª¨ë“œ)ë©ë‹ˆë‹¤.
""")
