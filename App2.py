# -*- coding: utf-8 -*-
import streamlit as st
import pandas as pd
import requests, re, json, time
from collections import Counter
from pathlib import Path
import datetime as dt
from zoneinfo import ZoneInfo

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê¸°ë³¸ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€
API_KEY = st.secrets.get("YOUTUBE_API_KEY", "")
KST = ZoneInfo("Asia/Seoul")
PT  = ZoneInfo("America/Los_Angeles")
SEARCH_URL = "https://www.googleapis.com/youtube/v3/search"
VIDEOS_URL = "https://www.googleapis.com/youtube/v3/videos"
DAILY_QUOTA = 10_000

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì¿¼í„°(ì¼ì¼) ì˜êµ¬ ëˆ„ì  ì €ì¥: íŒŒì¼ ë°©ì‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€
DATA_DIR   = Path(".")
QUOTA_FILE = DATA_DIR / "quota_usage.json"

def _today_pt_str():
    now_pt = dt.datetime.now(PT)
    return now_pt.strftime("%Y-%m-%d")

def load_quota_used():
    today = _today_pt_str()
    if QUOTA_FILE.exists():
        try:
            data = json.loads(QUOTA_FILE.read_text(encoding="utf-8"))
            if data.get("pt_date") == today:
                return int(data.get("used", 0))
        except Exception:
            pass
    return 0

def save_quota_used(value: int):
    data = {"pt_date": _today_pt_str(), "used": int(value)}
    QUOTA_FILE.write_text(json.dumps(data), encoding="utf-8")

def add_quota(cost: int):
    st.session_state["quota_used"] = st.session_state.get("quota_used", 0) + int(cost)
    current = load_quota_used()
    save_quota_used(current + int(cost))

# ì„¸ì…˜ ì´ˆê¸°í™”(íŒŒì¼ê³¼ ë™ê¸°í™”)
st.session_state["quota_used"] = load_quota_used()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì‹œê°„ì°½: ìµœê·¼ 48ì‹œê°„(KST) â”€â”€â”€â”€â”€â”€â”€â”€â”€
def kst_window_last_48h():
    now_kst = dt.datetime.now(KST)
    start_kst = now_kst - dt.timedelta(hours=48)
    start_utc = start_kst.astimezone(dt.timezone.utc).isoformat().replace("+00:00", "Z")
    end_utc   = now_kst.astimezone(dt.timezone.utc).isoformat().replace("+00:00", "Z")
    return start_utc, end_utc, now_kst

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ YouTube API helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€
def api_get(url, params, cost):
    r = requests.get(url, params=params, timeout=20)
    add_quota(cost)  # ìœ íš¨/ë¬´íš¨ í˜¸ì¶œ ëª¨ë‘ ë¹„ìš© ë°œìƒ ê·œì¹™ ë°˜ì˜
    r.raise_for_status()
    return r.json()

def parse_iso8601_duration(s):
    if not s or not s.startswith("PT"):
        return None
    s2 = s[2:]; h=m=sec=0; num=""
    for ch in s2:
        if ch.isdigit(): num += ch
        else:
            if ch=="H": h=int(num or 0)
            elif ch=="M": m=int(num or 0)
            elif ch=="S": sec=int(num or 0)
            num=""
    return h*3600 + m*60 + sec

def fmt_hms(seconds):
    if seconds is None: return ""
    h = seconds//3600; m=(seconds%3600)//60; s=seconds%60
    return f"{h:02d}:{m:02d}:{s:02d}" if h>0 else f"{m:02d}:{s:02d}"

def to_kst(iso_str):
    if not iso_str: return ""
    t = dt.datetime.fromisoformat(iso_str.replace("Z","+00:00")).astimezone(KST)
    return t.strftime("%Y-%m-%d %H:%M:%S")

def to_kst_dt(iso_str):
    # publishedAt(UTC ISO) â†’ timezone-aware KST datetime
    return dt.datetime.fromisoformat(iso_str.replace("Z","+00:00")).astimezone(KST) if iso_str else None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë°ì´í„° ìˆ˜ì§‘ (ìºì‹œ ê°€ëŠ¥) â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_data(show_spinner=False)
def fetch_shorts_df(pages:int=1):
    """pages: 1(â‰ˆ50ê°œ), 2(â‰ˆ100ê°œ), 4(â‰ˆ200ê°œ)"""
    start_iso, end_iso, _ = kst_window_last_48h()

    # 1) ID ìˆ˜ì§‘ (search.list = 100/í˜¸ì¶œ)
    vids, token, p = [], None, 0
    for _ in range(pages):
        params = {
            "key": API_KEY, "part": "snippet", "type":"video", "order":"date",
            "publishedAfter": start_iso, "publishedBefore": end_iso,
            "maxResults": 50, "videoDuration":"short",
            "regionCode":"KR", "relevanceLanguage":"ko", "safeSearch":"moderate",
        }
        if token: params["pageToken"] = token
        data = api_get(SEARCH_URL, params, cost=100)
        ids = [it.get("id",{}).get("videoId") for it in data.get("items",[]) if it.get("id",{}).get("videoId")]
        vids.extend(ids)
        token = data.get("nextPageToken")
        if not token:
            break

    # de-dup
    seen=set(); ordered=[]
    for v in vids:
        if v not in seen: ordered.append(v); seen.add(v)

    # 2) ìƒì„¸ (videos.list = 1/í˜¸ì¶œ, 50ê°œì”©)
    details=[]
    for i in range(0, len(ordered), 50):
        chunk = ordered[i:i+50]
        if not chunk: continue
        params = {"key": API_KEY, "part": "snippet,contentDetails,statistics", "id": ",".join(chunk)}
        data = api_get(VIDEOS_URL, params, cost=1)
        details.extend(data.get("items", []))

    # 3) DF ë§Œë“¤ê¸° (Shortsë§Œ)
    rows=[]
    for it in details:
        vid = it.get("id")
        sn  = it.get("snippet", {})
        cd  = it.get("contentDetails", {})
        stt = it.get("statistics", {})
        secs = parse_iso8601_duration(cd.get("duration",""))
        if secs is None or secs>60:
            continue
        pub_iso = sn.get("publishedAt","")
        rows.append({
            "video_id": vid,
            "title": sn.get("title",""),
            "description": sn.get("description",""),
            "view_count": int(pd.to_numeric(stt.get("viewCount","0"), errors="coerce") or 0),
            "length": fmt_hms(secs),
            "length_seconds": secs,
            "channel": sn.get("channelTitle",""),
            "url": f"https://www.youtube.com/watch?v={vid}",
            "published_at_kst": to_kst(pub_iso),
            "published_dt_kst": to_kst_dt(pub_iso),
        })
    df = pd.DataFrame(rows, columns=[
        "video_id","title","description","view_count","length","length_seconds",
        "channel","url","published_at_kst","published_dt_kst"
    ])

    # â”€ ê¸‰ìƒìŠ¹ ì§€í‘œ ê³„ì‚°
    now_kst = dt.datetime.now(KST)
    if not df.empty:
        df["hours_since_upload"] = (now_kst - pd.to_datetime(df["published_dt_kst"])).dt.total_seconds() / 3600.0
        df["hours_since_upload"] = df["hours_since_upload"].clip(lower=(1.0/60.0))  # ìµœì†Œ 1ë¶„
        df["views_per_hour"] = (df["view_count"] / df["hours_since_upload"]).round(1)
    else:
        df["hours_since_upload"] = []
        df["views_per_hour"] = []
    return df

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ í‚¤ì›Œë“œ ì¶”ì¶œ â”€â”€â”€â”€â”€â”€â”€â”€â”€
STOPWORDS = set("""
ê·¸ë¦¬ê³  ê·¸ëŸ¬ë‚˜ ê·¸ë˜ì„œ ë˜í•œ ë˜ëŠ” ë° ë¨¼ì € ì§€ê¸ˆ ë°”ë¡œ ë§¤ìš° ì •ë§ ê·¸ëƒ¥ ë„ˆë¬´ ë³´ë‹¤ ë³´ë‹¤ë„ ë•ŒëŠ” ë¼ëŠ” ì´ëŸ° ì €ëŸ° ê·¸ëŸ°
í•©ë‹ˆë‹¤ í–ˆë‹¤ í–ˆë‹¤ê°€ í•˜ëŠ” í•˜ê³  í•˜ë©° í•˜ë©´ ëŒ€í•œ ìœ„í•´ ì—ì„œ ì—ê²Œ ì—ë„ ì—ëŠ” ìœ¼ë¡œ ë¡œ ë¥¼ ì€ ëŠ” ì´ ê°€ ë„ ì˜ ì— ì™€ ê³¼
""".split())
STOPWORDS |= {"ì†ë³´","ë¸Œë¦¬í•‘","ë‹¨ë…","í˜„ì¥","ì˜ìƒ","ë‰´ìŠ¤","ê¸°ì","ë¦¬í¬íŠ¸","ë¼ì´ë¸Œ"}

def tokenize_ko_en(text:str):
    text = re.sub(r"[^0-9A-Za-zê°€-í£]+", " ", str(text))
    toks = [t.strip().lower() for t in text.split() if t.strip()]
    toks = [t for t in toks if len(t)>=2 and t not in STOPWORDS]
    return toks

def top_keywords_from_df(df: pd.DataFrame, topk:int=10):
    corpus = (df["title"].fillna("") + " " + df["description"].fillna("")).tolist()
    cnt = Counter()
    for line in corpus:
        cnt.update(tokenize_ko_en(line))
    items = [(w,c) for w,c in cnt.most_common() if not re.fullmatch(r"\d+", w)]
    return items[:topk]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ Google Trends (pytrends) â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_data(show_spinner=False, ttl=1800)  # 30ë¶„ ìºì‹œ
def google_trends_top():
    try:
        from pytrends.request import TrendReq
        pytrends = TrendReq(hl='ko', tz=540)
        df = pytrends.trending_searches(pn='south_korea')  # ìƒìœ„ 20
        kws = [str(x).strip() for x in df[0].dropna().tolist()]
        return kws[:10]
    except Exception:
        return []

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ UI â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="K-Politics/News Shorts Trend Board", page_icon="ğŸ“º", layout="wide")
st.title("ğŸ“º 48ì‹œê°„ ìœ íŠœë¸Œ ìˆì¸  íŠ¸ë Œë“œ ëŒ€ì‹œë³´ë“œ (ì •ì¹˜Â·ë‰´ìŠ¤)")

if not API_KEY:
    st.error("âš ï¸ API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. App â†’ Settings â†’ Secrets ì— `YOUTUBE_API_KEY = \"ë°œê¸‰í‚¤\"` ë¥¼ ë„£ì–´ì£¼ì„¸ìš”.")
    st.stop()

with st.sidebar:
    st.header("ìˆ˜ì§‘ ì˜µì…˜")
    size = st.selectbox("ìˆ˜ì§‘ ê·œëª¨(Shorts í›„ë³´ ìˆ˜)", [50, 100, 200], index=1)
    pages = {50:1, 100:2, 200:4}[size]

    ttl_choice = st.selectbox("ìºì‹œ TTL(ìë™ ì ˆì•½)", ["15ë¶„","30ë¶„(ì¶”ì²œ)","60ë¶„"], index=1)
    ttl_map = {"15ë¶„":900, "30ë¶„(ì¶”ì²œ)":1800, "60ë¶„":3600}
    ttl_sec = ttl_map[ttl_choice]

    rank_mode = st.radio("ì •ë ¬ ê¸°ì¤€", ["ìƒìŠ¹ì†ë„(ë·°/ì‹œê°„)", "ì¡°íšŒìˆ˜(ì´í•©)"], horizontal=True, index=0)

    show_speed_cols = st.checkbox("ìƒìŠ¹ì†ë„/ê²½ê³¼ì‹œê°„ ì»¬ëŸ¼ í‘œì‹œ", value=True)

    run = st.button("ìƒˆë¡œê³ ì¹¨(ë°ì´í„° ìˆ˜ì§‘)")

# ìºì‹œë¥¼ TTLë³„ë¡œ ë‹¤ë¥´ê²Œ ë§Œë“¤ê¸° ìœ„í•œ ë²„í‚·(ì˜µì…˜)
bucket = int(time.time() // ttl_sec)
if run:
    st.cache_data.clear()
    st.success("ë°ì´í„° ìƒˆë¡œê³ ì¹¨ ì‹œì‘!")

with st.spinner("ë°ì´í„° ìˆ˜ì§‘/ë¶„ì„ ì¤‘â€¦"):
    df = fetch_shorts_df(pages=pages)
    base_col = "views_per_hour" if rank_mode.startswith("ìƒìŠ¹ì†ë„") else "view_count"
    # ë¶„ì„ìš© ìƒìœ„ í’€(ìµœëŒ€ size ë˜ëŠ” 100 ì´ìƒì€ 100)
    base_pool_n = max(50, size)
    df_pool = df.sort_values(base_col, ascending=False, ignore_index=True).head(min(len(df), base_pool_n))
    # í‚¤ì›Œë“œ Top10
    yt_kw = top_keywords_from_df(df_pool, topk=10)
    yt_kw_words = [w for w,_ in yt_kw]
    # êµ¬ê¸€ íŠ¸ë Œë“œ
    g_kw = google_trends_top()

# --- improved intersection (bidirectional partial match, normalized) ---
def _norm(s: str) -> str:
    s = s.lower().strip()
    s = re.sub(r"\s+", "", s)             # ê³µë°± ì œê±°
    s = re.sub(r"[^\wê°€-í£]", "", s)       # íŠ¹ìˆ˜ë¬¸ì ì œê±°
    return s

yt_norm = [_norm(w) for w in yt_kw_words]
g_norm  = [_norm(g) for g in g_kw]

hot = []
for raw_y, y in zip(yt_kw_words, yt_norm):
    for g in g_norm:
        if y and g and (y in g or g in y):  # ì–‘ë°©í–¥ ë¶€ë¶„ì¼ì¹˜
            hot.append(raw_y)               # ì›ë˜ í‘œê¸°ë¥¼ ìœ ì§€
            break

# ìˆœì„œ ë³´ì¡´í•˜ë©° ì¤‘ë³µ ì œê±°
_seen = set()
hot_intersection = [x for x in hot if not (x in _seen or _seen.add(x))]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì¿¼í„°/ë¦¬ì…‹ ì •ë³´ â”€â”€â”€â”€â”€â”€â”€â”€â”€
now_pt = dt.datetime.now(PT)
reset_pt = (now_pt + dt.timedelta(days=1)).replace(hour=0,minute=0,second=0,microsecond=0)
remain_td = reset_pt - now_pt
used = st.session_state["quota_used"]
remaining = max(0, DAILY_QUOTA - used)
pct = min(1.0, used / DAILY_QUOTA)

quota1, quota2 = st.columns([2,1])
with quota1:
    st.subheader("ğŸ”‹ ì˜¤ëŠ˜ ì¿¼í„°(ì¶”ì •)")
    st.progress(pct, text=f"ì‚¬ìš© {used} / {DAILY_QUOTA}  (ë‚¨ì€ {remaining})")
with quota2:
    st.metric("ë‚¨ì€ ì¿¼í„°(ì¶”ì •)", value=f"{remaining:,}", delta=f"ë¦¬ì…‹ê¹Œì§€ {str(remain_td).split('.')[0]}")
st.caption("â€» YouTube Data API ì¼ì¼ ì¿¼í„°ëŠ” ë§¤ì¼ PT(ë¯¸êµ­ ì„œë¶€) ìì •ì— ë¦¬ì…‹ë©ë‹ˆë‹¤. (KST ê¸°ì¤€ ë‹¤ìŒë‚  16~17ì‹œ, ì„œë¨¸íƒ€ì„ ë”°ë¼ ë³€ë™)")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë³´ë“œ ìƒë‹¨: í‚¤ì›Œë“œ ë·° â”€â”€â”€â”€â”€â”€â”€â”€â”€
left, right = st.columns(2)
with left:
    st.subheader("ğŸ“ˆ ìœ íŠœë¸Œ(48hÂ·ìƒìœ„ í’€) í‚¤ì›Œë“œ Top10")
    if yt_kw:
        df_kw = pd.DataFrame(yt_kw, columns=["keyword","count"])
        st.bar_chart(df_kw.set_index("keyword")["count"])
        st.dataframe(df_kw, use_container_width=True, hide_index=True)
        st.download_button("ìœ íŠœë¸Œ í‚¤ì›Œë“œ CSV",
                           df_kw.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig"),
                           file_name="yt_keywords_top10.csv", mime="text/csv")
    else:
        st.info("í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•  ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ìˆ˜ì§‘ ê·œëª¨/í˜ì´ì§€ë¥¼ ëŠ˜ë ¤ë³´ì„¸ìš”.")

with right:
    st.subheader("ğŸŒ Google Trends (KR) Top10")
    if g_kw:
        df_g = pd.DataFrame({"keyword": g_kw})
        st.dataframe(df_g, use_container_width=True, hide_index=True)
        st.download_button("êµ¬ê¸€ íŠ¸ë Œë“œ CSV",
                           df_g.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig"),
                           file_name="google_trends_top10.csv", mime="text/csv")
    else:
        st.info("í˜„ì¬ Google Trends ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")

st.subheader("ğŸ”¥ êµì§‘í•©(ë‘˜ ë‹¤ ëœ¨ëŠ” í‚¤ì›Œë“œ)")
if hot_intersection:
    st.write(", ".join(f"`{w}`" for w in hot_intersection))
else:
    st.write("í˜„ì¬ êµì§‘í•© í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ í•˜ë‹¨: ê²°ê³¼ í…Œì´ë¸” â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.subheader("ğŸ¬ ê´€ë ¨ ìˆì¸  ë¦¬ìŠ¤íŠ¸")
default_kw = (hot_intersection[0] if hot_intersection
              else (yt_kw_words[0] if yt_kw_words else ""))
pick_kw = st.text_input("í‚¤ì›Œë“œë¡œ í•„í„°(ë¶€ë¶„ ì¼ì¹˜)", value=default_kw)

df_show = df_pool.copy()
if pick_kw.strip():
    pat = re.compile(re.escape(pick_kw.strip()), re.IGNORECASE)
    mask = df_show["title"].str.contains(pat) | df_show["description"].str.contains(pat)
    df_show = df_show[mask]

# í‘œ: ì œëª©-ì¡°íšŒìˆ˜-ì˜ìƒê¸¸ì´-ê³„ì •ëª…-ë§í¬-ì—…ë¡œë“œë‚ ì§œ(ì‹œê°„í¬í•¨) (+ ì˜µì…˜)
cols = ["title","view_count","length","channel","url","published_at_kst"]
if show_speed_cols:
    cols = ["title","view_count","views_per_hour","hours_since_upload","length","channel","url","published_at_kst"]

df_show = df_show.sort_values(base_col, ascending=False, ignore_index=True)[cols]
st.dataframe(df_show, use_container_width=True)
st.download_button("í˜„ì¬ í‘œ CSV ë‹¤ìš´ë¡œë“œ",
                   df_show.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig"),
                   file_name="shorts_ranked.csv", mime="text/csv")

# í•˜ë‹¨ ì•ˆë‚´
st.markdown("""
---
**ì°¸ê³ **
- ìœ íŠœë¸Œ APIëŠ” ì—…ë¡œë” êµ­ê°€ë¥¼ í™•ì • ì œê³µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë³¸ ì•±ì€ `regionCode=KR`, `relevanceLanguage=ko`ë¡œ í•œêµ­ ìš°ì„  ê²°ê³¼ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
- ì¿¼í„° ë¹„ìš©(ì¶”ì •): `search.list = 100/í˜¸ì¶œ`, `videos.list = 1/í˜¸ì¶œ(50ê°œ ë‹¨ìœ„)`. ìˆ˜ì§‘ ê·œëª¨ê°€ ì»¤ì§ˆìˆ˜ë¡ ë¹„ìš©ì´ ëŠ˜ì–´ë‚©ë‹ˆë‹¤.
- ìºì‹œ TTLì„ ê¸¸ê²Œ ì„¤ì •í•˜ë©´ ì¿¼í„° ì‚¬ìš©ëŸ‰ì„ í¬ê²Œ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
""")
