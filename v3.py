# -*- coding: utf-8 -*-
# ğŸ“º 24ì‹œê°„ ìœ íŠœë¸Œ ìˆì¸  íŠ¸ë Œë“œ ëŒ€ì‹œë³´ë“œ (ì •ì¹˜Â·ë‰´ìŠ¤)
import streamlit as st
import pandas as pd
import numpy as np
import requests, re, json, time
from collections import Counter
from pathlib import Path
import datetime as dt
from zoneinfo import ZoneInfo
import xml.etree.ElementTree as ET
from bs4 import BeautifulSoup

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê¸°ë³¸ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="K-Politics/News Shorts Trend Board", page_icon="ğŸ“º", layout="wide")

# (ë‹¨ì¼) API í‚¤
API_KEY = st.secrets.get("YOUTUBE_API_KEY", "")
if not API_KEY:
    st.error("âš ï¸ API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. App â†’ Settings â†’ Secrets ì— `YOUTUBE_API_KEY = \"ë°œê¸‰í‚¤\"` ë¥¼ ë„£ì–´ì£¼ì„¸ìš”.")
    st.stop()

KST = ZoneInfo("Asia/Seoul")
PT  = ZoneInfo("America/Los_Angeles")
SEARCH_URL = "https://www.googleapis.com/youtube/v3/search"
VIDEOS_URL = "https://www.googleapis.com/youtube/v3/videos"
DAILY_QUOTA = 10_000

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì¿¼í„°(ì¼ì¼) ì˜êµ¬ ëˆ„ì  ì €ì¥: íŒŒì¼ ë°©ì‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€
DATA_DIR   = Path(".")
QUOTA_FILE = DATA_DIR / "quota_usage.json"

def _today_pt_str():
    now_pt = dt.datetime.now(PT)
    return now_pt.strftime("%Y-%m-%d")

def load_quota_used():
    today = _today_pt_str()
    if QUOTA_FILE.exists():
        try:
            data = json.loads(QUOTA_FILE.read_text(encoding="utf-8"))
            if data.get("pt_date") == today:
                return int(data.get("used", 0))
        except Exception:
            pass
    return 0

def save_quota_used(value: int):
    data = {"pt_date": _today_pt_str(), "used": int(value)}
    QUOTA_FILE.write_text(json.dumps(data), encoding="utf-8")

def add_quota(cost: int):
    used = st.session_state.get("quota_used", 0) + int(cost)
    st.session_state["quota_used"] = used
    cur = load_quota_used()
    save_quota_used(cur + int(cost))

if "quota_used" not in st.session_state:
    st.session_state["quota_used"] = load_quota_used()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì‹œê°„ì°½: ìµœê·¼ 24ì‹œê°„(KST) â”€â”€â”€â”€â”€â”€â”€â”€â”€
def kst_window_last_24h():
    now_kst = dt.datetime.now(KST)
    start_kst = now_kst - dt.timedelta(hours=24)
    start_utc = start_kst.astimezone(dt.timezone.utc).isoformat().replace("+00:00", "Z")
    end_utc   = now_kst.astimezone(dt.timezone.utc).isoformat().replace("+00:00", "Z")
    return start_utc, end_utc, now_kst

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ YouTube API helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€
def api_get(url, params, cost):
    r = requests.get(url, params=params, timeout=20)
    add_quota(cost)
    r.raise_for_status()
    return r.json()

def parse_iso8601_duration(s):
    if not s or not s.startswith("PT"): return None
    s2 = s[2:]; h=m=sec=0; num=""
    for ch in s2:
        if ch.isdigit(): num += ch
        else:
            if ch=="H": h=int(num or 0)
            elif ch=="M": m=int(num or 0)
            elif ch=="S": sec=int(num or 0)
            num=""
    return h*3600 + m*60 + sec

def fmt_hms(seconds):
    if seconds is None: return ""
    h = seconds//3600; m=(seconds%3600)//60; s=seconds%60
    return f"{h:02d}:{m:02d}:{s:02d}" if h>0 else f"{m:02d}:{s:02d}"

def to_kst(iso_str):
    if not iso_str: return ""
    t = dt.datetime.fromisoformat(iso_str.replace("Z","+00:00")).astimezone(KST)
    return t.strftime("%Y-%m-%d %H:%M:%S")

def to_kst_dt(iso_str):
    return dt.datetime.fromisoformat(iso_str.replace("Z","+00:00")).astimezone(KST) if iso_str else None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ (ê³µìš©) ê¸ˆì§€ì–´/í˜•ì‹ì–´ & ê¸ˆì¹™êµ¬/ì‹œê°„í‘œí˜„ â”€â”€â”€â”€â”€â”€â”€â”€â”€
COMMON_STOPWORDS = {
    # ë„ë©”ì¸/í”Œë«í¼/ì¼ë°˜ í˜•ì‹ì–´
    "http","https","www","com","co","kr","net","org","youtube","shorts","watch","tv","cctv","sns",
    "ê¸°ì‚¬","ë‹¨ë…","ì†ë³´","ì˜ìƒ","ì „ë¬¸","ë¼ì´ë¸Œ","ê¸°ì","ë³´ë„","í—¤ë“œë¼ì¸","ë°ìŠ¤í¬","ì „ì²´ë³´ê¸°","ë”ë³´ê¸°",
    "ì˜¤ëŠ˜","ì–´ì œ","ê¸ˆì¼","ìµœê·¼","ë°©ê¸ˆ","ë°©ê¸ˆì „","ì•„ì¹¨","ì˜¤ì „","ì˜¤í›„","ë°¤","ìƒˆë²½","ì²«ë‚ ",
    "ê´€ë ¨","ë…¼ë€","ë…¼ìŸ","ìƒí™©","ì‚¬ê±´","ì´ìŠˆ","ë¶„ì„","ì „ë§","ë¸Œë¦¬í•‘","ë°œì–¸","ë°œí‘œ","ì…ì¥",
    "ì„œìš¸","í•œêµ­","êµ­ë‚´","í•´ì™¸","ì •ë¶€","ì—¬ë‹¹","ì•¼ë‹¹","ë‹¹êµ­","ìœ„ì›ì¥","ì¥ê´€","ëŒ€í†µë ¹","ì´ë¦¬","êµ­íšŒ","ê²€ì°°",
    # ìš”ì²­í•œ ê³µìš© ë¸”ë™ë¦¬ìŠ¤íŠ¸(ìœ íŠœë¸Œ/íŠ¸ë Œë“œ ê³µí†µ ì ìš©)
    "êµ¬ë…","ì •ì¹˜","ëŒ€í†µë ¹ì‹¤","ì±„ë„","news","ëŒ€ë²•ì›","íŠ¹ê²€","ë¯¼ì£¼ë‹¹","êµ­ë¯¼ì˜í˜","ì´ì¼",
    # ë°©ì†¡ì‚¬/ë§¤ì²´ ìƒìˆ˜
    "sbs","kbs","mbc","jtbc","tvì¡°ì„ ","mbn","ì—°í•©ë‰´ìŠ¤","mbcë‰´ìŠ¤",
    # ìì£¼ ëœ¨ëŠ” êµ°ë”ë”ê¸°
    "ì‹œì‘","ì‚¬ê³ ","ì „ë¬¸","ì‚¬ì§„"
}

# ë¬¸ì¥ ìˆ˜ì¤€ ê¸ˆì¹™(ëª…ë ¹/êµ¬ì–´/ë¶ˆëŸ‰ í‘œí˜„ + ì‹œê°„í‘œí˜„)
COMMON_BANNED_PAT = re.compile(
    r"(ì„ë°© ?í•˜ë¼|ì… ?ë‹¥ì¹˜ê³ |ë¬´ìŠ¨ ?ì¼|ìˆ˜ ìˆì„ê¹Œ|ìˆ˜ ìˆë‚˜|ìˆ˜ ì—†ë‚˜)",
    re.I
)
TEMPORAL_BAD_PAT = re.compile(
    r"""(
        \b\d+\s*(ë…„|ê°œì›”|ë‹¬|ì£¼|ì£¼ì¼|ì¼|ì‹œê°„)\s*(ë§Œì—|ì§¸|ë™ì•ˆ)\b
      | \b(ì´í›„|ì´ì „|ì „|í›„)\b
      | \b(ì˜¤ëŠ˜|ì–´ì œ)\s*(ì˜¤ì „|ì˜¤í›„)?\b
    )""",
    re.X | re.I
)

def _contains_common_banned(s: str) -> bool:
    s = s.lower()
    if COMMON_BANNED_PAT.search(s): return True
    if TEMPORAL_BAD_PAT.search(s):  return True
    return False

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë°©ì†¡ì‚¬/ìˆ«ì+ë‹¨ìœ„ ì»·ìš© íŒ¨í„´ â”€â”€â”€â”€â”€â”€â”€â”€â”€
PUBLISHER_PAT = re.compile(
    r"^(?:"
    r"ytn|sbs|kbs|mbc|jtbc|mbn|tvì¡°ì„ |tvchosun|ì±„ë„a|ì—°í•©ë‰´ìŠ¤(?:tv)?|"
    r"yonhap(?:news)?(?:tv)?|newsis|news1|kbsnews|sbsnews|mbcnews|jtbcnews|"
    r"chosun|donga|hani|kmib|herald|mk|hankyung|"
    r"kbc|yna|yonhapnewstv"
    r")$"
)

NUMERIC_COUNTER_PAT = re.compile(
    r"^\d+(?:ëª…|ì‚´|ì¸µ|ì°¨|ê°œ|ê±´|íšŒ|ë‹¬|ì›”|ë…„|%|ë§Œì›?|ì–µì›?|ì²œì›?|ë°±ë§Œì›?|ëŒ€)$"
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ìœ íŠœë¸Œ í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì €(ë‹¨ì–´ ì¤‘ì‹¬) â”€â”€â”€â”€â”€â”€â”€â”€â”€
STOPWORDS = set("""
ê·¸ë¦¬ê³  ê·¸ëŸ¬ë‚˜ ê·¸ë˜ì„œ ë˜í•œ ë˜ëŠ” ë° ë¨¼ì € ì§€ê¸ˆ ë°”ë¡œ ë§¤ìš° ì •ë§ ê·¸ëƒ¥ ë„ˆë¬´ ë³´ë‹¤ ë³´ë‹¤ë„ ë•ŒëŠ” ë¼ëŠ” ì´ëŸ° ì €ëŸ° ê·¸ëŸ°
í•©ë‹ˆë‹¤ í–ˆë‹¤ í–ˆë‹¤ê°€ í•˜ëŠ” í•˜ê³  í•˜ë©° í•˜ë©´ ëŒ€í•œ ìœ„í•´ ì—ì„œ ì—ê²Œ ì—ë„ ì—ëŠ” ìœ¼ë¡œ ë¡œ ë¥¼ ì€ ëŠ” ì´ ê°€ ë„ ì˜ ì— ì™€ ê³¼ ì‹œì‘
""".split())
STOPWORDS |= {
    "ì†ë³´","ë¸Œë¦¬í•‘","ë‹¨ë…","í˜„ì¥","ì˜ìƒ","ë‰´ìŠ¤","ê¸°ì","ë¦¬í¬íŠ¸","ë¼ì´ë¸Œ","ì—°í•©ë‰´ìŠ¤",
    "ì±„ë„","êµ¬ë…","ëŒ€í†µë ¹","ìœ íŠœë¸Œ","ì •ì¹˜","í™ˆí˜ì´ì§€","ëŒ€í•œë¯¼êµ­","ê¸ˆì§€","ì‹œì‚¬","ëª¨ì•„","ë‹µí•´ì£¼ì„¸ìš”",
    "ë‹¤íë””ê¹…","ë‚˜ëŠ”","ì ˆë¡œ","ì„ë°©í•˜ë¼","ì„ë°©","ì „ë¬¸","ì‚¬ê³ ","news","ëŒ€ë²•ì›","íŠ¹ê²€",
    "mbcë‰´ìŠ¤","ì´ì¼","ì²«ë‚ ","ë‰´ìŠ¤top10"
}
STOPWORDS |= COMMON_STOPWORDS

KO_JOSA = ("ì€","ëŠ”","ì´","ê°€","ì„","ë¥¼","ì˜","ì—","ì—ì„œ","ì—ê²Œ","ê»˜","ì™€","ê³¼","ìœ¼ë¡œ","ë¡œ","ë„","ë§Œ","ê¹Œì§€","ë¶€í„°","ë§ˆë‹¤","ì¡°ì°¨","ë¼ë„","ë§ˆì €","ë°–ì—","ì²˜ëŸ¼","ë¿","ê»˜ì„œ","ì±„")
KO_SUFFIX = ("í•˜ê¸°","í•˜ì„¸ìš”","ì‹­ì‹œì˜¤","í•´ì£¼ì„¸ìš”","í•©ë‹ˆë‹¤","í–ˆë‹¤","ì¤‘","ê´€ë ¨","ì˜ìƒ","ì±„ë„","ë‰´ìŠ¤","ë³´ê¸°","ë“±ë¡","êµ¬ë…","í™ˆí˜ì´ì§€","ë©ë‹ˆë‹¤")

def strip_korean_suffixes(t: str) -> str:
    for suf in KO_SUFFIX:
        if t.endswith(suf) and len(t) > len(suf)+1:
            t = t[:-len(suf)]
    for j in KO_JOSA:
        if t.endswith(j) and len(t) > len(j)+1:
            t = t[:-len(j)]
    return t

def tokenize_ko_en(text: str):
    text = str(text or "")
    if _contains_common_banned(text):
        return []
    text = re.sub(r"https?://\S+", " ", text)
    text = re.sub(r"www\.\S+", " ", text)
    text = re.sub(r"\S+@\S+", " ", text)
    text = re.sub(r"[#@_/\\]", " ", text)

    raw = re.findall(r"[0-9A-Za-zê°€-í£]+", text.lower())
    out = []
    for t in raw:
        if not t or t.isdigit():
            continue

        # ë°©ì†¡ì‚¬/ë§¤ì²´ ì»·, ìˆ«ì+ë‹¨ìœ„ ì»·
        if PUBLISHER_PAT.match(t):        continue
        if NUMERIC_COUNTER_PAT.match(t):  continue

        if re.fullmatch(r"[ê°€-í£]+", t):
            t = strip_korean_suffixes(t)

        if t in STOPWORDS or len(t) < 2:
            continue

        # '...tv' ì œê±° í›„ ì¬ê²€ì‚¬
        if t.endswith("tv") and len(t) > 2:
            t2 = t[:-2]
            if PUBLISHER_PAT.match(t2) or (t2 in STOPWORDS) or len(t2) < 2:
                continue
            t = t2

        if re.fullmatch(r"[a-z]+", t) and len(t) <= 2:
            continue

        out.append(t)

    return out

def top_words_from_df(df: pd.DataFrame, topk:int=10):
    corpus = (df["title"].fillna("") + " " + df["description"].fillna("")).tolist()
    cnt = Counter()
    for line in corpus:
        cnt.update(tokenize_ko_en(line))
    items = [(w,c) for w,c in cnt.most_common() if not re.fullmatch(r"\d+", w)]
    return items[:topk]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë°ì´í„° ìˆ˜ì§‘ (ìºì‹œ) â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_data(show_spinner=False)
def fetch_shorts_df(pages:int=1, bucket:int=0):
    """pages: 1â‰ˆ50, 2â‰ˆ100, 4â‰ˆ200 / bucket: TTL ë¶„ë¦¬ìš© í‚¤"""
    _ = bucket
    start_iso, end_iso, _now = kst_window_last_24h()

    # 1) search.list (100/í˜¸ì¶œ)
    vids, token = [], None
    for _ in range(pages):
        params = {
            "key": API_KEY, "part": "snippet", "type":"video", "order":"date",
            "publishedAfter": start_iso, "publishedBefore": end_iso,
            "maxResults": 50, "videoDuration":"short",
            "regionCode":"KR", "relevanceLanguage":"ko", "safeSearch":"moderate",
            "q": "ë‰´ìŠ¤ OR ì •ì¹˜ OR ì†ë³´ OR ë¸Œë¦¬í•‘"
        }
        if token: params["pageToken"] = token
        data = api_get(SEARCH_URL, params, cost=100)
        ids = [it.get("id",{}).get("videoId") for it in data.get("items",[]) if it.get("id",{}).get("videoId")]
        vids.extend(ids)
        token = data.get("nextPageToken")
        if not token:
            break

    # de-dup
    seen=set(); ordered=[]
    for v in vids:
        if v not in seen: ordered.append(v); seen.add(v)

    # 2) videos.list (1/í˜¸ì¶œ, 50ê°œì”©)
    details=[]
    for i in range(0, len(ordered), 50):
        chunk = ordered[i:i+50]
        if not chunk: continue
        params = {"key": API_KEY, "part": "snippet,contentDetails,statistics", "id": ",".join(chunk)}
        data = api_get(VIDEOS_URL, params, cost=1)
        details.extend(data.get("items", []))

    # 3) DF
    rows=[]
    for it in details:
        vid = it.get("id")
        sn  = it.get("snippet", {})
        cd  = it.get("contentDetails", {})
        stt = it.get("statistics", {})
        secs = parse_iso8601_duration(cd.get("duration",""))
        if secs is None or secs>60:
            continue
        pub_iso = sn.get("publishedAt","")
        rows.append({
            "video_id": vid,
            "title": sn.get("title",""),
            "description": sn.get("description",""),
            "view_count": int(pd.to_numeric(stt.get("viewCount","0"), errors="coerce") or 0),
            "length": fmt_hms(secs),
            "length_seconds": secs,
            "channel": sn.get("channelTitle",""),
            "url": f"https://www.youtube.com/watch?v={vid}",
            "published_at_kst": to_kst(pub_iso),
            "published_dt_kst": to_kst_dt(pub_iso),
        })
    df = pd.DataFrame(rows, columns=[
        "video_id","title","description","view_count","length","length_seconds",
        "channel","url","published_at_kst","published_dt_kst"
    ])

    now_kst = dt.datetime.now(KST)
    if not df.empty:
        df["hours_since_upload"] = (now_kst - pd.to_datetime(df["published_dt_kst"])).dt.total_seconds() / 3600.0
        df["hours_since_upload"] = df["hours_since_upload"].clip(lower=(1.0/60.0))
        df["views_per_hour"] = (df["view_count"] / df["hours_since_upload"]).round(1)
    else:
        df["hours_since_upload"] = []
        df["views_per_hour"] = []
    return df

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ íŠ¸ë Œë“œ(êµ¬ê¸€ RSSâ†’HTML, ë„¤ì´ë²„)ë„ ë‹¨ì–´ ê¸°ë°˜ìœ¼ë¡œ â”€â”€â”€â”€â”€â”€â”€â”€â”€
TREND_STOPWORDS = COMMON_STOPWORDS.copy()

def tokenize_trend_words(text: str):
    # íŠ¸ë Œë“œìš©ë„ ë™ì¼ ì •ì±… ì‚¬ìš©(ë°©ì†¡ì‚¬/ìˆ«ì+ë‹¨ìœ„ ì»· í¬í•¨)
    return tokenize_ko_en(text)

def _fetch_trends_naver() -> tuple[list[str], str]:
    headers = {"User-Agent": "Mozilla/5.0"}
    urls = [
        "https://news.naver.com/main/ranking/popularDay.naver",
        "https://news.naver.com/section/100",
        "https://news.naver.com/",
    ]
    selectors = [
        "ol.ranking_list a","div.rankingnews_box a",
        "ul.sa_list a.sa_text_title","a.sa_text_title_link",
        "a.cluster_text_headline","a[href*='/read?']",
    ]
    titles = []
    for u in urls:
        try:
            r = requests.get(u, headers=headers, timeout=12)
            r.raise_for_status()
            soup = BeautifulSoup(r.text, "lxml")
            got = []
            for css in selectors:
                for a in soup.select(css):
                    t = a.get_text(" ", strip=True)
                    if t: got.append(t)
            if got: titles.extend(got)
        except Exception:
            continue
    if not titles:
        return [], "none"

    cnt = Counter()
    for t in titles:
        if _contains_common_banned(t): continue
        cnt.update(tokenize_trend_words(t))
    words = [w for w,_ in cnt.most_common(50) if w not in TREND_STOPWORDS]
    words = _dedup_similar_words(words)[:10]
    return words, ("naver" if words else "none")

def _google_trends_words() -> tuple[list[str], str]:
    headers = {"User-Agent":"Mozilla/5.0"}
    bases = ["https://trends.google.com", "https://trends.google.co.kr"]

    # A) RSS
    for base in bases:
        try:
            url = f"{base}/trends/trendingsearches/daily/rss?geo=KR&hl=ko"
            r = requests.get(url, headers=headers, timeout=15, allow_redirects=True)
            r.raise_for_status()
            root = ET.fromstring(r.content)
            titles = []
            for item in root.findall(".//item"):
                t = (item.findtext("title") or "").strip()
                if t: titles.append(t)
                if len(titles) >= 50: break
            cnt = Counter()
            for t in titles:
                if _contains_common_banned(t): continue
                cnt.update(tokenize_trend_words(t))
            words = [w for w,_ in cnt.most_common(50) if w not in TREND_STOPWORDS]
            words = _dedup_similar_words(words)[:10]
            if words:
                return words, "google-rss"
        except Exception:
            pass

    # B) HTML fallback
    try:
        url = "https://trends.google.com/trends/trendingsearches/daily?geo=KR&hl=ko"
        r = requests.get(url, headers=headers, timeout=15)
        r.raise_for_status()
        soup = BeautifulSoup(r.text, "lxml")
        raw = [a.get_text(strip=True) for a in soup.select("div.feed-item h2 a")]
        cnt = Counter()
        for t in raw:
            if _contains_common_banned(t): continue
            cnt.update(tokenize_trend_words(t))
        words = [w for w,_ in cnt.most_common(50) if w not in TREND_STOPWORDS]
        words = _dedup_similar_words(words)[:10]
        if words:
            return words, "google-html"
    except Exception:
        pass

    return [], "none"

def _dedup_similar_words(words: list[str]) -> list[str]:
    """ê°„ë‹¨ ìœ ì‚¬ì–´ ì •ë¦¬: ì™„ì „ ë™ì¼Â·ë¶€ë¶„í¬í•¨Â·Jaccard(ë¬¸ìì…‹) 0.8 ì´ìƒ ì¤‘ë³µ ì œê±°"""
    kept = []
    sigs = []
    def jacc(a: set, b: set) -> float:
        if not a and not b: return 1.0
        return len(a & b) / max(1, len(a | b))
    for w in words:
        w2 = re.sub(r"\s+","", w)
        s  = set(w2)
        dup=False
        for old, so in zip(kept, sigs):
            if w2==old or w2 in old or old in w2 or jacc(s, so) >= 0.8:
                dup=True; break
        if not dup:
            kept.append(w2)
            sigs.append(set(w2))
    return kept

@st.cache_data(show_spinner=False, ttl=900)
def trends_words_top(source_mode: str = "auto"):
    # source_mode: auto | google | naver | youtube
    def _youtube_fallback():
        words = st.session_state.get("yt_kw_words", [])
        words = [w for w in words if w not in TREND_STOPWORDS]
        return (words[:10], "youtube-fallback") if words else ([], "none")

    if source_mode == "google":
        return *_google_trends_words(), []
    if source_mode == "naver":
        return *_fetch_trends_naver(), []
    if source_mode == "youtube":
        kws, src = _youtube_fallback()
        return kws, src, []

    # auto
    kws, src = _google_trends_words()
    if not kws:
        kws, src = _fetch_trends_naver()
    if not kws:
        kws, src = _youtube_fallback()
    return (kws or []), (src or "none"), []

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ UI â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.title("ğŸ“º 24ì‹œê°„ ìœ íŠœë¸Œ ìˆì¸  íŠ¸ë Œë“œ ëŒ€ì‹œë³´ë“œ (ì •ì¹˜Â·ë‰´ìŠ¤)")

with st.sidebar:
    st.header("ìˆ˜ì§‘ ì˜µì…˜")
    size = st.selectbox("ìˆ˜ì§‘ ê·œëª¨(Shorts í›„ë³´ ìˆ˜)", [50, 100, 200], index=1)
    pages = {50:1, 100:2, 200:4}[size]

    ttl_choice = st.selectbox("ìºì‹œ TTL(ìë™ ì ˆì•½)", ["15ë¶„","30ë¶„(ì¶”ì²œ)","60ë¶„"], index=1)
    ttl_map = {"15ë¶„":900, "30ë¶„(ì¶”ì²œ)":1800, "60ë¶„":3600}
    ttl_sec = ttl_map[ttl_choice]

    rank_mode = st.radio("ì •ë ¬ ê¸°ì¤€", ["ìƒìŠ¹ì†ë„(ë·°/ì‹œê°„)", "ì¡°íšŒìˆ˜(ì´í•©)"], horizontal=True, index=0)
    sort_order = st.radio("ì •ë ¬ ìˆœì„œ", ["ë‚´ë¦¼ì°¨ìˆœ", "ì˜¤ë¦„ì°¨ìˆœ"], horizontal=True, index=0)
    show_speed_cols = st.checkbox("ìƒìŠ¹ì†ë„/ê²½ê³¼ì‹œê°„ ì»¬ëŸ¼ í‘œì‹œ", value=True)

    trend_source = st.radio("íŠ¸ë Œë“œ ì†ŒìŠ¤ ì„ íƒ", ["ìë™(êµ¬ê¸€â†’ë„¤ì´ë²„)", "êµ¬ê¸€ë§Œ", "ë„¤ì´ë²„ë§Œ", "ìœ íŠœë¸Œë§Œ"], index=0)

    run = st.button("ìƒˆë¡œê³ ì¹¨(ë°ì´í„° ìˆ˜ì§‘)")

bucket = int(time.time() // ttl_sec)
if run:
    st.cache_data.clear()
    st.success("ë°ì´í„° ìƒˆë¡œê³ ì¹¨ ì‹œì‘!")

df = fetch_shorts_df(pages=pages, bucket=bucket)

base_col = "views_per_hour" if rank_mode.startswith("ìƒìŠ¹ì†ë„") else "view_count"
ascending_flag = (sort_order == "ì˜¤ë¦„ì°¨ìˆœ")
base_pool_n = max(50, len(df))
df_pool = df.sort_values(base_col, ascending=ascending_flag, ignore_index=True).head(base_pool_n)

# ìœ íŠœë¸Œ í‚¤ì›Œë“œ Top10(ë‹¨ì–´)
yt_kw = top_words_from_df(df_pool, topk=10)
yt_kw_words = [w for w, _ in yt_kw]
st.session_state["yt_kw_words"] = yt_kw_words  # ìœ íŠœë¸Œ fallbackìš©

# íŠ¸ë Œë“œ ì†ŒìŠ¤ ëª¨ë“œ
mode_map = {"ìë™(êµ¬ê¸€â†’ë„¤ì´ë²„)":"auto","êµ¬ê¸€ë§Œ":"google","ë„¤ì´ë²„ë§Œ":"naver","ìœ íŠœë¸Œë§Œ":"youtube"}
source_mode = mode_map[trend_source]

# íŠ¸ë Œë“œ í‚¤ì›Œë“œ(ë‹¨ì–´)
g_kw, g_src, _ = trends_words_top(source_mode=source_mode)
st.caption(f"íŠ¸ë Œë“œ ì†ŒìŠ¤: {g_src if g_kw else 'Unavailable'} Â· í‚¤ì›Œë“œ {len(g_kw)}ê°œ Â· ëª¨ë“œ={trend_source}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì¿¼í„°/ë¦¬ì…‹ ì •ë³´ â”€â”€â”€â”€â”€â”€â”€â”€â”€
now_pt = dt.datetime.now(PT)
reset_pt = (now_pt + dt.timedelta(days=1)).replace(hour=0,minute=0,second=0,microsecond=0)
remain_td = reset_pt - now_pt
used = st.session_state.get("quota_used", 0)
remaining = max(0, DAILY_QUOTA - used)
pct = min(1.0, used / DAILY_QUOTA)

quota1, quota2 = st.columns([2,1])
with quota1:
    st.subheader("ğŸ”‹ ì˜¤ëŠ˜ ì¿¼í„°(ì¶”ì •)")
    st.progress(pct, text=f"ì‚¬ìš© {used} / {DAILY_QUOTA}  (ë‚¨ì€ {remaining})")
with quota2:
    st.metric("ë‚¨ì€ ì¿¼í„°(ì¶”ì •)", value=f"{remaining:,}", delta=f"ë¦¬ì…‹ê¹Œì§€ {str(remain_td).split('.')[0]}")
st.caption("â€» YouTube Data API ì¼ì¼ ì¿¼í„°ëŠ” PT ìì •(=KST ì˜¤í›„ 4~5ì‹œê²½)ì— ë¦¬ì…‹ë©ë‹ˆë‹¤.")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ìƒë‹¨ ë³´ë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€
left, right = st.columns(2)
with left:
    st.subheader("ğŸ“ˆ ìœ íŠœë¸Œ(24hÂ·ìƒìœ„ í’€) í‚¤ì›Œë“œ Top10")
    if yt_kw:
        df_kw = pd.DataFrame(yt_kw, columns=["keyword","count"])
        df_kw_sorted = df_kw.sort_values("count", ascending=False)
        st.bar_chart(df_kw_sorted.set_index("keyword")["count"])
        st.dataframe(df_kw_sorted, use_container_width=True, hide_index=True)
        st.download_button("ìœ íŠœë¸Œ í‚¤ì›Œë“œ CSV",
                           df_kw_sorted.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig"),
                           file_name="yt_keywords_top10.csv", mime="text/csv")
    else:
        st.info("í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•  ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ìˆ˜ì§‘ ê·œëª¨/í˜ì´ì§€ë¥¼ ëŠ˜ë ¤ë³´ì„¸ìš”.")

with right:
    st.subheader("ğŸŒ Trends Top10 (ë‹¨ì–´)")
    if g_kw:
        df_g = pd.DataFrame({"keyword": g_kw}).dropna()
        df_g["keyword"] = df_g["keyword"].astype(str).str.strip()
        df_g = df_g[df_g["keyword"] != ""].drop_duplicates("keyword").head(10)
        if len(df_g) >= 1:
            df_g["rank"]  = np.arange(1, len(df_g) + 1, dtype=int)
            df_g["score"] = (len(df_g) + 1) - df_g["rank"]  # 1ë“±=ìµœëŒ€
            st.bar_chart(df_g.set_index("keyword")[["score"]])
            st.dataframe(df_g[["rank","keyword"]], use_container_width=True, hide_index=True)
            st.download_button("íŠ¸ë Œë“œ í‚¤ì›Œë“œ CSV",
                               df_g[["rank","keyword"]].to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig"),
                               file_name="trends_top10.csv", mime="text/csv")
        else:
            st.info("íŠ¸ë Œë“œ í‚¤ì›Œë“œê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
    else:
        st.info("ì„ íƒí•œ ì†ŒìŠ¤ì—ì„œ íŠ¸ë Œë“œ í‚¤ì›Œë“œë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ êµì§‘í•© â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _norm(s: str) -> str:
    s = s.lower().strip()
    s = re.sub(r"\s+", "", s)
    s = re.sub(r"[^\wê°€-í£]", "", s)
    return s

yt_norm = [_norm(w) for w in yt_kw_words]
g_norm  = [_norm(g) for g in (g_kw or [])]
hot = []
for raw_y, y in zip(yt_kw_words, yt_norm):
    for g in g_norm:
        if y and g and (y in g or g in y):
            hot.append(raw_y); break
_seen=set()
hot_intersection = [x for x in hot if not (x in _seen or _seen.add(x))]

st.subheader("ğŸ”¥ êµì§‘í•©(ë‘˜ ë‹¤ ëœ¨ëŠ” í‚¤ì›Œë“œ)")
st.write(", ".join(f"`{w}`" for w in hot_intersection) if hot_intersection else "í˜„ì¬ êµì§‘í•© í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ í•˜ë‹¨: ê²°ê³¼ í…Œì´ë¸” â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.subheader("ğŸ¬ ê´€ë ¨ ìˆì¸  ë¦¬ìŠ¤íŠ¸")
default_kw = (hot_intersection[0] if hot_intersection else (yt_kw_words[0] if yt_kw_words else ""))
pick_kw = st.text_input("í‚¤ì›Œë“œë¡œ í•„í„°(ë¶€ë¶„ ì¼ì¹˜)", value=default_kw)

df_show = df_pool.copy()
if pick_kw.strip():
    pat = re.compile(re.escape(pick_kw.strip()), re.IGNORECASE)
    mask = df_show["title"].str.contains(pat) | df_show["description"].str.contains(pat)
    df_show = df_show[mask]

cols = ["title","view_count","length","channel","url","published_at_kst"]
if show_speed_cols:
    cols = ["title","view_count","views_per_hour","hours_since_upload","length","channel","url","published_at_kst"]

df_show = df_show.sort_values(base_col, ascending=ascending_flag, ignore_index=True)[cols]

st.session_state["df_show_frozen"] = df_show.copy()
st.dataframe(st.session_state["df_show_frozen"], use_container_width=True)

csv_bytes = st.session_state["df_show_frozen"].to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig")
st.download_button("í˜„ì¬ í‘œ CSV ë‹¤ìš´ë¡œë“œ", data=csv_bytes, file_name="shorts_ranked.csv", mime="text/csv", key="dl_df_show")

st.markdown("""
---
**ì°¸ê³ **
- ìœ íŠœë¸Œ APIëŠ” ì—…ë¡œë” êµ­ê°€ë¥¼ í™•ì • ì œê³µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë³¸ ì•±ì€ `regionCode=KR`, `relevanceLanguage=ko`ë¡œ í•œêµ­ ìš°ì„  ê²°ê³¼ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
- ì¿¼í„° ë¹„ìš©(ì¶”ì •): `search.list = 100/í˜¸ì¶œ`, `videos.list = 1/í˜¸ì¶œ(50ê°œ ë‹¨ìœ„)`. ìˆ˜ì§‘ ê·œëª¨ê°€ ì»¤ì§ˆìˆ˜ë¡ ë¹„ìš©ì´ ëŠ˜ì–´ë‚©ë‹ˆë‹¤.
- ìºì‹œ TTLì„ ê¸¸ê²Œ ì„¤ì •í•˜ë©´ ì¿¼í„° ì‚¬ìš©ëŸ‰ì„ í¬ê²Œ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- íŠ¸ë Œë“œ ì†ŒìŠ¤ëŠ” *êµ¬ê¸€(RSSâ†’HTML)* ì‹¤íŒ¨ ì‹œ *ë„¤ì´ë²„ ì¸ê¸°ë‰´ìŠ¤* ë˜ëŠ” *ìœ íŠœë¸Œ í‚¤ì›Œë“œ*ë¡œ ìë™ ëŒ€ì²´ë©ë‹ˆë‹¤.
""")
