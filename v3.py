# -*- coding: utf-8 -*-
# ğŸ“º 48ì‹œê°„ ìœ íŠœë¸Œ ìˆì¸  íŠ¸ë Œë“œ ëŒ€ì‹œë³´ë“œ (ì •ì¹˜Â·ë‰´ìŠ¤)
import streamlit as st
import pandas as pd
import requests, re, json, time
from collections import Counter
from pathlib import Path
import datetime as dt
from zoneinfo import ZoneInfo
import xml.etree.ElementTree as ET
from bs4 import BeautifulSoup
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê¸°ë³¸ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="K-Politics/News Shorts Trend Board", page_icon="ğŸ“º", layout="wide")

API_KEY = st.secrets.get("YOUTUBE_API_KEY", "")
if not API_KEY:
    st.error("âš ï¸ API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. App â†’ Settings â†’ Secrets ì— `YOUTUBE_API_KEY = \"ë°œê¸‰í‚¤\"` ë¥¼ ë„£ì–´ì£¼ì„¸ìš”.")
    st.stop()

KST = ZoneInfo("Asia/Seoul")
PT  = ZoneInfo("America/Los_Angeles")
SEARCH_URL = "https://www.googleapis.com/youtube/v3/search"
VIDEOS_URL = "https://www.googleapis.com/youtube/v3/videos"
DAILY_QUOTA = 10_000

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì¿¼í„°(ì¼ì¼) ì˜êµ¬ ëˆ„ì  ì €ì¥: íŒŒì¼ ë°©ì‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€
DATA_DIR   = Path(".")
QUOTA_FILE = DATA_DIR / "quota_usage.json"

def _today_pt_str():
    now_pt = dt.datetime.now(PT)
    return now_pt.strftime("%Y-%m-%d")

def load_quota_used():
    today = _today_pt_str()
    if QUOTA_FILE.exists():
        try:
            data = json.loads(QUOTA_FILE.read_text(encoding="utf-8"))
            if data.get("pt_date") == today:
                return int(data.get("used", 0))
        except Exception:
            pass
    return 0

def save_quota_used(value: int):
    data = {"pt_date": _today_pt_str(), "used": int(value)}
    QUOTA_FILE.write_text(json.dumps(data), encoding="utf-8")

def add_quota(cost: int):
    used = st.session_state.get("quota_used", 0) + int(cost)
    st.session_state["quota_used"] = used
    cur = load_quota_used()
    save_quota_used(cur + int(cost))

if "quota_used" not in st.session_state:
    st.session_state["quota_used"] = load_quota_used()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì‹œê°„ì°½: ìµœê·¼ 48ì‹œê°„(KST) â”€â”€â”€â”€â”€â”€â”€â”€â”€
def kst_window_last_48h():
    now_kst = dt.datetime.now(KST)
    start_kst = now_kst - dt.timedelta(hours=48)
    start_utc = start_kst.astimezone(dt.timezone.utc).isoformat().replace("+00:00", "Z")
    end_utc   = now_kst.astimezone(dt.timezone.utc).isoformat().replace("+00:00", "Z")
    return start_utc, end_utc, now_kst

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ YouTube API helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€
def api_get(url, params, cost):
    r = requests.get(url, params=params, timeout=20)
    add_quota(cost)
    r.raise_for_status()
    return r.json()

def parse_iso8601_duration(s):
    if not s or not s.startswith("PT"): return None
    s2 = s[2:]; h=m=sec=0; num=""
    for ch in s2:
        if ch.isdigit(): num += ch
        else:
            if ch=="H": h=int(num or 0)
            elif ch=="M": m=int(num or 0)
            elif ch=="S": sec=int(num or 0)
            num=""
    return h*3600 + m*60 + sec

def fmt_hms(seconds):
    if seconds is None: return ""
    h = seconds//3600; m=(seconds%3600)//60; s=seconds%60
    return f"{h:02d}:{m:02d}:{s:02d}" if h>0 else f"{m:02d}:{s:02d}"

def to_kst(iso_str):
    if not iso_str: return ""
    t = dt.datetime.fromisoformat(iso_str.replace("Z","+00:00")).astimezone(KST)
    return t.strftime("%Y-%m-%d %H:%M:%S")

def to_kst_dt(iso_str):
    return dt.datetime.fromisoformat(iso_str.replace("Z","+00:00")).astimezone(KST) if iso_str else None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë°ì´í„° ìˆ˜ì§‘ (ìºì‹œ) â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_data(show_spinner=False)
def fetch_shorts_df(pages:int=1, bucket:int=0):
    """pages: 1â‰ˆ50, 2â‰ˆ100, 4â‰ˆ200 / bucket: TTL ë¶„ë¦¬ìš© í‚¤"""
    _ = bucket
    start_iso, end_iso, _now = kst_window_last_48h()

    # 1) search.list (100/í˜¸ì¶œ)
    vids, token = [], None
    for _ in range(pages):
        params = {
            "key": API_KEY, "part": "snippet", "type":"video", "order":"date",
            "publishedAfter": start_iso, "publishedBefore": end_iso,
            "maxResults": 50, "videoDuration":"short",
            "regionCode":"KR", "relevanceLanguage":"ko", "safeSearch":"moderate",
            "q": "ë‰´ìŠ¤ OR ì •ì¹˜ OR ì†ë³´ OR ë¸Œë¦¬í•‘"
        }
        if token: params["pageToken"] = token
        data = api_get(SEARCH_URL, params, cost=100)
        ids = [it.get("id",{}).get("videoId") for it in data.get("items",[]) if it.get("id",{}).get("videoId")]
        vids.extend(ids)
        token = data.get("nextPageToken")
        if not token:
            break

    # de-dup
    seen=set(); ordered=[]
    for v in vids:
        if v not in seen: ordered.append(v); seen.add(v)

    # 2) videos.list (1/í˜¸ì¶œ, 50ê°œì”©)
    details=[]
    for i in range(0, len(ordered), 50):
        chunk = ordered[i:i+50]
        if not chunk: continue
        params = {"key": API_KEY, "part": "snippet,contentDetails,statistics", "id": ",".join(chunk)}
        data = api_get(VIDEOS_URL, params, cost=1)
        details.extend(data.get("items", []))

    # 3) DF
    rows=[]
    for it in details:
        vid = it.get("id")
        sn  = it.get("snippet", {})
        cd  = it.get("contentDetails", {})
        stt = it.get("statistics", {})
        secs = parse_iso8601_duration(cd.get("duration",""))
        if secs is None or secs>60:
            continue
        pub_iso = sn.get("publishedAt","")
        rows.append({
            "video_id": vid,
            "title": sn.get("title",""),
            "description": sn.get("description",""),
            "view_count": int(pd.to_numeric(stt.get("viewCount","0"), errors="coerce") or 0),
            "length": fmt_hms(secs),
            "length_seconds": secs,
            "channel": sn.get("channelTitle",""),
            "url": f"https://www.youtube.com/watch?v={vid}",
            "published_at_kst": to_kst(pub_iso),
            "published_dt_kst": to_kst_dt(pub_iso),
        })
    df = pd.DataFrame(rows, columns=[
        "video_id","title","description","view_count","length","length_seconds",
        "channel","url","published_at_kst","published_dt_kst"
    ])

    now_kst = dt.datetime.now(KST)
    if not df.empty:
        df["hours_since_upload"] = (now_kst - pd.to_datetime(df["published_dt_kst"])).dt.total_seconds() / 3600.0
        df["hours_since_upload"] = df["hours_since_upload"].clip(lower=(1.0/60.0))
        df["views_per_hour"] = (df["view_count"] / df["hours_since_upload"]).round(1)
    else:
        df["hours_since_upload"] = []
        df["views_per_hour"] = []
    return df

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ìœ íŠœë¸Œ í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì €(í‚¤ì›Œë“œ) â”€â”€â”€â”€â”€â”€â”€â”€â”€
STOPWORDS = set("""
ê·¸ë¦¬ê³  ê·¸ëŸ¬ë‚˜ ê·¸ë˜ì„œ ë˜í•œ ë˜ëŠ” ë° ë¨¼ì € ì§€ê¸ˆ ë°”ë¡œ ë§¤ìš° ì •ë§ ê·¸ëƒ¥ ë„ˆë¬´ ë³´ë‹¤ ë³´ë‹¤ë„ ë•ŒëŠ” ë¼ëŠ” ì´ëŸ° ì €ëŸ° ê·¸ëŸ°
í•©ë‹ˆë‹¤ í–ˆë‹¤ í–ˆë‹¤ê°€ í•˜ëŠ” í•˜ê³  í•˜ë©° í•˜ë©´ ëŒ€í•œ ìœ„í•´ ì—ì„œ ì—ê²Œ ì—ë„ ì—ëŠ” ìœ¼ë¡œ ë¡œ ë¥¼ ì€ ëŠ” ì´ ê°€ ë„ ì˜ ì— ì™€ ê³¼
""".split())
STOPWORDS |= {"ì†ë³´","ë¸Œë¦¬í•‘","ë‹¨ë…","í˜„ì¥","ì˜ìƒ","ë‰´ìŠ¤","ê¸°ì","ë¦¬í¬íŠ¸","ë¼ì´ë¸Œ","ì—°í•©ë‰´ìŠ¤",
              "ì±„ë„","êµ¬ë…","ëŒ€í†µë ¹","ìœ íŠœë¸Œ","ì •ì¹˜","í™ˆí˜ì´ì§€","ëŒ€í•œë¯¼êµ­","ê¸ˆì§€","ì‹œì‚¬","ëª¨ì•„","ë‹µí•´ì£¼ì„¸ìš”"}
STOPWORDS |= {"http","https","www","com","co","kr","net","org",
              "youtu","youtube","be","shorts","watch","tv",
              "news","live","breaking","official","channel",
              "video","clip","yonhap","yonhapnews"}

KO_JOSA = ("ì€","ëŠ”","ì´","ê°€","ì„","ë¥¼","ì˜","ì—","ì—ì„œ","ì—ê²Œ","ê»˜","ì™€","ê³¼","ìœ¼ë¡œ","ë¡œ","ë„","ë§Œ","ê¹Œì§€","ë¶€í„°","ë§ˆë‹¤","ì¡°ì°¨","ë¼ë„","ë§ˆì €","ë°–ì—","ì²˜ëŸ¼","ë¿","ê»˜ì„œ")
KO_SUFFIX = ("í•˜ê¸°","í•˜ì„¸ìš”","ì‹­ì‹œì˜¤","í•´ì£¼ì„¸ìš”","í•©ë‹ˆë‹¤","í–ˆë‹¤","ì¤‘","ê´€ë ¨","ì˜ìƒ","ì±„ë„","ë‰´ìŠ¤","ë³´ê¸°","ë“±ë¡","êµ¬ë…","í™ˆí˜ì´ì§€","ë©ë‹ˆë‹¤")

def strip_korean_suffixes(t: str) -> str:
    for suf in KO_SUFFIX:
        if t.endswith(suf) and len(t) > len(suf)+1:
            t = t[:-len(suf)]
    for j in KO_JOSA:
        if t.endswith(j) and len(t) > len(j)+1:
            t = t[:-len(j)]
    return t

def tokenize_ko_en(text: str):
    text = str(text or "")
    text = re.sub(r"https?://\S+", " ", text)
    text = re.sub(r"www\.\S+", " ", text)
    text = re.sub(r"\S+@\S+", " ", text)
    text = re.sub(r"[#@_/\\]", " ", text)
    raw = re.findall(r"[0-9A-Za-zê°€-í£]+", text.lower())
    out = []
    for t in raw:
        if not t or t.isdigit(): 
            continue
        if re.fullmatch(r"[ê°€-í£]+", t):
            t = strip_korean_suffixes(t)
        if t in STOPWORDS or len(t) < 2:
            continue
        if re.fullmatch(r"[a-z]+", t) and len(t) <= 2:
            continue
        if t.endswith("tv") and len(t) > 2:
            t = t[:-2]
            if t in STOPWORDS or len(t) < 2:
                continue
        out.append(t)
    return out

def top_keywords_from_df(df: pd.DataFrame, topk:int=10):
    corpus = (df["title"].fillna("") + " " + df["description"].fillna("")).tolist()
    cnt = Counter()
    for line in corpus:
        cnt.update(tokenize_ko_en(line))
    items = [(w,c) for w,c in cnt.most_common() if not re.fullmatch(r"\d+", w)]
    return items[:topk]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ Trends ì „ìš© í•„í„°/ì •ê·œí™” ê·œì¹™ â”€â”€â”€â”€â”€â”€â”€â”€â”€
TREND_STOPWORDS = {
    "http","https","www","com","co","kr","net","org","youtube","shorts","watch","tv","cctv","sns",
    "ê¸°ì‚¬","ë‹¨ë…","ì†ë³´","ì˜ìƒ","ì „ë¬¸","ë¼ì´ë¸Œ","ê¸°ì","ë³´ë„","í—¤ë“œë¼ì¸","ë°ìŠ¤í¬","ì „ì²´ë³´ê¸°","ë”ë³´ê¸°",
    "ì˜¤ëŠ˜","ì–´ì œ","ê¸ˆì¼","ìµœê·¼","ë°©ê¸ˆ","ë°©ê¸ˆì „","ì•„ì¹¨","ì˜¤ì „","ì˜¤í›„","ë°¤","ìƒˆë²½",
    "ê´€ë ¨","ë…¼ë€","ë…¼ìŸ","ìƒí™©","ì‚¬ê±´","ì´ìŠˆ","ë¶„ì„","ì „ë§","ë¸Œë¦¬í•‘","ë°œì–¸","ë°œí‘œ","ì…ì¥",
    "ì„œìš¸","í•œêµ­","êµ­ë‚´","í•´ì™¸","ì •ë¶€","ì—¬ë‹¹","ì•¼ë‹¹","ë‹¹êµ­","ìœ„ì›ì¥","ì¥ê´€","ëŒ€í†µë ¹","ì´ë¦¬","êµ­íšŒ",
}

_BAD_START = {"ë¬´ìŠ¨","ì–´ë–¤","ì™œ","ì–´ì§¸ì„œ","ì–´ë””","ëˆ„ê°€","ëˆ„êµ¬","ì–¸ì œ","ì–¼ë§ˆë‚˜","ì´ë²ˆ","ì§€ë‚œ","í˜„ì§","ì „ì§","í˜„ì¬","í–¥í›„","ë‚´ë…„","ì˜¬í•´"}
_POSTPOSITION_SUFFIXES = ("ìœ¼ë¡œ","ë¡œ","ì—ê²Œ","ì—ì„œ","ë³´ë‹¤","ê¹Œì§€","ë¶€í„°","ë§Œ","ì¡°ì°¨","ë¼ë„","ì²˜ëŸ¼","ë¿","ê»˜","ì—","ì™€","ê³¼","ë‘","í•˜ê³ ","ë°–ì—","ì´ë¼","ë¼","ì´ë¼ë„","ì€","ëŠ”","ì´","ê°€")
_BAD_END_VERB_PAT = re.compile(r"(í•˜ë¼|í•´ë¼|í•´ì£¼ì„¸ìš”|í•©ì‹œë‹¤|í•˜ì|ëë‹¤|ëœë‹¤|ë”ë‹ˆ|í•´|ì¡Œë‹¤)$")
_BAD_END_QUESTION_PAT = re.compile(r"(ê¹Œ|ë‚˜|ëƒ|ì¼ê¹Œ|ì„ê¹Œ|ì˜€ë‚˜|ì˜€ì„ê¹Œ|ë‚˜ìš”|\?$)$")
_BAD_END_INTERROGATIVE = {"ì–´ë””","ëˆ„êµ¬","ëˆ„ê°€","ë¬´ì—‡","ë­","ì™œ","ì–¸ì œ","ì–¼ë§ˆë‚˜"}
_WEAK_LAST_TOKENS = {"ì°¨ë¦¼","ì—´ëŒ","ë…¼ìŸ","ë…¼ë€","ë°œì–¸","ë°œí‘œ","ì…ì¥","ì‚¬ì‹¤","ìƒí™©","ì‚¬ê±´","ë¬¸ì œ","ì˜í˜¹","ì˜ì‹¬","í–‰ìœ„","ì œê¸°","ì œì•ˆ","ìš”ì²­","ìš”êµ¬","ìš°ë ¤","ê°€ëŠ¥ì„±","ëª©ì†Œë¦¬","í›„","ì „","ì•ˆ","ë°–","ì†","ì¤‘","ìª½","ë‚´","ì™¸","ë“±","ê±´","ë¶€ë¶„","ì¸¡ë©´","ìë£Œ"}
_BANNED_PHRASES = {"ë¬´ìŠ¨ ì¼","ì… ë‹¥ì¹˜ê³ ","ì„ë°©í•˜ë¼","ì„ë°© í•˜ë¼"}
_BANNED_TOKENS  = {"ì„ë°©í•˜ë¼","í•˜ë¼"}
_TOKEN_PAT = r"[0-9A-Za-zê°€-í£]+"

def _strip_postposition(token: str) -> str:
    for suf in _POSTPOSITION_SUFFIXES:
        if token.endswith(suf) and len(token) > len(suf):
            stem = token[:-len(suf)]
            if len(stem) >= 2:
                return stem
    return token

def _tok_line_for_trends(s: str) -> list[str]:
    s = s.lower()
    s = re.sub(r"https?://\S+"," ",s); s = re.sub(r"www\.\S+"," ",s)
    toks = re.findall(_TOKEN_PAT, s)
    out=[]
    for t in toks:
        if t.isdigit(): continue
        if t in TREND_STOPWORDS: continue
        if re.fullmatch(r"[a-z]+", t) and len(t)<=2: continue
        out.append(t)
    return out

def _is_bad_phrase(ph: str) -> bool:
    if ph in _BANNED_PHRASES: 
        return True
    ws = ph.split()
    if len(ws) < 2:
        return True
    if ws[0] in _BAD_START:
        return True
    if any(t in _BANNED_TOKENS or t.endswith("í•˜ë¼") for t in ws):
        return True
    last_raw = ws[-1]
    last = _strip_postposition(last_raw)
    if last in _BAD_END_INTERROGATIVE: 
        return True
    if _BAD_END_VERB_PAT.search(last) or _BAD_END_QUESTION_PAT.search(last): 
        return True
    for i, t in enumerate(ws[:-1]):
        if t == "ìˆ˜":
            nxt = ws[i+1] if i+1 < len(ws) else ""
            if nxt.startswith(("ìˆ","ì—†")): 
                return True
    if last in _WEAK_LAST_TOKENS: 
        return True
    if len(ws)==2:
        w0 = ws[0]; w1 = last
        if (w0 in TREND_STOPWORDS or w1 in TREND_STOPWORDS or
            w0 in _BAD_START or w1 in _WEAK_LAST_TOKENS):
            return True
    if re.search(r"(ì–´|ì•„)$", last): 
        return True
    return False

def _normalize_phrase(ph: str) -> str:
    ws = ph.split()
    ws[-1] = _strip_postposition(ws[-1])
    if len(ws)==2:
        return " ".join(sorted(ws))
    return " ".join(ws)

def _extract_top_phrases(lines: list[str], topk: int = 10) -> list[str]:
    docs = []
    for x in lines:
        x = re.sub(r"\s+"," ", (x or "").replace("\u200b"," ")).strip()
        if x and len(x) > 4:
            toks = _tok_line_for_trends(x)
            if len(toks) >= 2:
                docs.append(" ".join(toks))
    if not docs:
        return []
    vec = TfidfVectorizer(tokenizer=lambda s: s.split(), token_pattern=None, lowercase=False, ngram_range=(2,3), min_df=2)
    X = vec.fit_transform(docs)
    ngrams = vec.get_feature_names_out()
    scores = np.asarray(X.sum(axis=0)).ravel()
    cand = []
    for ph, sc in zip(ngrams, scores):
        if _is_bad_phrase(ph): 
            continue
        norm = _normalize_phrase(ph)
        cand.append((norm, float(sc)))
    if not cand:
        return []
    best = {}
    for norm, sc in cand:
        if norm not in best or sc > best[norm]:
            best[norm] = sc
    ranked = sorted(best.items(), key=lambda x: x[1], reverse=True)
    return [p for p,_ in ranked[:topk]]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ Naver íŠ¸ë Œë“œ(ì¸ê¸°/ì •ì¹˜ í—¤ë“œë¼ì¸) â†’ í•µì‹¬ ëª…ì‚¬êµ¬ Top10 â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _fetch_trends_naver(add_log=None) -> tuple[list[str], str]:
    headers = {"User-Agent": "Mozilla/5.0"}
    urls = [
        "https://news.naver.com/main/ranking/popularDay.naver",
        "https://news.naver.com/section/100",
        "https://news.naver.com/",
    ]
    selectors = [
        "ol.ranking_list a","div.rankingnews_box a",
        "ul.sa_list a.sa_text_title","a.sa_text_title_link",
        "a.cluster_text_headline","a[href*='/read?']",
    ]
    titles = []
    for u in urls:
        try:
            r = requests.get(u, headers=headers, timeout=12)
            if add_log: add_log(f"[naver] {u} status={r.status_code}")
            r.raise_for_status()
            soup = BeautifulSoup(r.text, "lxml")
            got = []
            for css in selectors:
                for a in soup.select(css):
                    t = a.get_text(" ", strip=True)
                    if t: got.append(t)
            if got:
                titles.extend(got)
        except Exception as e:
            if add_log: add_log(f"[naver] error: {e}")
            continue
    if not titles:
        return [], "none"
    phrases = _extract_top_phrases(titles, topk=10)
    return phrases, ("naver" if phrases else "none")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ íŠ¸ë Œë“œ ì†ŒìŠ¤ (êµ¬ê¸€/ë„¤ì´ë²„/ìœ íŠœë¸Œ) â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_data(show_spinner=False, ttl=900)
def google_trends_top(debug_log: bool = False, source_mode: str = "auto"):
    """
    íŠ¸ë Œë“œ í‚¤ì›Œë“œ/ëª…ì‚¬êµ¬ Top10
    source_mode: "auto" | "google" | "naver" | "youtube"
    """
    logs = []
    def add(msg):
        if debug_log: logs.append(str(msg))

    def _google_try():
        headers = {
            "User-Agent": "Mozilla/5.0",
            "Accept": "application/json,text/plain,*/*",
            "Referer": "https://trends.google.com/",
            "Accept-Language": "ko-KR,ko;q=0.9,en;q=0.8",
        }
        bases = ["https://trends.google.com", "https://trends.google.co.kr"]

        # A. Daily
        for base in bases:
            try:
                url = f"{base}/trends/api/dailytrends"
                r = requests.get(url, headers=headers,
                                 params={"hl":"ko","tz":"540","geo":"KR"},
                                 timeout=15, allow_redirects=True)
                add(f"[google daily] {r.status_code} {r.url}")
                r.raise_for_status()
                data = json.loads(r.text.lstrip(")]}',\n "))
                days = data.get("default", {}).get("trendingSearchesDays", [])
                items = days[0].get("trendingSearches", []) if days else []
                kws = [ (it.get("title",{}) or {}).get("query","") for it in items ]
                kws = [k.strip() for k in kws if k and k.strip()]
                kws = [p for p in kws if not _is_bad_phrase(p)]
                if kws: return kws[:10], "google-daily"
            except Exception as e:
                add(f"[google daily] error: {e}")

        # B. Realtime
        for base in bases:
            try:
                url = f"{base}/trends/api/realtimetrends"
                r = requests.get(url, headers=headers,
                                 params={"hl":"ko","tz":"540","cat":"all","fi":0,"fs":0,"geo":"KR","ri":300,"rs":20},
                                 timeout=15, allow_redirects=True)
                add(f"[google realtime] {r.status_code} {r.url}")
                r.raise_for_status()
                data = json.loads(r.text.lstrip(")]}',\n "))
                stories = data.get("storySummaries", {}).get("trendingStories", [])
                kws = []
                for s in stories:
                    for e in s.get("entityNames", []):
                        e = (e or "").strip()
                        if e and e not in kws: 
                            kws.append(e)
                kws = [p for p in kws if not _is_bad_phrase(p)]
                if kws: return kws[:10], "google-realtime"
            except Exception as e:
                add(f"[google realtime] error: {e}")

        # C. RSS
        for base in bases:
            try:
                url = f"{base}/trends/trendingsearches/daily/rss?geo=KR&hl=ko"
                r = requests.get(url, headers={"User-Agent": headers["User-Agent"], "Accept":"application/rss+xml"},
                                 timeout=15, allow_redirects=True)
                add(f"[google rss] {r.status_code} {r.url}")
                r.raise_for_status()
                root = ET.fromstring(r.content)
                titles = []
                for item in root.findall(".//item"):
                    t = (item.findtext("title") or "").strip()
                    if t: titles.append(t)
                    if len(titles) >= 10: break
                titles = [p for p in titles if not _is_bad_phrase(p)]
                if titles: return titles, "google-rss"
            except Exception as e:
                add(f"[google rss] error: {e}")

        return [], "none"

    def _youtube_fallback():
        try:
            words = st.session_state.get("yt_kw_words", [])
            words = [w for w in words if not _is_bad_phrase(w)]
            return (words[:10], "youtube-fallback") if words else ([], "none")
        except Exception:
            return [], "none"

    if source_mode == "google":
        kws, src = _google_try()
        return kws, src, logs
    if source_mode == "naver":
        kws, src = _fetch_trends_naver(add)
        return kws, src, logs
    if source_mode == "youtube":
        kws, src = _youtube_fallback()
        return kws, src, logs

    # auto
    kws, src = _google_try()
    if not kws:
        kws, src = _fetch_trends_naver(add)
    if not kws:
        kws, src = _youtube_fallback()
    return (kws or []), (src or "none"), logs

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ UI â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.title("ğŸ“º 48ì‹œê°„ ìœ íŠœë¸Œ ìˆì¸  íŠ¸ë Œë“œ ëŒ€ì‹œë³´ë“œ (ì •ì¹˜Â·ë‰´ìŠ¤)")

with st.sidebar:
    st.header("ìˆ˜ì§‘ ì˜µì…˜")
    size = st.selectbox("ìˆ˜ì§‘ ê·œëª¨(Shorts í›„ë³´ ìˆ˜)", [50, 100, 200], index=1)
    pages = {50:1, 100:2, 200:4}[size]

    ttl_choice = st.selectbox("ìºì‹œ TTL(ìë™ ì ˆì•½)", ["15ë¶„","30ë¶„(ì¶”ì²œ)","60ë¶„"], index=1)
    ttl_map = {"15ë¶„":900, "30ë¶„(ì¶”ì²œ)":1800, "60ë¶„":3600}
    ttl_sec = ttl_map[ttl_choice]

    rank_mode = st.radio("ì •ë ¬ ê¸°ì¤€", ["ìƒìŠ¹ì†ë„(ë·°/ì‹œê°„)", "ì¡°íšŒìˆ˜(ì´í•©)"], horizontal=True, index=0)
    sort_order = st.radio("ì •ë ¬ ìˆœì„œ", ["ë‚´ë¦¼ì°¨ìˆœ", "ì˜¤ë¦„ì°¨ìˆœ"], horizontal=True, index=0)
    show_speed_cols = st.checkbox("ìƒìŠ¹ì†ë„/ê²½ê³¼ì‹œê°„ ì»¬ëŸ¼ í‘œì‹œ", value=True)

    trend_source = st.radio(
        "íŠ¸ë Œë“œ ì†ŒìŠ¤ ì„ íƒ",
        ["ìë™(êµ¬ê¸€â†’ë„¤ì´ë²„)", "êµ¬ê¸€ë§Œ", "ë„¤ì´ë²„ë§Œ", "ìœ íŠœë¸Œë§Œ"],
        index=0
    )

    run = st.button("ìƒˆë¡œê³ ì¹¨(ë°ì´í„° ìˆ˜ì§‘)")

bucket = int(time.time() // ttl_sec)
if run:
    st.cache_data.clear()
    st.success("ë°ì´í„° ìƒˆë¡œê³ ì¹¨ ì‹œì‘!")

df = fetch_shorts_df(pages=pages, bucket=bucket)

base_col = "views_per_hour" if rank_mode.startswith("ìƒìŠ¹ì†ë„") else "view_count"
ascending_flag = (sort_order == "ì˜¤ë¦„ì°¨ìˆœ")

base_pool_n = max(50, len(df))
df_pool = df.sort_values(base_col, ascending=ascending_flag, ignore_index=True).head(base_pool_n)

# ìœ íŠœë¸Œ í‚¤ì›Œë“œ Top10
yt_kw = top_keywords_from_df(df_pool, topk=10)
yt_kw_words = [w for w, _ in yt_kw]
st.session_state["yt_kw_words"] = yt_kw_words  # ìœ íŠœë¸Œ fallbackìš©

# íŠ¸ë Œë“œ ì†ŒìŠ¤ ëª¨ë“œ
mode_map = {
    "ìë™(êµ¬ê¸€â†’ë„¤ì´ë²„)": "auto",
    "êµ¬ê¸€ë§Œ": "google",
    "ë„¤ì´ë²„ë§Œ": "naver",
    "ìœ íŠœë¸Œë§Œ": "youtube",
}
source_mode = mode_map[trend_source]

# íŠ¸ë Œë“œ í‚¤ì›Œë“œ
g_kw, g_src, g_logs = google_trends_top(source_mode=source_mode, debug_log=False)
st.caption(f"íŠ¸ë Œë“œ ì†ŒìŠ¤: {g_src if g_kw else 'Unavailable'} Â· í‚¤ì›Œë“œ {len(g_kw)}ê°œ Â· ëª¨ë“œ={trend_source}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì¿¼í„°/ë¦¬ì…‹ ì •ë³´ â”€â”€â”€â”€â”€â”€â”€â”€â”€
now_pt = dt.datetime.now(PT)
reset_pt = (now_pt + dt.timedelta(days=1)).replace(hour=0,minute=0,second=0,microsecond=0)
remain_td = reset_pt - now_pt
used = st.session_state.get("quota_used", 0)
remaining = max(0, DAILY_QUOTA - used)
pct = min(1.0, used / DAILY_QUOTA)

quota1, quota2 = st.columns([2,1])
with quota1:
    st.subheader("ğŸ”‹ ì˜¤ëŠ˜ ì¿¼í„°(ì¶”ì •)")
    st.progress(pct, text=f"ì‚¬ìš© {used} / {DAILY_QUOTA}  (ë‚¨ì€ {remaining})")
with quota2:
    st.metric("ë‚¨ì€ ì¿¼í„°(ì¶”ì •)", value=f"{remaining:,}", delta=f"ë¦¬ì…‹ê¹Œì§€ {str(remain_td).split('.')[0]}")
st.caption("â€» YouTube Data API ì¼ì¼ ì¿¼í„°ëŠ” ë§¤ì¼ PT(ë¯¸êµ­ ì„œë¶€) ìì •ì— ë¦¬ì…‹ë©ë‹ˆë‹¤. (KST ê¸°ì¤€ ë‹¤ìŒë‚  16~17ì‹œ, ì„œë¨¸íƒ€ì„ ë”°ë¼ ë³€ë™)")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ìƒë‹¨ ë³´ë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€
left, right = st.columns(2)
with left:
    st.subheader("ğŸ“ˆ ìœ íŠœë¸Œ(48hÂ·ìƒìœ„ í’€) í‚¤ì›Œë“œ Top10")
    if yt_kw:
        df_kw = pd.DataFrame(yt_kw, columns=["keyword","count"])
        df_kw_sorted = df_kw.sort_values("count", ascending=ascending_flag)
        st.bar_chart(df_kw_sorted.set_index("keyword")["count"])
        st.dataframe(df_kw_sorted, use_container_width=True, hide_index=True)
        st.download_button("ìœ íŠœë¸Œ í‚¤ì›Œë“œ CSV",
                           df_kw_sorted.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig"),
                           file_name="yt_keywords_top10.csv", mime="text/csv")
    else:
        st.info("í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•  ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ìˆ˜ì§‘ ê·œëª¨/í˜ì´ì§€ë¥¼ ëŠ˜ë ¤ë³´ì„¸ìš”.")
with right:
    st.subheader("ğŸŒ Trends Top10")

    # ë””ë²„ê·¸ ëª¨ë“œë©´ ì›ë³¸ ë¡œê·¸/í‚¤ì›Œë“œ ë³´ì—¬ì£¼ê¸°
    if trend_debug:
        with st.expander("ğŸ” íŠ¸ë Œë“œ ë””ë²„ê·¸ ë¡œê·¸/ì›ë³¸"):
            st.write(f"source_mode={source_mode}, src={g_src}")
            st.write(f"raw keywords({len(g_kw)}):", g_kw)
            if g_logs:
                st.code("\n".join(g_logs[-40:]), language="text")

    if g_kw:
        # 1) DataFrame ì •ë¦¬
        df_g = pd.DataFrame({"keyword": g_kw})
        # ê³µë°±/NaN/ì¤‘ë³µ ì œê±°í•˜ê³  Top10ë§Œ
        df_g = df_g.dropna()
        df_g["keyword"] = df_g["keyword"].astype(str).str.strip()
        df_g = df_g[df_g["keyword"] != ""].drop_duplicates("keyword").head(10)

        if len(df_g) >= 1:
            # 2) ìˆœìœ„ & ì‹œê°í™”ìš© ì ìˆ˜(1ë“±ì´ ê°€ì¥ í° ë§‰ëŒ€)
            df_g["rank"]  = np.arange(1, len(df_g) + 1, dtype=int)
            df_g["score"] = (len(df_g) + 1) - df_g["rank"]  # 1ë“±=ìµœëŒ€

            # 3) ë§‰ëŒ€ ê·¸ë˜í”„ (DataFrame â†’ ì•ˆì •)
            st.bar_chart(df_g.set_index("keyword")[["score"]])

            # 4) í‘œ & ë‹¤ìš´ë¡œë“œ
            st.dataframe(df_g[["rank", "keyword"]], use_container_width=True, hide_index=True)
            st.download_button(
                "íŠ¸ë Œë“œ í‚¤ì›Œë“œ CSV",
                df_g[["rank","keyword"]].to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig"),
                file_name="trends_top10.csv",
                mime="text/csv"
            )
        else:
            st.info("íŠ¸ë Œë“œ í‚¤ì›Œë“œê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. (ì¤‘ë³µ/ê³µë°± ì œê±° í›„ 0ê°œ)")
    else:
        st.info("ì„ íƒí•œ ì†ŒìŠ¤ì—ì„œ íŠ¸ë Œë“œ í‚¤ì›Œë“œë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (ëª¨ë“œë¥¼ ë°”ê¾¸ê±°ë‚˜ ë””ë²„ê·¸ë¡œ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”)")
# â”€â”€â”€â”€â”€â”€â”€â”€â”€ êµì§‘í•© â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _norm(s: str) -> str:
    s = s.lower().strip()
    s = re.sub(r"\s+", "", s)
    s = re.sub(r"[^\wê°€-í£]", "", s)
    return s

yt_norm = [_norm(w) for w in yt_kw_words]
g_norm  = [_norm(g) for g in g_kw]
hot = []
for raw_y, y in zip(yt_kw_words, yt_norm):
    for g in g_norm:
        if y and g and (y in g or g in y):
            hot.append(raw_y); break
_seen=set()
hot_intersection = [x for x in hot if not (x in _seen or _seen.add(x))]

st.subheader("ğŸ”¥ êµì§‘í•©(ë‘˜ ë‹¤ ëœ¨ëŠ” í‚¤ì›Œë“œ)")
st.write(", ".join(f"`{w}`" for w in hot_intersection) if hot_intersection else "í˜„ì¬ êµì§‘í•© í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ í•˜ë‹¨: ê²°ê³¼ í…Œì´ë¸” â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.subheader("ğŸ¬ ê´€ë ¨ ìˆì¸  ë¦¬ìŠ¤íŠ¸")
default_kw = (hot_intersection[0] if hot_intersection
              else (yt_kw_words[0] if yt_kw_words else ""))
pick_kw = st.text_input("í‚¤ì›Œë“œë¡œ í•„í„°(ë¶€ë¶„ ì¼ì¹˜)", value=default_kw)

df_show = df_pool.copy()
if pick_kw.strip():
    pat = re.compile(re.escape(pick_kw.strip()), re.IGNORECASE)
    mask = df_show["title"].str.contains(pat) | df_show["description"].str.contains(pat)
    df_show = df_show[mask]

cols = ["title","view_count","length","channel","url","published_at_kst"]
if show_speed_cols:
    cols = ["title","view_count","views_per_hour","hours_since_upload","length","channel","url","published_at_kst"]

df_show = df_show.sort_values(base_col, ascending=ascending_flag, ignore_index=True)[cols]

st.session_state["df_show_frozen"] = df_show.copy()
st.dataframe(st.session_state["df_show_frozen"], use_container_width=True)

csv_bytes = st.session_state["df_show_frozen"].to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig")
st.download_button("í˜„ì¬ í‘œ CSV ë‹¤ìš´ë¡œë“œ", data=csv_bytes, file_name="shorts_ranked.csv", mime="text/csv", key="dl_df_show")

st.markdown("""
---
**ì°¸ê³ **
- ìœ íŠœë¸Œ APIëŠ” ì—…ë¡œë” êµ­ê°€ë¥¼ í™•ì • ì œê³µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë³¸ ì•±ì€ `regionCode=KR`, `relevanceLanguage=ko`ë¡œ í•œêµ­ ìš°ì„  ê²°ê³¼ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
- ì¿¼í„° ë¹„ìš©(ì¶”ì •): `search.list = 100/í˜¸ì¶œ`, `videos.list = 1/í˜¸ì¶œ(50ê°œ ë‹¨ìœ„)`. ìˆ˜ì§‘ ê·œëª¨ê°€ ì»¤ì§ˆìˆ˜ë¡ ë¹„ìš©ì´ ëŠ˜ì–´ë‚©ë‹ˆë‹¤.
- ìºì‹œ TTLì„ ê¸¸ê²Œ ì„¤ì •í•˜ë©´ ì¿¼í„° ì‚¬ìš©ëŸ‰ì„ í¬ê²Œ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- íŠ¸ë Œë“œ ì†ŒìŠ¤ëŠ” *êµ¬ê¸€(realtimeâ†’dailyâ†’rss)* ì‹¤íŒ¨ ì‹œ *ë„¤ì´ë²„ ì¸ê¸°ë‰´ìŠ¤*ë¡œ ìë™ ëŒ€ì²´(â€œìë™â€ ëª¨ë“œ)ë©ë‹ˆë‹¤.
""")
