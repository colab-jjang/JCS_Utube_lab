# -*- coding: utf-8 -*-
# ðŸ“º 24ì‹œê°„ ìœ íŠœë¸Œ ìˆì¸  íŠ¸ë Œë“œ ëŒ€ì‹œë³´ë“œ (ì •ì¹˜Â·ë‰´ìŠ¤)
import streamlit as st
import pandas as pd
import numpy as np
import requests, re, json, time
from collections import Counter
from pathlib import Path
import datetime as dt
from zoneinfo import ZoneInfo
import xml.etree.ElementTree as ET
from bs4 import BeautifulSoup
from sklearn.feature_extraction.text import TfidfVectorizer
from typing import List, Tuple

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê¸°ë³¸ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="K-Politics/News Shorts Trend Board", page_icon="ðŸ“º", layout="wide")

API_KEY = st.secrets.get("YOUTUBE_API_KEY", "")
if not API_KEY:
    st.error("âš ï¸ API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. App â†’ Settings â†’ Secrets ì— `YOUTUBE_API_KEY = \"ë°œê¸‰í‚¤\"` ë¥¼ ë„£ì–´ì£¼ì„¸ìš”.")
    st.stop()

KST = ZoneInfo("Asia/Seoul")
PT  = ZoneInfo("America/Los_Angeles")
SEARCH_URL = "https://www.googleapis.com/youtube/v3/search"
VIDEOS_URL = "https://www.googleapis.com/youtube/v3/videos"
DAILY_QUOTA = 10_000

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì¿¼í„°(ì¼ì¼) ì˜êµ¬ ëˆ„ì  ì €ìž¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€
DATA_DIR   = Path(".")
QUOTA_FILE = DATA_DIR / "quota_usage.json"

def _today_pt_str():
    return dt.datetime.now(PT).strftime("%Y-%m-%d")

def load_quota_used():
    today = _today_pt_str()
    if QUOTA_FILE.exists():
        try:
            data = json.loads(QUOTA_FILE.read_text(encoding="utf-8"))
            if data.get("pt_date") == today:
                return int(data.get("used", 0))
        except Exception:
            pass
    return 0

def save_quota_used(value: int):
    data = {"pt_date": _today_pt_str(), "used": int(value)}
    QUOTA_FILE.write_text(json.dumps(data), encoding="utf-8")

def add_quota(cost: int):
    used = st.session_state.get("quota_used", 0) + int(cost)
    st.session_state["quota_used"] = used
    cur = load_quota_used()
    save_quota_used(cur + int(cost))

if "quota_used" not in st.session_state:
    st.session_state["quota_used"] = load_quota_used()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì‹œê°„ì°½: ìµœê·¼ 24ì‹œê°„(KST) â”€â”€â”€â”€â”€â”€â”€â”€â”€
def kst_window_last_24h():
    now_kst = dt.datetime.now(KST)
    start_kst = now_kst - dt.timedelta(hours=24)
    start_utc = start_kst.astimezone(dt.timezone.utc).isoformat().replace("+00:00", "Z")
    end_utc   = now_kst.astimezone(dt.timezone.utc).isoformat().replace("+00:00", "Z")
    return start_utc, end_utc, now_kst

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ API helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€
def api_get(url, params, cost):
    r = requests.get(url, params=params, timeout=20)
    add_quota(cost)
    r.raise_for_status()
    return r.json()

def parse_iso8601_duration(s):
    if not s or not s.startswith("PT"): return None
    s2 = s[2:]; h=m=sec=0; num=""
    for ch in s2:
        if ch.isdigit(): num += ch
        else:
            if ch=="H": h=int(num or 0)
            elif ch=="M": m=int(num or 0)
            elif ch=="S": sec=int(num or 0)
            num=""
    return h*3600 + m*60 + sec

def fmt_hms(seconds):
    if seconds is None: return ""
    h = seconds//3600; m=(seconds%3600)//60; s=seconds%60
    return f"{h:02d}:{m:02d}:{s:02d}" if h>0 else f"{m:02d}:{s:02d}"

def to_kst(iso_str):
    if not iso_str: return ""
    t = dt.datetime.fromisoformat(iso_str.replace("Z","+00:00")).astimezone(KST)
    return t.strftime("%Y-%m-%d %H:%M:%S")

def to_kst_dt(iso_str):
    return dt.datetime.fromisoformat(iso_str.replace("Z","+00:00")).astimezone(KST) if iso_str else None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë°ì´í„° ìˆ˜ì§‘ (ìºì‹œ) â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_data(show_spinner=False)
def fetch_shorts_df(pages:int=1, bucket:int=0):
    _ = bucket
    start_iso, end_iso, _now = kst_window_last_24h()

    vids, token = [], None
    for _ in range(pages):
        params = {
            "key": API_KEY, "part": "snippet", "type":"video", "order":"date",
            "publishedAfter": start_iso, "publishedBefore": end_iso,
            "maxResults": 50, "videoDuration":"short",
            "regionCode":"KR", "relevanceLanguage":"ko", "safeSearch":"moderate",
            "q": "ë‰´ìŠ¤ OR ì •ì¹˜ OR ì†ë³´ OR ë¸Œë¦¬í•‘"
        }
        if token: params["pageToken"] = token
        data = api_get(SEARCH_URL, params, cost=100)
        ids = [it.get("id",{}).get("videoId") for it in data.get("items",[]) if it.get("id",{}).get("videoId")]
        vids.extend(ids)
        token = data.get("nextPageToken")
        if not token: break

    seen=set(); ordered=[]
    for v in vids:
        if v not in seen: ordered.append(v); seen.add(v)

    details=[]
    for i in range(0, len(ordered), 50):
        chunk = ordered[i:i+50]
        if not chunk: continue
        params = {"key": API_KEY, "part": "snippet,contentDetails,statistics", "id": ",".join(chunk)}
        data = api_get(VIDEOS_URL, params, cost=1)
        details.extend(data.get("items", []))

    rows=[]
    for it in details:
        vid = it.get("id")
        sn  = it.get("snippet", {})
        cd  = it.get("contentDetails", {})
        stt = it.get("statistics", {})
        secs = parse_iso8601_duration(cd.get("duration",""))
        if secs is None or secs>60: continue
        pub_iso = sn.get("publishedAt","")
        rows.append({
            "video_id": vid,
            "title": sn.get("title",""),
            "description": sn.get("description",""),
            "view_count": int(pd.to_numeric(stt.get("viewCount","0"), errors="coerce") or 0),
            "like_count": int(pd.to_numeric(stt.get("likeCount","0"), errors="coerce") or 0),
            "comment_count": int(pd.to_numeric(stt.get("commentCount","0"), errors="coerce") or 0),
            "length": fmt_hms(secs),
            "length_seconds": secs,
            "channel": sn.get("channelTitle",""),
            "url": f"https://www.youtube.com/watch?v={vid}",
            "published_at_kst": to_kst(pub_iso),
            "published_dt_kst": to_kst_dt(pub_iso),
        })
    df = pd.DataFrame(rows)

    now_kst = dt.datetime.now(KST)
    if not df.empty:
        df["hours_since_upload"] = (now_kst - pd.to_datetime(df["published_dt_kst"])).dt.total_seconds() / 3600.0
        df["hours_since_upload"] = df["hours_since_upload"].clip(lower=(1.0/60.0))
        df["views_per_hour"] = (df["view_count"] / df["hours_since_upload"]).round(1)
    else:
        df["hours_since_upload"] = []
        df["views_per_hour"] = []
    return df

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ (ê³µìš©) ê¸ˆì§€ì–´/ë¶ˆìš©ì–´ & ë¬¸ìž¥ ê¸ˆì¹™/ì‹œê°„í‘œí˜„ ì»· â”€â”€â”€â”€â”€â”€â”€â”€â”€
COMMON_STOPWORDS = {
    # ë„ë©”ì¸/í”Œëž«í¼/í˜•ì‹
    "http","https","www","com","co","kr","net","org","youtube","shorts","watch","tv","cctv","sns",
    # ë‰´ìŠ¤ êµ°ë”ë”ê¸°
    "ê¸°ì‚¬","ë‹¨ë…","ì†ë³´","ì˜ìƒ","ì „ë¬¸","ë¼ì´ë¸Œ","ê¸°ìž","ë³´ë„","í—¤ë“œë¼ì¸","ë°ìŠ¤í¬","ì „ì²´ë³´ê¸°","ë”ë³´ê¸°",
    # ì‹œì /ì‹œê°„
    "ì˜¤ëŠ˜","ì–´ì œ","ìµœê·¼","ë°©ê¸ˆ","ë°©ê¸ˆì „","ì•„ì¹¨","ì˜¤ì „","ì˜¤í›„","ë°¤","ìƒˆë²½","ì²«ë‚ ","ë‰´ìŠ¤top10",
    # ë‚´ìš© ë¹ˆì•½/ìƒíˆ¬
    "ê´€ë ¨","ë…¼ëž€","ë…¼ìŸ","ìƒí™©","ì‚¬ê±´","ì´ìŠˆ","ë¶„ì„","ì „ë§","ë¸Œë¦¬í•‘","ë°œì–¸","ë°œí‘œ","ìž…ìž¥",
    # ì§€ëª…/ê¸°ê´€(ìƒíˆ¬)
    "ì„œìš¸","í•œêµ­","êµ­ë‚´","í•´ì™¸","ì •ë¶€","ì—¬ë‹¹","ì•¼ë‹¹","ë‹¹êµ­","ìœ„ì›ìž¥","ìž¥ê´€","ëŒ€í†µë ¹","ì´ë¦¬","êµ­íšŒ","ê²€ì°°",
    # ë„¤ê°€ ìš”ì²­í•œ ê³µìš© ë¸”ëž™ë¦¬ìŠ¤íŠ¸(ìœ íŠœë¸Œ/íŠ¸ë Œë“œ ê³µí†µ)
    "êµ¬ë…","ì •ì¹˜","ëŒ€í†µë ¹ì‹¤","ì±„ë„","news","ëŒ€ë²•ì›","íŠ¹ê²€","ì´ìž¼",
    # ë°©ì†¡ì‚¬/ë§¤ì²´ ìƒìˆ˜
    "sbs","kbs","mbc","jtbc","tvì¡°ì„ ","mbn","ì—°í•©ë‰´ìŠ¤","mbcë‰´ìŠ¤",
    # ìžì£¼ ëœ¨ëŠ” êµ°ë”ë”ê¸°
    "ì‹œìž‘","ì‚¬ê³ ","ì „ë¬¸","ì‚¬ì§„",
    # ê³¼ê±°ì— ê³µìœ í–ˆë˜ ê²ƒë“¤
    "ë‹¤íë””ê¹…","ë‚˜ëŠ”","ì ˆë¡œ"
}

# â€œì„ë°©í•˜ë¼â€ ë“± ë¬¸ìž¥ ì „ì²´ ê¸ˆì¹™ + â€œë‹¬ ë§Œì—/ì£¼ì§¸â€¦â€ ê°™ì€ ì‹œê°„ í‘œí˜„ ì»·
COMMON_BANNED_PAT = re.compile(r"(ì„ë°© ?í•˜ë¼|ìž… ?ë‹¥ì¹˜ê³ |ë¬´ìŠ¨ ?ì¼|ìˆ˜ ìžˆ(ì„ê¹Œ|ë‚˜)|ìˆ˜ ì—†ë‚˜)", re.I)
TEMPORAL_BAD_PAT = re.compile(
    r"""(
        \b\d+\s*(ë…„|ê°œì›”|ë‹¬|ì£¼|ì£¼ì¼|ì¼|ì‹œê°„)\s*(ë§Œì—|ì§¸|ë™ì•ˆ)\b
      | \b(ì´í›„|ì´ì „|ì „|í›„)\b
      | \b(ì˜¤ëŠ˜|ì–´ì œ)\s*(ì˜¤ì „|ì˜¤í›„)?\b
    )""",
    re.X | re.I
)

def _contains_common_banned(s: str) -> bool:
    s = s.lower()
    if COMMON_BANNED_PAT.search(s): return True
    if TEMPORAL_BAD_PAT.search(s):  return True
    return False

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ í‚¤ì›Œë“œ ì¶”ì¶œ â”€â”€â”€â”€â”€â”€â”€â”€â”€
KO_JOSA = ("ëŠ”","ì´","ê°€","ì„","ë¥¼","ì˜","ì—","ì—ì„œ","ì—ê²Œ","ê»˜","ì™€","ê³¼","ìœ¼ë¡œ","ë¡œ","ë„","ë§Œ","ê¹Œì§€","ë¶€í„°","ë§ˆë‹¤","ì¡°ì°¨","ë¼ë„","ë§ˆì €","ë°–ì—","ì²˜ëŸ¼","ë¿","ê»˜ì„œ","ì±„")
KO_SUFFIX = ("í•˜ê¸°","í•˜ì„¸ìš”","ì‹­ì‹œì˜¤","í•´ì£¼ì„¸ìš”","í•©ë‹ˆë‹¤","í–ˆë‹¤","ì¤‘","ê´€ë ¨","ì˜ìƒ","ì±„ë„","ë‰´ìŠ¤","ë³´ê¸°","ë“±ë¡","êµ¬ë…","í™ˆíŽ˜ì´ì§€","ë©ë‹ˆë‹¤")

def strip_korean_suffixes(t: str) -> str:
    for suf in KO_SUFFIX:
        if t.endswith(suf) and len(t) > len(suf)+1: t = t[:-len(suf)]
    for j in KO_JOSA:
        if t.endswith(j) and len(t) > len(j)+1: t = t[:-len(j)]
    return t

def tokenize_ko_en(text: str):
    text = str(text or "")
    if _contains_common_banned(text): return []
    text = re.sub(r"https?://\S+", " ", text)
    text = re.sub(r"www\.\S+", " ", text)
    raw = re.findall(r"[0-9A-Za-zê°€-íž£]+", text.lower())
    out=[]
    for t in raw:
        if not t or t.isdigit(): continue
        if re.fullmatch(r"[ê°€-íž£]+", t): t = strip_korean_suffixes(t)
        if t in COMMON_STOPWORDS or len(t) < 2: continue
        if re.fullmatch(r"[a-z]+", t) and len(t) <= 2: continue
        if t.endswith("tv") and len(t) > 2:
            t = t[:-2]
            if t in COMMON_STOPWORDS or len(t) < 2: continue
        out.append(t)
    return out

def top_keywords_from_df(df: pd.DataFrame, topk:int=10):
    corpus = (df["title"].fillna("") + " " + df["description"].fillna("")).tolist()
    cnt = Counter()
    for line in corpus: cnt.update(tokenize_ko_en(line))
    items = [(w,c) for w,c in cnt.most_common() if not re.fullmatch(r"\d+", w)]
    return items[:topk]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ UI â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.title("ðŸ“º 24ì‹œê°„ ìœ íŠœë¸Œ ìˆì¸  íŠ¸ë Œë“œ ëŒ€ì‹œë³´ë“œ (ì •ì¹˜Â·ë‰´ìŠ¤)")

with st.sidebar:
    st.header("ìˆ˜ì§‘ ì˜µì…˜")
    size = st.selectbox("ìˆ˜ì§‘ ê·œëª¨", [50, 100, 200], index=1)
    pages = {50:1, 100:2, 200:4}[size]
    ttl_choice = st.selectbox("ìºì‹œ TTL", ["15ë¶„","30ë¶„(ì¶”ì²œ)","60ë¶„"], index=1)
    ttl_map = {"15ë¶„":900,"30ë¶„(ì¶”ì²œ)":1800,"60ë¶„":3600}
    ttl_sec = ttl_map[ttl_choice]
    rank_mode = st.radio("ì •ë ¬ ê¸°ì¤€", ["ìƒìŠ¹ì†ë„(ë·°/ì‹œê°„)","ì¡°íšŒìˆ˜(ì´í•©)"], horizontal=True, index=0)
    sort_order = st.radio("ì •ë ¬ ìˆœì„œ", ["ë‚´ë¦¼ì°¨ìˆœ","ì˜¤ë¦„ì°¨ìˆœ"], horizontal=True, index=0)
    show_speed_cols = st.checkbox("ìƒìŠ¹ì†ë„/ê²½ê³¼ì‹œê°„ í‘œì‹œ", value=True)
    run = st.button("ìƒˆë¡œê³ ì¹¨")

bucket = int(time.time() // ttl_sec)
if run:
    st.cache_data.clear()
    st.success("ë°ì´í„° ìƒˆë¡œê³ ì¹¨!")

df = fetch_shorts_df(pages=pages, bucket=bucket)
base_col = "views_per_hour" if rank_mode.startswith("ìƒìŠ¹ì†ë„") else "view_count"
ascending_flag = (sort_order=="ì˜¤ë¦„ì°¨ìˆœ")
df_pool = df.sort_values(base_col, ascending=ascending_flag, ignore_index=True)

# í‚¤ì›Œë“œ Top10 (ê³µìš© ê¸ˆì§€ì–´/ë¬¸ìž¥ê¸ˆì¹™ ë°˜ì˜)
yt_kw = top_keywords_from_df(df_pool, topk=10)
yt_kw_words = [w for w,_ in yt_kw]
st.session_state["yt_kw_words"] = yt_kw_words

left, right = st.columns(2)
with left:
    st.subheader("ðŸ“ˆ ìœ íŠœë¸Œ í‚¤ì›Œë“œ Top10")
    if yt_kw:
        df_kw = pd.DataFrame(yt_kw, columns=["keyword","count"])
        st.bar_chart(df_kw.set_index("keyword")["count"])
        st.dataframe(df_kw, use_container_width=True, hide_index=True)
        st.download_button("CSV ë‹¤ìš´ë¡œë“œ",
            df_kw.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig"),
            file_name="yt_keywords_top10.csv", mime="text/csv")
    else:
        st.info("í‚¤ì›Œë“œ ë¶€ì¡±")

with right:
    st.subheader("ðŸŽ¬ ìˆì¸  ë¦¬ìŠ¤íŠ¸")
    cols = ["title","view_count","length","channel","like_count","comment_count","url","published_at_kst"]

    df_show = df_pool.copy()
    # ì•ˆì „ ì²˜ë¦¬: ì—†ëŠ” ì»¬ëŸ¼ì€ NaN/ë¹ˆ ë¬¸ìžì—´ë¡œ ì±„ì›Œì£¼ê¸°
    safe_cols = [c for c in cols if c in df_show.columns]
    for c in cols:
        if c not in df_show.columns:
            if c in ("view_count","like_count","comment_count"):
                df_show[c] = 0
            elif c == "length":
                df_show[c] = ""
            else:
                df_show[c] = ""
    df_show = df_show[cols]  # ìˆœì„œ ë³´ìž¥

    st.dataframe(df_show, use_container_width=True, hide_index=True)
    st.download_button("CSV ë‹¤ìš´ë¡œë“œ",
        df_show.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig"),
        file_name="shorts_list.csv", mime="text/csv")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì¿¼í„° í‘œì‹œ â”€â”€â”€â”€â”€â”€â”€â”€â”€
now_pt = dt.datetime.now(PT)
reset_pt = (now_pt+dt.timedelta(days=1)).replace(hour=0,minute=0,second=0,microsecond=0)
remain_td = reset_pt - now_pt
used = st.session_state.get("quota_used", 0)
remaining = max(0, DAILY_QUOTA-used)
pct = min(1.0, used/DAILY_QUOTA)

q1,q2=st.columns([2,1])
with q1:
    st.subheader("ðŸ”‹ ì˜¤ëŠ˜ ì¿¼í„°(ì¶”ì •)")
    st.progress(pct, text=f"ì‚¬ìš© {used} / {DAILY_QUOTA} (ë‚¨ì€ {remaining})")
with q2:
    st.metric("ë‚¨ì€ ì¿¼í„°", value=f"{remaining:,}", delta=f"ë¦¬ì…‹ê¹Œì§€ {str(remain_td).split('.')[0]}")
