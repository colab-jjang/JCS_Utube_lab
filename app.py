import streamlit as st
import pandas as pd
import datetime as dt
from zoneinfo import ZoneInfo
import requests

# ====== Settings ======
API_KEY = st.secrets.get("YOUTUBE_API_KEY", "")
REGION_CODE = "KR"         # í•œêµ­ ê²°ê³¼ ìš°ì„ 
RELEVANCE_LANG = "ko"      # í•œêµ­ì–´ ìš°ì„ 
KST = ZoneInfo("Asia/Seoul")
PT  = ZoneInfo("America/Los_Angeles")
DAILY_QUOTA = 10_000       # YouTube Data API ê¸°ë³¸ ì¼ì¼ ì¿¼í„°

# ì¿¼í„° ì„¸ì…˜ ëˆ„ì ì‹œí‚´

import json, os
from pathlib import Path

DATA_DIR = Path(".")
QUOTA_FILE = DATA_DIR / "quota_usage.json"   # ì•± í´ë”ì— ì €ì¥ (ì•±ì´ ì‚´ì•„ìˆëŠ” í•œ ìœ ì§€)

def _today_pt_str():
    from zoneinfo import ZoneInfo
    PT = ZoneInfo("America/Los_Angeles")
    now_pt = dt.datetime.now(PT)
    return now_pt.strftime("%Y-%m-%d")

def load_quota_used():
    """íŒŒì¼ì—ì„œ ì˜¤ëŠ˜(PT) ì‚¬ìš©ëŸ‰ì„ ì½ì–´ì˜¨ë‹¤. ë‚ ì§œ ë‹¤ë¥´ë©´ 0ìœ¼ë¡œ ë¦¬ì…‹."""
    today = _today_pt_str()
    if QUOTA_FILE.exists():
        try:
            data = json.loads(QUOTA_FILE.read_text(encoding="utf-8"))
            if data.get("pt_date") == today:
                return int(data.get("used", 0))
        except Exception:
            pass
    return 0

def save_quota_used(value):
    """ì˜¤ëŠ˜(PT) ì‚¬ìš©ëŸ‰ì„ íŒŒì¼ì— ì €ì¥."""
    data = {"pt_date": _today_pt_str(), "used": int(value)}
    QUOTA_FILE.write_text(json.dumps(data), encoding="utf-8")

def add_quota(cost):
    """ì¿¼í„°ë¥¼ ëˆ„ì (íŒŒì¼+ì„¸ì…˜ ëª¨ë‘)"""
    # ì„¸ì…˜(í™”ë©´ í‘œì‹œìš©)
    st.session_state["quota_used"] = st.session_state.get("quota_used", 0) + int(cost)
    # íŒŒì¼(ì˜êµ¬ ëˆ„ì )
    current_file_val = load_quota_used()
    save_quota_used(current_file_val + int(cost))

# ====== Time window (ë§ˆì§€ë§‰ 48ì‹œê°„, KST ê¸°ì¤€) ======
def kst_window_last_48h():
    now_kst = dt.datetime.now(KST)
    start_kst = now_kst - dt.timedelta(hours=48)
    start_utc = start_kst.astimezone(dt.timezone.utc).isoformat().replace("+00:00", "Z")
    end_utc   = now_kst.astimezone(dt.timezone.utc).isoformat().replace("+00:00", "Z")
    return start_utc, end_utc, now_kst

# ====== ISO8601 PT-duration -> seconds ======
def parse_iso8601_duration(s):
    if not s or not s.startswith("PT"):
        return None
    s2 = s[2:]; h=m=sec=0; num=""
    for ch in s2:
        if ch.isdigit(): num += ch
        else:
            if ch == "H":
                h = int(num or 0)
            elif ch == "M":
                m = int(num or 0)
            elif ch == "S":
                sec = int(num or 0)
            num = ""
    return h*3600 + m*60 + sec

def fmt_hms(seconds):
    if seconds is None: return ""
    h = seconds//3600; m=(seconds%3600)//60; s=seconds%60
    return f"{h:02d}:{m:02d}:{s:02d}" if h>0 else f"{m:02d}:{s:02d}"

# ====== API helper (ì¿¼í„° ì¹´ìš´íŠ¸ í¬í•¨) ======
def api_get(url, params, cost):
    r = requests.get(url, params=params, timeout=20)
    # ì„±ê³µ/ì‹¤íŒ¨ì™€ ë¬´ê´€, ìœ íš¨/ë¬´íš¨ ìš”ì²­ ëª¨ë‘ ë¹„ìš© ë°œìƒ -> ë¬¸ì„œ ê·œì •
    add_quota(cost)
    r.raise_for_status()
    return r.json()

SEARCH_URL = "https://www.googleapis.com/youtube/v3/search"
VIDEOS_URL = "https://www.googleapis.com/youtube/v3/videos"

def search_ids(keyword, max_pages=1):
    start_iso, end_iso, _ = kst_window_last_48h()
    vids, token, pages = [], None, 0
    while True:
        params = {
            "key": API_KEY,
            "part": "snippet",
            "q": keyword,
            "type": "video",
            "order": "date",
            "publishedAfter": start_iso,
            "publishedBefore": end_iso,
            "maxResults": 50,
            "videoDuration": "short",
            "regionCode": REGION_CODE,
            "relevanceLanguage": RELEVANCE_LANG,
        }
        if token: params["pageToken"] = token
        data = api_get(SEARCH_URL, params, cost=100)  # search.list = 100
        ids = [it.get("id", {}).get("videoId") for it in data.get("items", []) if it.get("id", {}).get("videoId")]
        vids.extend(ids)
        token = data.get("nextPageToken"); pages += 1
        if not token or pages >= max_pages or len(vids) >= 200:
            break
    # de-dup
    seen, ordered = set(), []
    for v in vids:
        if v not in seen:
            ordered.append(v); seen.add(v)
    return ordered

def fetch_details(video_ids):
    out=[]
    for i in range(0, len(video_ids), 50):
        chunk = video_ids[i:i+50]
        params = {"key": API_KEY, "part": "snippet,contentDetails,statistics", "id": ",".join(chunk)}
        data = api_get(VIDEOS_URL, params, cost=1)  # videos.list = 1
        out.extend(data.get("items", []))
    return out

def to_kst(iso_str):
    t = dt.datetime.fromisoformat(iso_str.replace("Z", "+00:00")).astimezone(KST)
    return t.strftime("%Y-%m-%d %H:%M:%S (%Z)")

def make_dataframe(keyword, max_pages=1):
    ids = search_ids(keyword, max_pages=max_pages)
    details = fetch_details(ids)
    rows=[]
    for item in details:
        vid=item.get("id",""); sn=item.get("snippet",{}); cd=item.get("contentDetails",{}); stt=item.get("statistics",{})
        secs = parse_iso8601_duration(cd.get("duration",""))
        if secs is None or secs>60:  # Shortsë§Œ
            continue
        rows.append({
            "title": sn.get("title",""),
            "view_count": stt.get("viewCount",""),
            "length": fmt_hms(secs),
            "channel": sn.get("channelTitle",""),
            "url": f"https://www.youtube.com/watch?v={vid}",
            "published_at_kst": to_kst(sn.get("publishedAt","")) if sn.get("publishedAt") else "",
        })
    df = pd.DataFrame(rows, columns=["title","view_count","length","channel","url","published_at_kst"])
    df["view_count"] = pd.to_numeric(df["view_count"], errors="coerce").fillna(0).astype(int)
    return df

def next_reset_info():
    now_pt = dt.datetime.now(PT)
    reset_pt = (now_pt + dt.timedelta(days=1)).replace(hour=0, minute=0, second=0, microsecond=0)
    remaining = reset_pt - now_pt
    reset_kst = reset_pt.astimezone(KST)
    return reset_pt, reset_kst, remaining

# ====== ì¿¼í„° ì¹´ìš´íŠ¸ ======
# ì„¸ì…˜ ìƒíƒœì— ì¿¼í„° ì¹´ìš´í„° ì¤€ë¹„
if "quota_used" not in st.session_state:
    st.session_state["quota_used"] = load_quota_used()
else:
#ë‚ ì§œê°€ ë°”ë€Œì—ˆì„ ìˆ˜ë„ ìˆìœ¼ë‹ˆ ì¬ë™ê¸°í™”
    st.session_state["quota_used"] = load_quota_used()

# ====== UI ======
st.set_page_config(page_title="YouTube Shorts 48h Finder", page_icon="ğŸ“º", layout="wide")
st.title("ğŸ“º 48ì‹œê°„ ì´ë‚´ ì—…ë¡œë“œëœ YouTube Shorts ì°¾ê¸° (KR)")

if not API_KEY:
    st.error("âš ï¸ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì¢Œì¸¡ ë©”ë‰´(â–¶) > Settings > Secrets ì— YOUTUBE_API_KEYë¥¼ ì¶”ê°€í•˜ì„¸ìš”.")
    st.stop()

with st.sidebar:
    st.header("ì„¤ì •")
    keyword = st.text_input("ê²€ìƒ‰ì–´", "")
    max_pages = st.radio("ê²€ìƒ‰ í˜ì´ì§€ ìˆ˜(ì¿¼í„° ì ˆì•½)", options=[1,2], index=0)
    st.caption("ë²”ìœ„: í˜„ì¬ ì‹œê°(KST) ê¸°ì¤€ **ì§€ë‚œ 48ì‹œê°„**")
    run_btn = st.button("ê²€ìƒ‰ ì‹¤í–‰")

# ì¿¼í„° íŒ¨ë„
used = st.session_state["quota_used"]
remaining = max(0, DAILY_QUOTA - used)
pct = min(1.0, used / DAILY_QUOTA) if DAILY_QUOTA else 0.0

reset_pt, reset_kst, remaining_td = next_reset_info()

quota_col1, quota_col2 = st.columns([2,1])
with quota_col1:
    st.subheader("ğŸ”‹ ì¿¼í„° ì‚¬ìš©ëŸ‰(ì¶”ì •)")
    st.progress(pct, text=f"ì‚¬ìš© {used} / {DAILY_QUOTA}  (ë‚¨ì€ {remaining})")
with quota_col2:
    st.metric("ë‚¨ì€ ì¿¼í„°(ì¶”ì •)", value=f"{remaining:,}", delta=f"ë¦¬ì…‹ê¹Œì§€ {remaining_td}".replace("days","ì¼").replace("day","ì¼"))
st.caption(f"â€» ì¼ì¼ ì¿¼í„°ëŠ” PT ìì •(í•œêµ­ì‹œê°„ ë‹¤ìŒë‚  16~17ì‹œ)ì— ë¦¬ì…‹")

# ì‹¤í–‰
if run_btn:
    with st.spinner("ê²€ìƒ‰ ì¤‘â€¦ â³"):
        df = make_dataframe(keyword, max_pages=max_pages)
        df_top = df.sort_values("view_count", ascending=False, ignore_index=True).head(20)
    st.success(f"ê²€ìƒ‰ ì™„ë£Œ: í›„ë³´ {len(df)}ê°œ ì¤‘ ìƒìœ„ 20ê°œ í‘œì‹œ")

    sort_col = st.selectbox("ì •ë ¬ ì»¬ëŸ¼", ["view_count","title","length","channel","published_at_kst"])
    sort_order = st.radio("ì •ë ¬ ìˆœì„œ", ["ë‚´ë¦¼ì°¨ìˆœ","ì˜¤ë¦„ì°¨ìˆœ"], horizontal=True, index=0)
    asc = (sort_order == "ì˜¤ë¦„ì°¨ìˆœ")
    df_show = df_top.sort_values(sort_col, ascending=asc, ignore_index=True)

    df_show = df_show[["title","view_count","length","channel","url","published_at_kst"]]

    st.dataframe(df_show, use_container_width=True)

    csv_bytes = df_show.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig")
    st.download_button("CSV ë‹¤ìš´ë¡œë“œ", data=csv_bytes,
                       file_name=f"shorts_48h_{keyword}.csv", mime="text/csv")

    st.info(f"ì´ë²ˆ ì‹¤í–‰ìœ¼ë¡œ ì¶”ì • ì‚¬ìš©ëŸ‰: search.list {100 * (max_pages)} + videos.list {1}")
